{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re # For cleaning text\n",
    "import string # For punctuation removal\n",
    "import time\n",
    "import os\n",
    "import scipy.sparse # Needed for sparse matrix checks and operations\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Sentence Transformers and Torch for Dense Embeddings\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    print(\"Sentence Transformers and Torch loaded.\")\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Sentence Transformers or Torch not found. Dense embedding methods will be skipped.\")\n",
    "    print(\"Install them (`pip install sentence-transformers torch`) to enable.\")\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "\n",
    "# Download necessary NLTK data (run only once)\n",
    "print(\"Downloading NLTK data (if necessary)...\")\n",
    "nltk_packages = ['wordnet', 'stopwords', 'punkt']\n",
    "for package in nltk_packages:\n",
    "    try:\n",
    "        if package == 'punkt':\n",
    "            nltk.data.find(f'tokenizers/{package}')\n",
    "        else:\n",
    "             nltk.data.find(f'corpora/{package}')\n",
    "        # print(f\"NLTK package '{package}' already downloaded.\")\n",
    "    except:\n",
    "        try:\n",
    "           print(f\"Downloading NLTK package '{package}'...\")\n",
    "           nltk.download(package, quiet=True)\n",
    "           print(f\"NLTK package '{package}' downloaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading NLTK package '{package}': {e}\")\n",
    "print(\"NLTK check complete.\")\n",
    "\n",
    "# ## 0.1 Helper Functions (Modified and New)\n",
    "print(\"Defining helper functions...\")\n",
    "\n",
    "# Initialize lemmatizer, stemmer and stopwords globally for efficiency\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = set([\n",
    "    'claim', 'claims', 'claimed', 'method', 'system', 'device', 'apparatus', 'assembly', 'unit',\n",
    "    'comprising', 'comprises', 'thereof', 'wherein', 'said', 'thereby', 'herein', 'accordance',\n",
    "    'invention', 'present', 'related', 'relates', 'figure', 'fig', 'example', 'examples',\n",
    "    'embodiment', 'embodiments', 'accordance', 'therein', 'associated', 'provided', 'configured',\n",
    "    'includes', 'including', 'based', 'least', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',\n",
    "    'first', 'second', 'third', 'fourth', 'fifth', 'etc', 'eg', 'ie',\n",
    "    'may', 'further', 'also', 'within', 'upon', 'used', 'using', 'use', 'capable', 'adapted',\n",
    "    'generally', 'typically', 'respectively', 'particularly', 'preferably', 'various', 'such',\n",
    "    'described', 'disclosed', 'illustrated', 'shown',\n",
    "    'portion', 'member', 'element', 'surface', 'axis', 'position', 'direction', 'side', 'end', 'top', 'bottom',\n",
    "    'lower', 'upper', 'inner', 'outer', 'rear', 'front', 'lateral',\n",
    "    'set', 'provide', 'generate', 'control', 'controlling', 'operation', 'value', 'signal', 'process', 'data',\n",
    "    'group', 'range', 'level', 'time', 'number', 'result', 'type', 'form', 'part', 'manner', 'step'\n",
    "])\n",
    "all_stopwords = stop_words.union(custom_stopwords)\n",
    "\n",
    "\n",
    "def preprocess_text(text, use_stemming=False, use_custom_stopwords=True):\n",
    "    \"\"\"Enhanced preprocessing: lowercase, remove punctuation/numbers, lemmatize/stem, remove stopwords.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # Remove single letters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Normalize whitespace\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    current_stopwords = all_stopwords if use_custom_stopwords else stop_words\n",
    "\n",
    "    if use_stemming:\n",
    "        processed_tokens = [stemmer.stem(word) for word in tokens if word not in current_stopwords and len(word) > 2]\n",
    "    else:\n",
    "        processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in current_stopwords and len(word) > 2]\n",
    "\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding='utf-8') as file: # Added encoding\n",
    "            contents = json.load(file)\n",
    "        return contents\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_corpus(corpus, text_type, preprocess=False, config={}):\n",
    "    \"\"\"\n",
    "    Extracts and optionally preprocesses text data from a corpus based on the specified text type.\n",
    "    Now passes config to allow conditional preprocessing.\n",
    "    \"\"\"\n",
    "    if not corpus: # Handle case where corpus failed to load\n",
    "        print(f\"Warning: Attempting to create corpus from empty or None input for '{text_type}'.\")\n",
    "        return []\n",
    "\n",
    "    app_ids = []\n",
    "    texts = []\n",
    "    cnt = 0 # count the number of documents skipped\n",
    "\n",
    "    print(f\"Creating corpus for text_type: '{text_type}'...\")\n",
    "\n",
    "    required_parts = []\n",
    "    if 'title' in text_type: required_parts.append('title')\n",
    "    if 'abstract' in text_type: required_parts.append('pa01')\n",
    "    if 'claim1' in text_type: required_parts.append('c-en-0001')\n",
    "\n",
    "    for doc in tqdm(corpus, desc=f\"Processing {text_type}\", leave=False):\n",
    "        doc_id = doc.get('Application_Number', '') + doc.get('Application_Category', '')\n",
    "        if not doc_id: # Skip if ID is missing\n",
    "            cnt+=1\n",
    "            continue\n",
    "        content = doc.get('Content', {})\n",
    "        if not content: # Skip if content is missing\n",
    "             cnt += 1\n",
    "             continue\n",
    "\n",
    "        doc_text_parts = []\n",
    "        missing_part = False\n",
    "\n",
    "        # Simplified collection logic using a mapping\n",
    "        part_map = {\n",
    "            'title': ['title'],\n",
    "            'abstract': ['pa01'],\n",
    "            'claim1': ['c-en-0001'],\n",
    "            'claims': [k for k in content if k.startswith('c-en-')],\n",
    "            'description': [k for k in content if k.startswith('p')],\n",
    "            'fulltext': list(content.keys())\n",
    "        }\n",
    "\n",
    "        keys_to_extract = set()\n",
    "        if text_type == 'title_abstract': keys_to_extract.update(part_map['title'] + part_map['abstract'])\n",
    "        elif text_type == 'title_abstract_claim1': keys_to_extract.update(part_map['title'] + part_map['abstract'] + part_map['claim1'])\n",
    "        elif text_type == 'title_abstract_claims': keys_to_extract.update(part_map['title'] + part_map['abstract'] + part_map['claims'])\n",
    "        elif text_type in part_map: keys_to_extract.update(part_map[text_type])\n",
    "        else: print(f\"Warning: Unknown text_type '{text_type}' in create_corpus.\")\n",
    "\n",
    "        # Extract text for the required keys, removing None values\n",
    "        extracted_texts = [content.get(key) for key in keys_to_extract if content.get(key)]\n",
    "        doc_text_parts = list(dict.fromkeys(filter(None, extracted_texts))) # Unique parts, preserving order\n",
    "\n",
    "        # Check if required parts are missing ONLY if it's a specific type (not combo or fulltext)\n",
    "        if text_type in ['title', 'abstract', 'claim1', 'claims', 'description']:\n",
    "             if not doc_text_parts: # If the specific part(s) were not found\n",
    "                 missing_part = True\n",
    "\n",
    "        # Final check and processing\n",
    "        if not doc_text_parts or missing_part:\n",
    "            cnt += 1\n",
    "        else:\n",
    "            final_text = ' '.join(doc_text_parts)\n",
    "\n",
    "            # Apply preprocessing based on config and method type\n",
    "            if preprocess and config.get('method') != 'dense': # Only preprocess if requested AND method is not 'dense'\n",
    "                use_stemming_flag = config.get('use_stemming', False)\n",
    "                use_custom_stopwords_flag = config.get('use_custom_stopwords', True)\n",
    "                final_text = preprocess_text(final_text, use_stemming=use_stemming_flag, use_custom_stopwords=use_custom_stopwords_flag)\n",
    "\n",
    "            if not final_text or not final_text.strip():\n",
    "                 cnt += 1\n",
    "            else:\n",
    "                texts.append(final_text)\n",
    "                app_ids.append(doc_id)\n",
    "\n",
    "    if cnt > 0:\n",
    "         print(f\"Number of documents skipped (missing ID/Content or required text part for '{text_type}' or empty after preprocess): {cnt}\")\n",
    "         final_count = len(app_ids)\n",
    "         print(f\"Original corpus size: {len(corpus)}. Final corpus size: {final_count}\")\n",
    "         if final_count == 0:\n",
    "              print(f\"Warning: Resulting corpus for '{text_type}' is empty!\")\n",
    "\n",
    "    corpus_data = [{'id': app_id, 'text': text} for app_id, text in zip(app_ids, texts)]\n",
    "    return corpus_data\n",
    "\n",
    "\n",
    "def create_tfidf_matrix(citing_texts, nonciting_texts, vectorizer=TfidfVectorizer()):\n",
    "    \"\"\"Creates TF-IDF matrix.\"\"\"\n",
    "    all_text = citing_texts + nonciting_texts\n",
    "    print(\"Fitting TF-IDF Vectorizer...\")\n",
    "    vectorizer.fit(tqdm(all_text, desc=\"Fit TF-IDF\", leave=False))\n",
    "    print(\"Transforming Citing Texts...\")\n",
    "    tfidf_matrix_citing = vectorizer.transform(tqdm(citing_texts, desc=\"Transform Citing\", leave=False))\n",
    "    print(\"Transforming Non-Citing Texts...\")\n",
    "    tfidf_matrix_nonciting = vectorizer.transform(tqdm(nonciting_texts, desc=\"Transform Non-Citing\", leave=False))\n",
    "    print(\"Size of vocabulary:\", len(vectorizer.vocabulary_))\n",
    "    return tfidf_matrix_citing, tfidf_matrix_nonciting, vectorizer\n",
    "\n",
    "\n",
    "class BM25Score:\n",
    "    \"\"\"BM25 scoring algorithm implementation.\"\"\"\n",
    "    def __init__(self, vectorized_docs, k1=1.5, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.vectorized_docs = vectorized_docs # Should be non-citing counts\n",
    "\n",
    "    def fit(self, vectorized_queries=None, query_ids=None, args=None):\n",
    "        \"\"\"Fits BM25 based on the non-citing document stats.\"\"\"\n",
    "        if not isinstance(self.vectorized_docs, scipy.sparse.csr_matrix):\n",
    "            try:\n",
    "                self.vectorized_docs = scipy.sparse.csr_matrix(self.vectorized_docs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting BM25 input to CSR: {e}\")\n",
    "                raise\n",
    "\n",
    "        self.n_d = self.vectorized_docs.sum(axis=1).A\n",
    "        self.avgdl = np.mean(self.n_d)\n",
    "        if self.avgdl == 0:\n",
    "            print(\"Warning: Average document length is zero. Setting to 1.\")\n",
    "            self.avgdl = 1.0\n",
    "\n",
    "        self.n_docs = self.vectorized_docs.shape[0]\n",
    "        self.nq = np.array(self.vectorized_docs.getnnz(axis=0)).reshape(1,-1)\n",
    "        epsilon = 1e-9\n",
    "        self.idf = np.log(((self.n_docs - self.nq + 0.5) / (self.nq + 0.5 + epsilon)) + 1.0)\n",
    "        self.idf = np.maximum(self.idf, 0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, vectorized_queries):\n",
    "        \"\"\"Calculates BM25 scores for queries against fitted documents.\"\"\"\n",
    "        if not isinstance(vectorized_queries, scipy.sparse.csr_matrix):\n",
    "            try:\n",
    "                vectorized_queries = scipy.sparse.csr_matrix(vectorized_queries)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting BM25 query input to CSR: {e}\")\n",
    "                raise\n",
    "\n",
    "        if vectorized_queries.shape[1] != self.vectorized_docs.shape[1]:\n",
    "             raise ValueError(f\"Query vector shape {vectorized_queries.shape} incompatible with document vector shape {self.vectorized_docs.shape}\")\n",
    "\n",
    "        idf = self.idf\n",
    "        term_freq_docs = self.vectorized_docs\n",
    "        term_freq_queries = vectorized_queries\n",
    "\n",
    "        doc_len_norm_factor = self.k1 * (1 - self.b + self.b * (self.n_d / self.avgdl))\n",
    "        k1_plus_1 = self.k1 + 1\n",
    "        denominator = term_freq_docs.copy().astype(np.float32)\n",
    "\n",
    "        denominator_dense = term_freq_docs.toarray() + doc_len_norm_factor\n",
    "        denominator_dense[denominator_dense == 0] = 1e-9\n",
    "\n",
    "        score_part_docs = term_freq_docs.multiply(k1_plus_1)\n",
    "        score_part_docs_dense = score_part_docs.toarray() / denominator_dense\n",
    "\n",
    "        weighted_scores = score_part_docs_dense * idf\n",
    "\n",
    "        query_term_presence = (term_freq_queries > 0).astype(np.float32)\n",
    "        final_scores = query_term_presence @ weighted_scores.T\n",
    "\n",
    "        return final_scores\n",
    "\n",
    "\n",
    "def create_bm25_matrix(citing_texts, nonciting_texts, vectorizer=CountVectorizer(), bm25_params={'k1': 1.5, 'b': 0.75}):\n",
    "    \"\"\"Creates BM25 similarity scores.\"\"\"\n",
    "    all_text = citing_texts + nonciting_texts\n",
    "    print(\"Fitting CountVectorizer...\")\n",
    "    vectorizer.fit(tqdm(all_text, desc=\"Fit CV\", leave=False))\n",
    "    print(\"Transforming Citing Texts...\")\n",
    "    count_matrix_citing = vectorizer.transform(tqdm(citing_texts, desc=\"Transform Citing\", leave=False))\n",
    "    print(\"Transforming Non-Citing Texts...\")\n",
    "    count_matrix_nonciting = vectorizer.transform(tqdm(nonciting_texts, desc=\"Transform Non-Citing\", leave=False))\n",
    "    print(\"Size of vocabulary:\", len(vectorizer.vocabulary_))\n",
    "    print(\"Fitting BM25 model...\")\n",
    "    bm25 = BM25Score(count_matrix_nonciting, k1=bm25_params.get('k1', 1.5), b=bm25_params.get('b', 0.75))\n",
    "    bm25.fit()\n",
    "    print(\"Computing BM25 scores...\")\n",
    "    bm25_scores = bm25.predict(count_matrix_citing)\n",
    "    return bm25_scores, vectorizer, bm25\n",
    "\n",
    "\n",
    "def create_dense_embeddings(texts, model_name='multi-qa-mpnet-base-dot-v1', batch_size=64):\n",
    "    \"\"\"Generates dense embeddings for a list of texts using Sentence Transformers.\"\"\"\n",
    "    if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Sentence Transformers not available. Skipping dense embeddings.\")\n",
    "        return None\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device} for embeddings\")\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name, device=device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Sentence Transformer model '{model_name}': {e}\")\n",
    "        return None\n",
    "    print(f\"Generating embeddings using {model_name}...\")\n",
    "    try:\n",
    "        embeddings = model.encode(texts, convert_to_tensor=True, show_progress_bar=True, batch_size=batch_size)\n",
    "        return embeddings.detach().cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Sentence Transformer encoding: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_dense_similarity(citing_embeddings, nonciting_embeddings):\n",
    "    \"\"\"Calculates cosine similarity between two sets of embeddings.\"\"\"\n",
    "    if citing_embeddings is None or nonciting_embeddings is None:\n",
    "        print(\"Cannot calculate dense similarity due to missing embeddings.\")\n",
    "        return None\n",
    "    print(\"Calculating Dense Cosine Similarities...\")\n",
    "    if isinstance(citing_embeddings, torch.Tensor):\n",
    "        citing_embeddings = citing_embeddings.cpu().numpy()\n",
    "    if isinstance(nonciting_embeddings, torch.Tensor):\n",
    "        nonciting_embeddings = nonciting_embeddings.cpu().numpy()\n",
    "    try:\n",
    "        similarity_scores = cosine_similarity(citing_embeddings, nonciting_embeddings)\n",
    "        return similarity_scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating cosine similarity: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_mapping_dict(mapping_df):\n",
    "    \"\"\"Creates dictionary of citing ids to cited ids.\"\"\"\n",
    "    mapping_dict = {}\n",
    "    if not isinstance(mapping_df, pd.DataFrame) or mapping_df.shape[1] < 3:\n",
    "        print(\"Warning: mapping_df invalid in get_mapping_dict.\")\n",
    "        return mapping_dict\n",
    "    for _, row in mapping_df.iterrows():\n",
    "        try:\n",
    "            key = row.iloc[0]\n",
    "            value = row.iloc[2]\n",
    "            if key in mapping_dict:\n",
    "                mapping_dict[key].append(value)\n",
    "            else:\n",
    "                mapping_dict[key] = [value]\n",
    "        except IndexError:\n",
    "            print(f\"Warning: Index error in mapping_df row: {row}\")\n",
    "            continue\n",
    "    return mapping_dict\n",
    "\n",
    "# --- Metrics Functions ---\n",
    "def get_true_and_predicted(citing_to_cited_dict, recommendations_dict):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    not_in_citation_mapping = 0\n",
    "    if not recommendations_dict: return [], [], 0\n",
    "    for citing_id in recommendations_dict.keys():\n",
    "        if citing_id in citing_to_cited_dict:\n",
    "            true_labels.append(citing_to_cited_dict[citing_id])\n",
    "            prediction = recommendations_dict[citing_id]\n",
    "            predicted_labels.append(prediction if isinstance(prediction, list) else [])\n",
    "        else:\n",
    "            not_in_citation_mapping += 1\n",
    "    return true_labels, predicted_labels, not_in_citation_mapping\n",
    "\n",
    "def mean_recall_at_k(true_labels, predicted_labels, k=10):\n",
    "    recalls_at_k = []\n",
    "    if not true_labels or not predicted_labels: return 0.0\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        if not isinstance(true, (list, set)) or not isinstance(pred, list): continue\n",
    "        true_set = set(true)\n",
    "        if not true_set: continue\n",
    "        actual_k = min(k, len(pred))\n",
    "        relevant_count = sum(1 for item in pred[:actual_k] if item in true_set)\n",
    "        recall = relevant_count / len(true_set)\n",
    "        recalls_at_k.append(recall)\n",
    "    mean_recall = sum(recalls_at_k) / len(recalls_at_k) if recalls_at_k else 0\n",
    "    return mean_recall\n",
    "\n",
    "def mean_average_precision(true_labels, predicted_labels, k=10):\n",
    "    average_precisions = []\n",
    "    if not true_labels or not predicted_labels: return 0.0\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        if not isinstance(true, (list, set)) or not isinstance(pred, list): continue\n",
    "        true_set = set(true)\n",
    "        if not true_set: continue\n",
    "        precision_at_k = []\n",
    "        relevant_count = 0\n",
    "        actual_k = min(k, len(pred))\n",
    "        for i, item in enumerate(pred[:actual_k]):\n",
    "            if item in true_set:\n",
    "                relevant_count += 1\n",
    "                precision_at_k.append(relevant_count / (i + 1))\n",
    "        average_precision = sum(precision_at_k) / len(true_set)\n",
    "        average_precisions.append(average_precision)\n",
    "    mean_average_precision_val = sum(average_precisions) / len(average_precisions) if average_precisions else 0\n",
    "    return mean_average_precision_val\n",
    "\n",
    "def mean_ranking(true_labels, predicted_labels):\n",
    "    mean_ranks = []\n",
    "    if not true_labels or not predicted_labels: return float('inf')\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        if not isinstance(true, (list, set)) or not isinstance(pred, list): continue\n",
    "        if not true: continue\n",
    "        ranks = []\n",
    "        pred_list = list(pred) # Ensure it's indexable\n",
    "        max_rank = len(pred_list) + 1\n",
    "        for item in true:\n",
    "            try:\n",
    "                rank = pred_list.index(item) + 1\n",
    "            except ValueError:\n",
    "                rank = max_rank\n",
    "            ranks.append(rank)\n",
    "        mean_rank = sum(ranks) / len(ranks) if ranks else max_rank\n",
    "        mean_ranks.append(mean_rank)\n",
    "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks) if mean_ranks else float('inf')\n",
    "    return mean_of_mean_ranks\n",
    "\n",
    "def top_k_ranks(citing_corpus_data, nonciting_corpus_data, similarity_scores, k=10):\n",
    "    \"\"\"Generates top k ranks dictionary from similarity scores.\"\"\"\n",
    "    top_k_results = {}\n",
    "    if similarity_scores is None or not citing_corpus_data or not nonciting_corpus_data:\n",
    "        print(\"Warning: Cannot generate ranks due to missing scores or corpus data.\")\n",
    "        return top_k_results\n",
    "\n",
    "    num_citing = similarity_scores.shape[0]\n",
    "    num_nonciting = len(nonciting_corpus_data)\n",
    "\n",
    "    if num_citing != len(citing_corpus_data):\n",
    "         print(f\"Warning: Citing scores ({num_citing}) != citing corpus ({len(citing_corpus_data)}). Adjusting...\")\n",
    "         num_citing = min(num_citing, len(citing_corpus_data))\n",
    "\n",
    "    if similarity_scores.shape[1] != num_nonciting:\n",
    "        print(f\"Warning: Similarity score columns ({similarity_scores.shape[1]}) != non-citing docs ({num_nonciting}). Cannot rank.\")\n",
    "        return {}\n",
    "\n",
    "    actual_k = min(k, num_nonciting)\n",
    "    print(f\"Generating top {actual_k} ranks...\")\n",
    "    for i in tqdm(range(num_citing), desc=\"Ranking\", leave=False):\n",
    "        try:\n",
    "            citing_id = citing_corpus_data[i]['id']\n",
    "            patent_scores = similarity_scores[i]\n",
    "            if isinstance(patent_scores, (np.matrix, scipy.sparse.spmatrix)):\n",
    "                patent_scores = patent_scores.toarray().flatten()\n",
    "            elif not isinstance(patent_scores, np.ndarray):\n",
    "                 patent_scores = np.array(patent_scores)\n",
    "\n",
    "            if patent_scores.ndim != 1 or len(patent_scores) != num_nonciting:\n",
    "                 print(f\"Warning: Skipping citing ID {citing_id} due to score shape/length mismatch.\")\n",
    "                 continue\n",
    "\n",
    "            # Argsort returns indices of the smallest values, so negate for descending order\n",
    "            top_indices = np.argsort(-patent_scores)[:actual_k] # Negate scores\n",
    "            top_nonciting_ids = [nonciting_corpus_data[j]['id'] for j in top_indices if j < num_nonciting]\n",
    "            top_k_results[citing_id] = top_nonciting_ids\n",
    "        except IndexError as e:\n",
    "            print(f\"Warning: Index error processing citing item {i} (ID: {citing_corpus_data[i].get('id', 'N/A')}). Skipping. Error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Unexpected error processing citing item {i} (ID: {citing_corpus_data[i].get('id', 'N/A')}): {e}. Skipping.\")\n",
    "    return top_k_results\n",
    "\n",
    "\n",
    "def combine_rankings_rrf(rank_dict_list, k_rrf=60):\n",
    "    \"\"\"Combines multiple ranking dictionaries using Reciprocal Rank Fusion (RRF).\"\"\"\n",
    "    print(f\"Combining {len(rank_dict_list)} rankings using RRF (k={k_rrf})...\")\n",
    "    if not rank_dict_list or len(rank_dict_list) < 2:\n",
    "        print(\"Warning: Need at least two ranking lists for RRF.\")\n",
    "        return rank_dict_list[0] if rank_dict_list else {}\n",
    "\n",
    "    query_ids = set(rank_dict_list[0].keys())\n",
    "    for r_dict in rank_dict_list[1:]:\n",
    "        query_ids.intersection_update(r_dict.keys())\n",
    "    if not query_ids:\n",
    "        print(\"Warning: No common query IDs found among ranking lists for RRF.\")\n",
    "        return {}\n",
    "\n",
    "    combined_scores = {query_id: {} for query_id in query_ids}\n",
    "    print(f\"Processing {len(query_ids)} common queries for RRF.\")\n",
    "\n",
    "    for ranks_dict in tqdm(rank_dict_list, desc=\"Processing Rank Lists\", leave=False):\n",
    "        for query_id in query_ids:\n",
    "            ranked_docs = ranks_dict.get(query_id, [])\n",
    "            for rank, doc_id in enumerate(ranked_docs):\n",
    "                rank_score = 1.0 / (k_rrf + rank + 1)\n",
    "                combined_scores[query_id][doc_id] = combined_scores[query_id].get(doc_id, 0) + rank_score\n",
    "\n",
    "    final_rankings = {}\n",
    "    for query_id, doc_scores in tqdm(combined_scores.items(), desc=\"Sorting RRF Results\", leave=False):\n",
    "        sorted_docs = sorted(doc_scores.items(), key=lambda item: (-item[1], item[0]))\n",
    "        final_rankings[query_id] = [doc_id for doc_id, score in sorted_docs]\n",
    "\n",
    "    return final_rankings\n",
    "\n",
    "\n",
    "# --- Function to run experiments and evaluate on training data ---\n",
    "# DEFINE run_experiment HERE\n",
    "def run_experiment(config, json_citing_train, json_nonciting, mapping_dict, k_eval=100):\n",
    "    \"\"\"Runs a single experiment configuration and returns metrics.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n--- Running Experiment: {config['name']} ---\")\n",
    "\n",
    "    if config['method'] == 'dense' and not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Skipping dense experiment as libraries are not available.\")\n",
    "        return None, None\n",
    "\n",
    "    print(\"Creating corpora...\")\n",
    "    citing_corpus = create_corpus(json_citing_train, config['text_type'], preprocess=config.get('preprocess', False), config=config)\n",
    "    nonciting_corpus = create_corpus(json_nonciting, config['text_type'], preprocess=config.get('preprocess', False), config=config)\n",
    "\n",
    "    if not citing_corpus or not nonciting_corpus:\n",
    "        print(\"Skipping experiment due to empty corpus.\")\n",
    "        return None, None\n",
    "\n",
    "    citing_texts = [doc['text'] for doc in citing_corpus]\n",
    "    nonciting_texts = [doc['text'] for doc in nonciting_corpus]\n",
    "\n",
    "    similarity_scores = None\n",
    "    fitted_vectorizer = None\n",
    "    fitted_bm25_model = None\n",
    "    nonciting_matrix_tfidf = None\n",
    "    nonciting_embeddings = None\n",
    "    model_details_run = {} # Initialize dictionary to hold model details\n",
    "\n",
    "    try:\n",
    "        if config['method'] == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(**config.get('vectorizer_params', {}))\n",
    "            tfidf_citing, tfidf_nonciting, fitted_vectorizer = create_tfidf_matrix(\n",
    "                citing_texts, nonciting_texts, vectorizer\n",
    "            )\n",
    "            print(\"Calculating Cosine Similarities...\")\n",
    "            similarity_scores = linear_kernel(tfidf_citing, tfidf_nonciting)\n",
    "            nonciting_matrix_tfidf = tfidf_nonciting\n",
    "\n",
    "        elif config['method'] == 'bm25':\n",
    "            vectorizer = CountVectorizer(**config.get('vectorizer_params', {}))\n",
    "            bm25_scores, fitted_vectorizer, fitted_bm25_model = create_bm25_matrix(\n",
    "                citing_texts, nonciting_texts, vectorizer, config.get('bm25_params', {})\n",
    "            )\n",
    "            similarity_scores = bm25_scores\n",
    "\n",
    "        elif config['method'] == 'dense':\n",
    "            print(\"Generating Dense Embeddings...\")\n",
    "            citing_embeddings = create_dense_embeddings(\n",
    "                citing_texts,\n",
    "                model_name=config.get('embedding_model'),\n",
    "                batch_size=config.get('embedding_batch_size')\n",
    "            )\n",
    "            nonciting_embeddings = create_dense_embeddings(\n",
    "                nonciting_texts,\n",
    "                model_name=config.get('embedding_model'),\n",
    "                batch_size=config.get('embedding_batch_size')\n",
    "            )\n",
    "            if citing_embeddings is None or nonciting_embeddings is None:\n",
    "                 raise ValueError(\"Dense embedding generation failed.\")\n",
    "            similarity_scores = calculate_dense_similarity(citing_embeddings, nonciting_embeddings)\n",
    "            model_details_run['nonciting_embeddings'] = nonciting_embeddings # Store needed embeddings\n",
    "\n",
    "        else:\n",
    "            print(f\"Unknown method: {config['method']}\")\n",
    "            return None, None\n",
    "\n",
    "        if similarity_scores is None:\n",
    "            raise ValueError(\"Failed to compute similarity scores.\")\n",
    "\n",
    "        print(f\"Shape of similarity/scores matrix: {similarity_scores.shape}\")\n",
    "\n",
    "        # Get full ranking first\n",
    "        full_rank = top_k_ranks(citing_corpus, nonciting_corpus, similarity_scores, k=len(nonciting_corpus))\n",
    "\n",
    "        # Store ranks if this config is needed for RRF\n",
    "        if config['name'] == best_bm25_config_name_for_rrf:\n",
    "            print(f\"Storing BM25 (Best MAP/Recall) ranks for RRF from {config['name']}...\")\n",
    "            global best_bm25_ranks_train\n",
    "            best_bm25_ranks_train = full_rank\n",
    "        elif config['name'] == best_dense_config_name_for_rrf:\n",
    "            print(f\"Storing Dense ranks for RRF from {config['name']}...\")\n",
    "            global best_dense_ranks_train\n",
    "            best_dense_ranks_train = full_rank\n",
    "        elif config['name'] == best_mean_rank_bm25_config_name:\n",
    "            print(f\"Storing BM25 (Best Mean Rank) ranks for RRF from {config['name']}...\")\n",
    "            global best_mean_rank_bm25_ranks_train\n",
    "            best_mean_rank_bm25_ranks_train = full_rank\n",
    "\n",
    "        # Trim ranks for evaluation\n",
    "        top_k_rank_eval = {qid: ranks[:k_eval] for qid, ranks in full_rank.items()}\n",
    "        print(\"Calculating metrics...\")\n",
    "        true_labels, predicted_labels, not_in_mapping = get_true_and_predicted(mapping_dict, top_k_rank_eval)\n",
    "\n",
    "        if not predicted_labels:\n",
    "            print(\"No predictions generated for metric calculation.\")\n",
    "            metrics = {'recall@10': 0,'recall@20': 0,'recall@50': 0,'recall@100': 0, 'map@100': 0, 'mean_rank': float('inf'), 'num_measured': 0, 'not_in_mapping': not_in_mapping}\n",
    "        else:\n",
    "            metrics = {\n",
    "                'recall@10': mean_recall_at_k(true_labels, predicted_labels, k=10),\n",
    "                'recall@20': mean_recall_at_k(true_labels, predicted_labels, k=20),\n",
    "                'recall@50': mean_recall_at_k(true_labels, predicted_labels, k=50),\n",
    "                'recall@100': mean_recall_at_k(true_labels, predicted_labels, k=100),\n",
    "                'map@100': mean_average_precision(true_labels, predicted_labels, k=100),\n",
    "                'mean_rank': mean_ranking(true_labels, predicted_labels),\n",
    "                'num_measured': len(predicted_labels), 'not_in_mapping': not_in_mapping\n",
    "            }\n",
    "\n",
    "        print(f\"Recall@10: {metrics['recall@10']:.4f}\")\n",
    "        print(f\"Recall@100: {metrics['recall@100']:.4f}\")\n",
    "        print(f\"MAP@100: {metrics['map@100']:.4f}\")\n",
    "        print(f\"Mean Rank: {metrics['mean_rank']:.4f}\")\n",
    "\n",
    "        # Populate model_details_run consistently\n",
    "        model_details_run.update({\n",
    "            'vectorizer': fitted_vectorizer, # None for dense\n",
    "            'bm25_model': fitted_bm25_model, # None for tfidf/dense\n",
    "            'nonciting_corpus': nonciting_corpus,\n",
    "            'nonciting_matrix': nonciting_matrix_tfidf, # None for bm25/dense\n",
    "            'nonciting_embeddings': model_details_run.get('nonciting_embeddings', None) # Added if dense\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during experiment '{config['name']}': {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Experiment '{config['name']}' completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    return metrics, model_details_run\n",
    "\n",
    "\n",
    "# # 1.0 Load Datasets\n",
    "print(\"\\nLoading datasets...\")\n",
    "# --- Define paths ---\n",
    "DATA_DIR = \"./datasets\"\n",
    "content_path = os.path.join(DATA_DIR, \"Content_JSONs\")\n",
    "citation_path = os.path.join(DATA_DIR, \"Citation_JSONs\")\n",
    "path_citing_train = os.path.join(content_path, \"Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TRAIN.json\")\n",
    "path_citing_test = os.path.join(content_path, \"Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TEST.json\")\n",
    "path_nonciting = os.path.join(content_path, \"Cited_2020_Uncited_2010-2019_Cleaned_Content_22k/CLEANED_CONTENT_DATASET_cited_patents_by_2020_uncited_2010-2019.json\")\n",
    "path_citations = os.path.join(citation_path, \"Citation_Train.json\")\n",
    "\n",
    "# --- Load data ---\n",
    "json_citing_train = load_json_data(path_citing_train)\n",
    "json_citing_test = load_json_data(path_citing_test)\n",
    "json_nonciting = load_json_data(path_nonciting)\n",
    "json_citing_to_cited = load_json_data(path_citations)\n",
    "\n",
    "if not all([json_citing_train, json_citing_test, json_nonciting, json_citing_to_cited]):\n",
    "    print(\"\\nCritical Error: One or more dataset files failed to load. Please check paths. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\nDatasets loaded successfully.\")\n",
    "print(f\"Citing Train: {len(json_citing_train)}\")\n",
    "print(f\"Citing Test: {len(json_citing_test)}\")\n",
    "print(f\"Non-Citing Pool: {len(json_nonciting)}\")\n",
    "print(f\"Training Citations Raw Pairs: {len(json_citing_to_cited)}\")\n",
    "\n",
    "mapping_dataset_df = pd.DataFrame(json_citing_to_cited)\n",
    "mapping_dict = get_mapping_dict(mapping_dataset_df)\n",
    "print(f\"Training Citations Dict (Unique Citing Patents): {len(mapping_dict)}\")\n",
    "\n",
    "# # 2.0 Experiments Setup\n",
    "print(\"\\nSetting up experiments...\")\n",
    "\n",
    "# --- Define configurations ---\n",
    "# Define base names for RRF components FIRST\n",
    "best_bm25_config_name_for_rrf = 'T+A+Claims BM25 (Pre, ngram=1, k1=2.0, b=0.9)'\n",
    "best_dense_config_name_for_rrf = 'Dense (multi-qa-mpnet, T+A+Claims)'\n",
    "best_mean_rank_bm25_config_name = 'T+A+Claims BM25 (Pre, ngram=1, k1=2.5, b=0.8)'\n",
    "\n",
    "configs = [\n",
    "    # {'name': 'Title BM25', 'method': 'bm25', 'text_type': 'title', 'preprocess': False, 'vectorizer_params': {'stop_words': 'english', 'max_features': 10000}, 'bm25_params': {'k1': 1.5, 'b': 0.75}},\n",
    "    # {'name': 'Claim1 BM25', 'method': 'bm25', 'text_type': 'claim1', 'preprocess': False, 'vectorizer_params': {'stop_words': 'english', 'max_features': 10000}, 'bm25_params': {'k1': 1.5, 'b': 0.75}},\n",
    "\n",
    "    # {'name': best_bm25_config_name_for_rrf,\n",
    "    #  'method': 'bm25', 'text_type': 'title_abstract_claims',\n",
    "    #  'preprocess': True, 'use_stemming': False, 'use_custom_stopwords': True,\n",
    "    #  'vectorizer_params': {'max_features': 20000, 'ngram_range': (1, 1), 'min_df': 1},\n",
    "    #  'bm25_params': {'k1': 2.0, 'b': 0.9}},\n",
    "    # {'name': best_mean_rank_bm25_config_name,\n",
    "    #  'method': 'bm25', 'text_type': 'title_abstract_claims',\n",
    "    #  'preprocess': True, 'use_stemming': False, 'use_custom_stopwords': True,\n",
    "    #  'vectorizer_params': {'max_features': 20000, 'ngram_range': (1, 1), 'min_df': 1},\n",
    "    #  'bm25_params': {'k1': 2.5, 'b': 0.8}},\n",
    "\n",
    "    {'name': best_dense_config_name_for_rrf,\n",
    "     'method': 'dense', 'text_type': 'title_abstract_claims', 'preprocess': False,\n",
    "     'embedding_model': 'multi-qa-mpnet-base-dot-v1', 'embedding_batch_size': 128 },\n",
    "    {'name': 'Dense (PatentSBERTa, T+A+Claims)', # Keep this to compare dense models\n",
    "     'method': 'dense', 'text_type': 'title_abstract_claims', 'preprocess': False,\n",
    "     'embedding_model': 'AI-Growth-Lab/PatentSBERTa', 'embedding_batch_size': 64 },\n",
    "]\n",
    "\n",
    "# Filter out dense methods if library not available\n",
    "if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    print(\"\\nSentence Transformers not available, removing Dense configurations.\")\n",
    "    configs = [c for c in configs if c['method'] != 'dense']\n",
    "\n",
    "results = {}\n",
    "best_recall_100 = -1.0\n",
    "best_map_100 = -1.0\n",
    "best_config_name_recall = None\n",
    "best_config_name_map = None\n",
    "best_model_details = {} # Details of the best SINGLE model by MAP\n",
    "best_model_config = None # Config of the best SINGLE model by MAP\n",
    "\n",
    "# Initialize rank storage\n",
    "best_bm25_ranks_train = None\n",
    "best_dense_ranks_train = None\n",
    "best_mean_rank_bm25_ranks_train = None\n",
    "\n",
    "\n",
    "# --- Run Experiments ---\n",
    "print(\"\\n=== Running Experiments on Training Data ===\")\n",
    "k_eval_metrics = 100\n",
    "\n",
    "if not all([json_citing_train, json_nonciting, mapping_dict]):\n",
    "     print(\"Cannot run experiments, datasets not loaded properly.\")\n",
    "else:\n",
    "    for config in configs:\n",
    "        metrics, model_details_run = run_experiment(config, json_citing_train, json_nonciting, mapping_dict, k_eval=k_eval_metrics)\n",
    "        if metrics:\n",
    "            results[config['name']] = metrics\n",
    "            current_recall_100 = metrics['recall@100']\n",
    "            current_map_100 = metrics['map@100']\n",
    "\n",
    "            if current_map_100 > best_map_100:\n",
    "                 best_map_100 = current_map_100\n",
    "                 best_config_name_map = config['name']\n",
    "                 best_model_details = model_details_run\n",
    "                 best_model_config = config\n",
    "                 print(f\"*** New best MAP@100 model found: {best_config_name_map} ({best_map_100:.4f}) ***\")\n",
    "\n",
    "            if current_recall_100 > best_recall_100:\n",
    "                 best_recall_100 = current_recall_100\n",
    "                 best_config_name_recall = config['name']\n",
    "                 if config['name'] == best_config_name_map:\n",
    "                     best_model_details = model_details_run\n",
    "                     best_model_config = config\n",
    "                 print(f\"*** New best Recall@100 model found: {best_config_name_recall} ({best_recall_100:.4f}) ***\")\n",
    "        else:\n",
    "            print(f\"--- Experiment {config['name']} failed or produced no results. ---\")\n",
    "\n",
    "\n",
    "# --- Evaluate RRF on Training Data ---\n",
    "print(\"\\n=== Evaluating Hybrid RRF Variants on Training Data ===\")\n",
    "rrf_results = {}\n",
    "best_rrf_map = -1.0\n",
    "best_rrf_config_details = {}\n",
    "\n",
    "# Check if necessary ranks were captured\n",
    "rrf_possible_best_map = best_bm25_ranks_train is not None and best_dense_ranks_train is not None\n",
    "rrf_possible_best_mean_rank = best_mean_rank_bm25_ranks_train is not None and best_dense_ranks_train is not None\n",
    "\n",
    "if not rrf_possible_best_map: print(f\"Warning: Ranks missing for BM25 MAP ('{best_bm25_config_name_for_rrf}') or Dense ('{best_dense_config_name_for_rrf}'). Cannot run primary RRF.\")\n",
    "if not rrf_possible_best_mean_rank: print(f\"Warning: Ranks missing for BM25 Mean Rank ('{best_mean_rank_bm25_config_name}') or Dense ('{best_dense_config_name_for_rrf}'). Cannot run alternative RRF.\")\n",
    "\n",
    "# Variant 1: Best MAP/Recall BM25 + Best Dense, tune RRF k\n",
    "if rrf_possible_best_map:\n",
    "    base_rank_list = [best_bm25_ranks_train, best_dense_ranks_train]\n",
    "    bm25_map_val = results.get(best_bm25_config_name_for_rrf, {}).get('map@100', 0)\n",
    "    dense_map_val = results.get(best_dense_config_name_for_rrf, {}).get('map@100', 0)\n",
    "    base_component_names = f\"BM25(MAP={bm25_map_val:.3f}) + Dense(MAP={dense_map_val:.3f})\"\n",
    "\n",
    "    for rrf_k_val in [10, 60, 120]:\n",
    "        rrf_name = f\"RRF (k={rrf_k_val}, {base_component_names})\"\n",
    "        print(f\"\\nEvaluating: {rrf_name}\")\n",
    "        try:\n",
    "            rrf_combined_ranks = combine_rankings_rrf(base_rank_list, k_rrf=rrf_k_val)\n",
    "            true_labels_rrf, predicted_labels_rrf, not_in_mapping_rrf = get_true_and_predicted(mapping_dict, rrf_combined_ranks)\n",
    "            if not predicted_labels_rrf: raise ValueError(\"No predictions from RRF combine.\")\n",
    "\n",
    "            metrics = {\n",
    "                'recall@10': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=10),\n",
    "                'recall@20': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=20),\n",
    "                'recall@50': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=50),\n",
    "                'recall@100': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=100),\n",
    "                'map@100': mean_average_precision(true_labels_rrf, predicted_labels_rrf, k=100),\n",
    "                'mean_rank': mean_ranking(true_labels_rrf, predicted_labels_rrf),\n",
    "                'num_measured': len(predicted_labels_rrf), 'not_in_mapping': not_in_mapping_rrf\n",
    "            }\n",
    "            rrf_results[rrf_name] = metrics\n",
    "            print(f\"  RRF Metrics: R@100={metrics['recall@100']:.4f}, MAP@100={metrics['map@100']:.4f}, MeanRank={metrics['mean_rank']:.2f}\")\n",
    "\n",
    "            if metrics['map@100'] > best_rrf_map:\n",
    "                best_rrf_map = metrics['map@100']\n",
    "                best_rrf_config_details = {\n",
    "                    'name': rrf_name, 'k': rrf_k_val,\n",
    "                    'bm25_config_name': best_bm25_config_name_for_rrf,\n",
    "                    'dense_config_name': best_dense_config_name_for_rrf,\n",
    "                    'metrics': metrics}\n",
    "                print(f\"  *** New best RRF configuration found: {rrf_name} (MAP@100: {best_rrf_map:.4f}) ***\")\n",
    "        except Exception as e: print(f\"Error evaluating {rrf_name}: {e}\")\n",
    "\n",
    "# Variant 2: Best Mean Rank BM25 + Best Dense, k=60\n",
    "if rrf_possible_best_mean_rank:\n",
    "    alt_rank_list = [best_mean_rank_bm25_ranks_train, best_dense_ranks_train]\n",
    "    bm25_mr_val = results.get(best_mean_rank_bm25_config_name,{}).get('mean_rank', float('inf'))\n",
    "    dense_map_val = results.get(best_dense_config_name_for_rrf,{}).get('map@100',0)\n",
    "    alt_component_names = f\"BM25(MR={bm25_mr_val:.2f}) + Dense(MAP={dense_map_val:.3f})\"\n",
    "    rrf_k_val = 60\n",
    "    rrf_name = f\"RRF (k={rrf_k_val}, {alt_component_names})\"\n",
    "    print(f\"\\nEvaluating: {rrf_name}\")\n",
    "    try:\n",
    "        rrf_combined_ranks = combine_rankings_rrf(alt_rank_list, k_rrf=rrf_k_val)\n",
    "        true_labels_rrf, predicted_labels_rrf, not_in_mapping_rrf = get_true_and_predicted(mapping_dict, rrf_combined_ranks)\n",
    "        if not predicted_labels_rrf: raise ValueError(\"No predictions from RRF combine.\")\n",
    "\n",
    "        metrics = {\n",
    "            'recall@10': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=10),\n",
    "            'recall@20': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=20),\n",
    "            'recall@50': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=50),\n",
    "            'recall@100': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=100),\n",
    "            'map@100': mean_average_precision(true_labels_rrf, predicted_labels_rrf, k=100),\n",
    "            'mean_rank': mean_ranking(true_labels_rrf, predicted_labels_rrf),\n",
    "            'num_measured': len(predicted_labels_rrf), 'not_in_mapping': not_in_mapping_rrf\n",
    "        }\n",
    "        rrf_results[rrf_name] = metrics\n",
    "        print(f\"  RRF Metrics: R@100={metrics['recall@100']:.4f}, MAP@100={metrics['map@100']:.4f}, MeanRank={metrics['mean_rank']:.2f}\")\n",
    "\n",
    "        if metrics['map@100'] > best_rrf_map:\n",
    "            best_rrf_map = metrics['map@100']\n",
    "            best_rrf_config_details = {\n",
    "                'name': rrf_name, 'k': rrf_k_val,\n",
    "                'bm25_config_name': best_mean_rank_bm25_config_name, # Use the mean rank one\n",
    "                'dense_config_name': best_dense_config_name_for_rrf,\n",
    "                'metrics': metrics}\n",
    "            print(f\"  *** New best RRF configuration found: {rrf_name} (MAP@100: {best_rrf_map:.4f}) ***\")\n",
    "    except Exception as e: print(f\"Error evaluating {rrf_name}: {e}\")\n",
    "\n",
    "# --- Determine final best prediction method ---\n",
    "print(\"\\n--- Determining Best Prediction Method ---\")\n",
    "best_method_for_prediction = None\n",
    "final_prediction_config = None\n",
    "\n",
    "# Use MAP@100 as the primary decision metric\n",
    "if best_rrf_map > best_map_100 and best_rrf_config_details:\n",
    "    print(f\"Best method is RRF: '{best_rrf_config_details['name']}' (MAP@100: {best_rrf_map:.4f})\")\n",
    "    best_method_for_prediction = 'rrf'\n",
    "    final_prediction_config = best_rrf_config_details # Store RRF details\n",
    "elif best_config_name_map:\n",
    "    print(f\"Best method is Single Model: '{best_config_name_map}' (MAP@100: {best_map_100:.4f})\")\n",
    "    best_method_for_prediction = best_config_name_map\n",
    "    final_prediction_config = best_model_config # Config dict of the best single model\n",
    "    if final_prediction_config: final_prediction_config['details'] = best_model_details # Attach fitted objects\n",
    "else:\n",
    "    print(\"Warning: Could not determine best method. Check experiment results and logs.\")\n",
    "\n",
    "# Add RRF results to the main results dictionary\n",
    "results.update(rrf_results)\n",
    "\n",
    "# --- Plot Results ---\n",
    "print(\"\\n=== Experiment Results Summary ===\")\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results).T.sort_values(by='map@100', ascending=False) # Sort by MAP\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    print(results_df)\n",
    "    pd.reset_option('display.max_rows')\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    k_values_plot = [10, 20, 50, 100]\n",
    "    sorted_results_plot = sorted(results.items(), key=lambda item: item[1].get('recall@100', 0), reverse=True)\n",
    "    for name, metrics_res in sorted_results_plot:\n",
    "        recalls = [metrics_res.get(f'recall@{k}', 0) for k in k_values_plot]\n",
    "        if any(not isinstance(r, (int, float)) for r in recalls): continue\n",
    "        plt.plot(k_values_plot, recalls, label=f\"{name} (MAP@100: {metrics_res.get('map@100', 0):.3f})\", marker='o', linewidth=1.5, markersize=5)\n",
    "\n",
    "    plt.xlabel('Top K')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Recall@K Comparison of Methods (Train Set)')\n",
    "    plt.xticks(k_values_plot)\n",
    "    plt.legend(loc='best', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.tight_layout(rect=[0, 0, 0.75, 1])\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to display.\")\n",
    "\n",
    "\n",
    "# # 4.0 Get Test Predictions for CodaBench using Best Approach\n",
    "print(\"\\n=== Generating Test Predictions for CodaBench ===\")\n",
    "\n",
    "if not best_method_for_prediction:\n",
    "     print(\"Error: No best method determined. Cannot generate predictions.\")\n",
    "     exit()\n",
    "\n",
    "print(f\"Selected approach for final prediction: {best_method_for_prediction}\")\n",
    "\n",
    "k_submission = 100\n",
    "test_predictions = None\n",
    "output_filename = 'prediction1.json'\n",
    "\n",
    "if best_method_for_prediction == 'rrf':\n",
    "    # --- RRF Prediction Workflow ---\n",
    "    print(\"\\nGenerating RRF predictions for test set...\")\n",
    "    if not final_prediction_config or 'bm25_config_name' not in final_prediction_config or 'dense_config_name' not in final_prediction_config:\n",
    "         print(\"Error: RRF selected, but configuration details are missing.\")\n",
    "    else:\n",
    "        config_bm25 = next((c for c in configs if c['name'] == final_prediction_config['bm25_config_name']), None)\n",
    "        config_dense = next((c for c in configs if c['name'] == final_prediction_config['dense_config_name']), None)\n",
    "        rrf_k_val = final_prediction_config['k']\n",
    "\n",
    "        if not config_bm25 or not config_dense:\n",
    "            print(\"Error: Could not find original configurations for RRF components. Cannot proceed.\")\n",
    "        elif not SENTENCE_TRANSFORMERS_AVAILABLE and config_dense['method'] == 'dense':\n",
    "             print(\"Error: RRF requires dense model, but sentence-transformers is not available.\")\n",
    "        else:\n",
    "            try:\n",
    "                # A. Prepare Corpora\n",
    "                print(\"Creating corpora for RRF test prediction...\")\n",
    "                citing_corpus_test_bm25 = create_corpus(json_citing_test, config_bm25['text_type'], preprocess=True, config=config_bm25)\n",
    "                citing_texts_test_bm25 = [doc['text'] for doc in citing_corpus_test_bm25]\n",
    "                nonciting_corpus_bm25 = create_corpus(json_nonciting, config_bm25['text_type'], preprocess=True, config=config_bm25)\n",
    "                nonciting_texts_bm25 = [doc['text'] for doc in nonciting_corpus_bm25]\n",
    "\n",
    "                citing_corpus_test_dense = create_corpus(json_citing_test, config_dense['text_type'], preprocess=False, config=config_dense)\n",
    "                citing_texts_test_dense = [doc['text'] for doc in citing_corpus_test_dense]\n",
    "                nonciting_corpus_dense = create_corpus(json_nonciting, config_dense['text_type'], preprocess=False, config=config_dense)\n",
    "                nonciting_texts_dense = [doc['text'] for doc in nonciting_corpus_dense]\n",
    "\n",
    "                if not all([citing_corpus_test_bm25, nonciting_corpus_bm25, citing_corpus_test_dense, nonciting_corpus_dense]):\n",
    "                    raise ValueError(\"One or more corpora creation failed for RRF.\")\n",
    "\n",
    "                # B. Get BM25 Ranks for Test Set\n",
    "                print(f\"\\nCalculating BM25 scores for test set (using {config_bm25['name']} settings)...\")\n",
    "                train_corpus_bm25 = create_corpus(json_citing_train, config_bm25['text_type'], preprocess=True, config=config_bm25)\n",
    "                all_train_texts_bm25 = [d['text'] for d in train_corpus_bm25] + nonciting_texts_bm25\n",
    "                bm25_vectorizer = CountVectorizer(**config_bm25['vectorizer_params'])\n",
    "                bm25_vectorizer.fit(tqdm(all_train_texts_bm25, desc=\"Fit BM25 Vectorizer\", leave=False))\n",
    "                test_citing_counts = bm25_vectorizer.transform(tqdm(citing_texts_test_bm25, desc=\"Transform Test Citing (BM25)\"))\n",
    "                test_nonciting_counts = bm25_vectorizer.transform(tqdm(nonciting_texts_bm25, desc=\"Transform Non-Citing (BM25)\"))\n",
    "                bm25_model_test = BM25Score(test_nonciting_counts, **config_bm25['bm25_params'])\n",
    "                bm25_model_test.fit()\n",
    "                test_bm25_scores = bm25_model_test.predict(test_citing_counts)\n",
    "                print(f\"Shape of test BM25 scores matrix: {test_bm25_scores.shape}\")\n",
    "                test_bm25_ranks = top_k_ranks(citing_corpus_test_bm25, nonciting_corpus_bm25, test_bm25_scores, k=max(k_submission * 2, 500)) # Increase candidate pool size\n",
    "\n",
    "                # C. Get Dense Ranks for Test Set\n",
    "                print(f\"\\nCalculating Dense embeddings/similarities for test set (using {config_dense['name']} settings)...\")\n",
    "                test_citing_embed = create_dense_embeddings(citing_texts_test_dense, model_name=config_dense['embedding_model'], batch_size=config_dense['embedding_batch_size'])\n",
    "                test_nonciting_embed = create_dense_embeddings(nonciting_texts_dense, model_name=config_dense['embedding_model'], batch_size=config_dense['embedding_batch_size'])\n",
    "                if test_citing_embed is None or test_nonciting_embed is None: raise ValueError(\"Dense embedding failed for test.\")\n",
    "                test_dense_sim = calculate_dense_similarity(test_citing_embed, test_nonciting_embed)\n",
    "                if test_dense_sim is None: raise ValueError(\"Dense similarity failed for test.\")\n",
    "                print(f\"Shape of test Dense similarity matrix: {test_dense_sim.shape}\")\n",
    "                test_dense_ranks = top_k_ranks(citing_corpus_test_dense, nonciting_corpus_dense, test_dense_sim, k=max(k_submission * 2, 500)) # Increase candidate pool size\n",
    "\n",
    "                # D. Combine Ranks using RRF with the best k\n",
    "                print(f\"\\nCombining test rankings using RRF (k={rrf_k_val})...\")\n",
    "                common_test_citing_ids = set(test_bm25_ranks.keys()).intersection(test_dense_ranks.keys())\n",
    "                if len(common_test_citing_ids) < len(citing_corpus_test_bm25): # Check if we lost test queries\n",
    "                     print(f\"Warning: Mismatch in test citing IDs between BM25 ({len(test_bm25_ranks)}) and Dense ({len(test_dense_ranks)}). Using {len(common_test_citing_ids)} common IDs.\")\n",
    "                rank_list_for_rrf = [\n",
    "                    {qid: ranks for qid, ranks in test_bm25_ranks.items() if qid in common_test_citing_ids},\n",
    "                    {qid: ranks for qid, ranks in test_dense_ranks.items() if qid in common_test_citing_ids}\n",
    "                ]\n",
    "                test_predictions_rrf_combined = combine_rankings_rrf(rank_list_for_rrf, k_rrf=rrf_k_val)\n",
    "\n",
    "                # E. Trim to final k for submission\n",
    "                test_predictions = {qid: ranks[:k_submission] for qid, ranks in test_predictions_rrf_combined.items()}\n",
    "                print(f\"Generated RRF predictions for {len(test_predictions)} test patents.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during RRF test prediction generation: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                test_predictions = None # Ensure None on error\n",
    "\n",
    "elif best_method_for_prediction and final_prediction_config: # Fallback to best single model\n",
    "    print(f\"\\nGenerating predictions using best single model: {best_method_for_prediction}\")\n",
    "    best_config = final_prediction_config\n",
    "    single_model_details = best_config.get('details', {})\n",
    "\n",
    "    if not single_model_details:\n",
    "         print(f\"Error: Details (fitted models) for the best single model '{best_method_for_prediction}' are missing.\")\n",
    "    elif best_config['method'] == 'dense' and not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "         print(\"Error: Best single model is dense, but sentence-transformers not available.\")\n",
    "    else:\n",
    "        try:\n",
    "            print(\"Creating test citing corpus...\")\n",
    "            citing_corpus_test = create_corpus(json_citing_test, best_config['text_type'], preprocess=best_config.get('preprocess', False), config=best_config)\n",
    "            citing_texts_test = [doc['text'] for doc in citing_corpus_test]\n",
    "\n",
    "            # Retrieve components from the *training run* details stored in best_config\n",
    "            fitted_vectorizer = single_model_details.get('vectorizer')\n",
    "            fitted_bm25_model = single_model_details.get('bm25_model')\n",
    "            nonciting_corpus_for_ranking = single_model_details.get('nonciting_corpus')\n",
    "            nonciting_matrix_tfidf = single_model_details.get('nonciting_matrix')\n",
    "            nonciting_embeddings = single_model_details.get('nonciting_embeddings')\n",
    "\n",
    "            if not citing_corpus_test or not nonciting_corpus_for_ranking:\n",
    "                print(\"Test citing corpus or non-citing corpus for ranking is missing/empty.\")\n",
    "            else:\n",
    "                test_similarity_scores = None\n",
    "                print(f\"Applying method: {best_config['method']}\")\n",
    "                if best_config['method'] == 'tfidf':\n",
    "                     if fitted_vectorizer and nonciting_matrix_tfidf is not None:\n",
    "                         citing_matrix_test = fitted_vectorizer.transform(tqdm(citing_texts_test, desc=\"Transform Test Citing (TFIDF)\"))\n",
    "                         test_similarity_scores = linear_kernel(citing_matrix_test, nonciting_matrix_tfidf)\n",
    "                     else: print(\"Error: Missing components for TF-IDF prediction.\")\n",
    "                elif best_config['method'] == 'bm25':\n",
    "                     if fitted_vectorizer and fitted_bm25_model:\n",
    "                         citing_matrix_test = fitted_vectorizer.transform(tqdm(citing_texts_test, desc=\"Transform Test Citing (BM25)\"))\n",
    "                         test_similarity_scores = fitted_bm25_model.predict(citing_matrix_test)\n",
    "                     else: print(\"Error: Missing components for BM25 prediction.\")\n",
    "                elif best_config['method'] == 'dense':\n",
    "                     if nonciting_embeddings is not None:\n",
    "                         citing_embeddings_test = create_dense_embeddings(\n",
    "                             citing_texts_test,\n",
    "                             model_name=best_config.get('embedding_model'),\n",
    "                             batch_size=best_config.get('embedding_batch_size')\n",
    "                         )\n",
    "                         if citing_embeddings_test is not None:\n",
    "                              test_similarity_scores = calculate_dense_similarity(citing_embeddings_test, nonciting_embeddings)\n",
    "                         else: print(\"Error generating test dense embeddings.\")\n",
    "                     else: print(\"Error: Missing non-citing embeddings for dense prediction.\")\n",
    "\n",
    "                if test_similarity_scores is not None:\n",
    "                    print(f\"Shape of test similarity/scores matrix: {test_similarity_scores.shape}\")\n",
    "                    test_predictions = top_k_ranks(citing_corpus_test, nonciting_corpus_for_ranking, test_similarity_scores, k=k_submission)\n",
    "                    print(f\"Generated single model predictions for {len(test_predictions)} test patents.\")\n",
    "                else:\n",
    "                    print(\"Failed to compute test similarity scores for single best model.\")\n",
    "                    test_predictions = None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during single model test prediction generation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            test_predictions = None\n",
    "else:\n",
    "    print(\"No best model configuration identified or details missing. Cannot generate predictions.\")\n",
    "\n",
    "\n",
    "# 5. Save Final Predictions to JSON\n",
    "if test_predictions is not None and isinstance(test_predictions, dict) and test_predictions:\n",
    "    print(f\"\\nSaving final predictions ({len(test_predictions)} queries) to {output_filename} using method: {best_method_for_prediction}...\")\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_predictions, f, indent=4)\n",
    "        print(\"Predictions saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving predictions: {e}\")\n",
    "elif test_predictions is None:\n",
    "     print(\"No predictions were generated due to errors.\")\n",
    "else:\n",
    "     print(\"Predictions dictionary is empty, not saving.\")\n",
    "\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, models, util # Added CrossEncoder\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import types # Used for SimpleNamespace if preferred\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "config = {\n",
    "    # --- Data Files ---\n",
    "    'base_dir': '.', # Base directory containing the data files.\n",
    "    'query_list_file': 'test_queries.json', # Path to the JSON file with query IDs (relative to base_dir). REQUIRED.\n",
    "    'pre_ranking_file': 'shuffled_pre_ranking.json', # Path to the initial ranking JSON (relative to base_dir).\n",
    "    'queries_content_file': 'queries_content_with_features.json', # Path to queries content JSON (relative to base_dir).\n",
    "    'documents_content_file': 'documents_content_with_features.json', # Path to documents content JSON (relative to base_dir).\n",
    "    'output_file': 'prediction_reranked.json', # Path to save the re-ranked prediction JSON (relative to base_dir).\n",
    "\n",
    "    # --- Re-ranking Strategy ---\n",
    "    # Choose ONE: 'bi-encoder', 'cross-encoder', 'hybrid-text'\n",
    "    'reranker_type': 'bi-encoder',\n",
    "\n",
    "    # --- Model Settings (Relevant based on 'reranker_type') ---\n",
    "    'bi_encoder_model': 'AI-Growth-Lab/PatentSBERTa', # Used for 'bi-encoder' and 'hybrid-text'\n",
    "    'cross_encoder_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2', # Used for 'cross-encoder'\n",
    "    'pooling': 'mean', # Pooling strategy for bi-encoder (Note: may be overridden by model config). Choices: 'mean', 'max', 'cls'\n",
    "    'max_length': 512, # Max sequence length for the model.\n",
    "\n",
    "    # --- Text Settings ---\n",
    "    # For 'bi-encoder' and 'cross-encoder', choose ONE: 'TA', 'claims', 'tac1', 'description', 'full', 'features'\n",
    "    'text_type': 'tac1',\n",
    "    # For 'hybrid-text', define the types and their weights\n",
    "    'hybrid_text_types': ['tac1', 'claims'], # List of text types to combine (e.g., ['TA', 'claims'])\n",
    "    'hybrid_weights': [0.6, 0.4],      # Weights for each text type (must sum to 1, match order of types)\n",
    "\n",
    "    # --- Execution Settings ---\n",
    "    'batch_size': 32, # Batch size for encoding document texts / cross-encoder predictions.\n",
    "    'device': None # Device: 'cuda', 'cpu', or None (auto-detect).\n",
    "}\n",
    "\n",
    "# --- Auto-detect device if not specified ---\n",
    "if config['device'] is None:\n",
    "    config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "elif config['device'] == 'cuda' and not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA requested but not available. Using CPU.\")\n",
    "    config['device'] = 'cpu'\n",
    "\n",
    "# ----------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load JSON data from a file\"\"\"\n",
    "    print(f\"Loading JSON from: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Successfully loaded {len(data)} items.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_json_file(data, file_path):\n",
    "    \"\"\"Save data to a JSON file\"\"\"\n",
    "    print(f\"Saving JSON to: {file_path}\")\n",
    "    try:\n",
    "        output_dir = os.path.dirname(file_path)\n",
    "        if output_dir:\n",
    "             os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"Successfully saved data to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred saving to {file_path}: {e}\")\n",
    "\n",
    "def load_content_data(file_path):\n",
    "    \"\"\"Load content data from a JSON file and create a FAN to Content mapping.\"\"\"\n",
    "    data = load_json_file(file_path)\n",
    "    if data is None:\n",
    "        return {}\n",
    "\n",
    "    content_dict = {}\n",
    "    key_options = ['FAN', 'Application_Number'] # Handle potential key variations\n",
    "\n",
    "    for item in data:\n",
    "        fan_key = None\n",
    "        for key in key_options:\n",
    "            if key in item:\n",
    "                # Sometimes Application_Number needs Application_Category appended\n",
    "                if key == 'Application_Number' and 'Application_Category' in item:\n",
    "                   fan_key = item[key] + item.get('Application_Category', '') # Safely get category\n",
    "                else:\n",
    "                   fan_key = item[key]\n",
    "                break # Found a key, stop looking\n",
    "\n",
    "        if fan_key and 'Content' in item:\n",
    "             content_dict[fan_key] = item['Content']\n",
    "        # else:\n",
    "        #     print(f\"Warning: Could not find FAN key or Content in item: {item.keys()}\")\n",
    "\n",
    "    print(f\"Created content dictionary with {len(content_dict)} entries.\")\n",
    "    return content_dict\n",
    "\n",
    "\n",
    "def extract_text(content_dict, text_type=\"TA\"):\n",
    "    \"\"\"Extract text from patent content based on text_type\"\"\"\n",
    "    if not isinstance(content_dict, dict):\n",
    "        # print(f\"Warning: Invalid content_dict provided (type: {type(content_dict)}), expected dict.\")\n",
    "        return \"\"\n",
    "\n",
    "    text_parts = []\n",
    "\n",
    "    # Standard types\n",
    "    if text_type in [\"TA\", \"tac1\", \"full\", \"title_abstract\"]:\n",
    "        text_parts.append(content_dict.get(\"title\", \"\"))\n",
    "        text_parts.append(content_dict.get(\"pa01\", \"\")) # Abstract\n",
    "\n",
    "    if text_type in [\"claims\", \"tac1\", \"full\"]:\n",
    "        claims = []\n",
    "        first_claim = None\n",
    "        # Sort keys to approximate claim order\n",
    "        sorted_keys = sorted([key for key in content_dict if key.startswith('c-')], key=lambda x: int(x.split('-')[1]))\n",
    "        for key in sorted_keys:\n",
    "            claim_text = content_dict.get(key, \"\")\n",
    "            if claim_text:\n",
    "                claims.append(claim_text)\n",
    "                if first_claim is None and text_type == \"tac1\":\n",
    "                    first_claim = claim_text\n",
    "\n",
    "        if text_type == \"claims\" or text_type == \"full\":\n",
    "            text_parts.extend(claims)\n",
    "        elif text_type == \"tac1\" and first_claim:\n",
    "            text_parts.append(first_claim)\n",
    "\n",
    "    if text_type in [\"description\", \"full\"]:\n",
    "        desc_parts = []\n",
    "        # Sort keys to approximate paragraph order\n",
    "        sorted_keys = sorted([key for key in content_dict if key.startswith('p')], key=lambda x: int(x.split('-')[1]))\n",
    "        for key in sorted_keys:\n",
    "             desc_parts.append(content_dict.get(key,\"\"))\n",
    "        text_parts.extend(desc_parts)\n",
    "\n",
    "    # Feature type\n",
    "    if text_type == \"features\":\n",
    "        text_parts.append(content_dict.get(\"features\", \"\")) # Assumes 'features' key holds text\n",
    "\n",
    "    # Join non-empty parts with a space\n",
    "    result = \" \".join(filter(None, text_parts)).strip()\n",
    "    # print(f\"Extracted text type '{text_type}': {result[:100]}...\") # Debugging: print start of text\n",
    "    return result\n",
    "\n",
    "# ----------------------------\n",
    "# Main Re-ranking Logic\n",
    "# ----------------------------\n",
    "\n",
    "def main(cfg):\n",
    "    # --- Device Setup ---\n",
    "    device = torch.device(cfg['device'])\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Selected Re-ranker Type: {cfg['reranker_type']}\")\n",
    "\n",
    "    # --- Construct Full Paths ---\n",
    "    def get_full_path(path):\n",
    "        return path if os.path.isabs(path) else os.path.join(cfg['base_dir'], path)\n",
    "\n",
    "    query_list_file = get_full_path(cfg['query_list_file'])\n",
    "    pre_ranking_file = get_full_path(cfg['pre_ranking_file'])\n",
    "    queries_content_file = get_full_path(cfg['queries_content_file'])\n",
    "    documents_content_file = get_full_path(cfg['documents_content_file'])\n",
    "    output_file = get_full_path(cfg['output_file'])\n",
    "\n",
    "    # --- Load Data ---\n",
    "    query_ids = load_json_file(query_list_file)\n",
    "    pre_ranking_data = load_json_file(pre_ranking_file)\n",
    "    queries_content = load_content_data(queries_content_file)\n",
    "    documents_content = load_content_data(documents_content_file)\n",
    "\n",
    "    if not query_ids or not pre_ranking_data or not queries_content or not documents_content:\n",
    "        print(\"Error: Failed to load one or more essential data files. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- Load Model ---\n",
    "    model = None\n",
    "    reranker_type = cfg['reranker_type']\n",
    "\n",
    "    try:\n",
    "        if reranker_type == 'bi-encoder' or reranker_type == 'hybrid-text':\n",
    "            model_name = cfg['bi_encoder_model']\n",
    "            print(f\"Loading Bi-Encoder model: {model_name}\")\n",
    "            # Optional: Define specific pooling if needed, otherwise model default is used\n",
    "            # word_embedding_model = models.Transformer(model_name, max_seq_length=cfg['max_length'])\n",
    "            # pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=cfg['pooling'])\n",
    "            # model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device=device)\n",
    "            model = SentenceTransformer(model_name, device=device)\n",
    "            model.max_seq_length = cfg['max_length'] # Ensure max_length is set\n",
    "            print(f\"Bi-Encoder model '{model_name}' loaded successfully.\")\n",
    "\n",
    "        elif reranker_type == 'cross-encoder':\n",
    "            model_name = cfg['cross_encoder_model']\n",
    "            print(f\"Loading Cross-Encoder model: {model_name}\")\n",
    "            model = CrossEncoder(model_name, device=device, max_length=cfg['max_length'])\n",
    "            print(f\"Cross-Encoder model '{model_name}' loaded successfully.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Error: Invalid reranker_type specified: {reranker_type}\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name} for type {reranker_type}: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Validate Hybrid Config (if applicable) ---\n",
    "    if reranker_type == 'hybrid-text':\n",
    "        if len(cfg['hybrid_text_types']) != len(cfg['hybrid_weights']):\n",
    "            print(\"Error: Mismatch between hybrid_text_types and hybrid_weights lengths. Exiting.\")\n",
    "            return\n",
    "        if not abs(sum(cfg['hybrid_weights']) - 1.0) < 1e-6:\n",
    "             print(\"Error: hybrid_weights must sum to 1.0. Exiting.\")\n",
    "             return\n",
    "        print(f\"Using Hybrid-Text approach with types: {cfg['hybrid_text_types']} and weights: {cfg['hybrid_weights']}\")\n",
    "    else:\n",
    "         print(f\"Using Text Type: '{cfg['text_type']}'\")\n",
    "\n",
    "    # --- Re-ranking Process ---\n",
    "    print(f\"Starting re-ranking for {len(query_ids)} queries...\")\n",
    "    results = {}\n",
    "    missing_query_content_count = 0\n",
    "    missing_pre_ranking_count = 0\n",
    "    queries_with_no_valid_docs_count = 0\n",
    "    query_encoding_errors = 0\n",
    "\n",
    "    for query_id in tqdm(query_ids, desc=\"Processing queries\"):\n",
    "\n",
    "        # 1. Get Candidate Document IDs\n",
    "        candidate_doc_ids = pre_ranking_data.get(query_id)\n",
    "        if not candidate_doc_ids:\n",
    "            # print(f\"Warning: Pre-ranking not found for query {query_id}\")\n",
    "            missing_pre_ranking_count += 1\n",
    "            results[query_id] = []\n",
    "            continue\n",
    "\n",
    "        # --- Prepare Query Text(s) ---\n",
    "        query_content_dict = queries_content.get(query_id)\n",
    "        if not query_content_dict:\n",
    "            # print(f\"Warning: Content not found for query {query_id}\")\n",
    "            missing_query_content_count += 1\n",
    "            results[query_id] = candidate_doc_ids # Fallback to original ranking\n",
    "            continue\n",
    "\n",
    "        query_texts = {} # Store {text_type: text}\n",
    "        query_valid = True\n",
    "        text_types_to_extract = cfg['hybrid_text_types'] if reranker_type == 'hybrid-text' else [cfg['text_type']]\n",
    "\n",
    "        for ttype in text_types_to_extract:\n",
    "            q_text = extract_text(query_content_dict, ttype)\n",
    "            if not q_text:\n",
    "                # print(f\"Warning: Extracted text is empty for query {query_id} with type '{ttype}'\")\n",
    "                if reranker_type != 'hybrid-text': # Only critical if it's the *only* type needed\n",
    "                    missing_query_content_count += 1\n",
    "                    query_valid = False\n",
    "                    break\n",
    "                # For hybrid, we might proceed if *other* texts are available, handled later\n",
    "            query_texts[ttype] = q_text\n",
    "\n",
    "        if not query_valid:\n",
    "            results[query_id] = candidate_doc_ids # Fallback\n",
    "            continue\n",
    "        # For hybrid, check if at least one query text was found (more robust check later)\n",
    "        if reranker_type == 'hybrid-text' and not any(query_texts.values()):\n",
    "             print(f\"Warning: No valid query text found for any hybrid type for query {query_id}\")\n",
    "             missing_query_content_count += 1\n",
    "             results[query_id] = candidate_doc_ids # Fallback\n",
    "             continue\n",
    "\n",
    "\n",
    "        # --- Prepare Document Texts & Identify Valid Docs ---\n",
    "        valid_docs_data = {} # Store {doc_id: {text_type: text}} for docs with *required* content\n",
    "        original_candidate_set = set(candidate_doc_ids)\n",
    "\n",
    "        for doc_id in candidate_doc_ids:\n",
    "            doc_content_dict = documents_content.get(doc_id)\n",
    "            if not doc_content_dict:\n",
    "                continue # Skip docs without any content entry\n",
    "\n",
    "            doc_texts_extracted = {}\n",
    "            doc_is_valid = True\n",
    "            for ttype in text_types_to_extract:\n",
    "                d_text = extract_text(doc_content_dict, ttype)\n",
    "                if not d_text:\n",
    "                    if reranker_type != 'hybrid-text': # Invalid if the required single text type is missing\n",
    "                        doc_is_valid = False\n",
    "                        break\n",
    "                    # For hybrid, store None temporarily, check later if *all* are missing\n",
    "                    doc_texts_extracted[ttype] = None\n",
    "                else:\n",
    "                    doc_texts_extracted[ttype] = d_text\n",
    "\n",
    "            if not doc_is_valid:\n",
    "                continue # Skip doc if required single text type was missing\n",
    "\n",
    "            # For hybrid, check if *at least one* text type was successfully extracted\n",
    "            if reranker_type == 'hybrid-text' and not any(doc_texts_extracted.values()):\n",
    "                continue # Skip doc if no text could be extracted for any hybrid type\n",
    "\n",
    "            valid_docs_data[doc_id] = doc_texts_extracted # Store extracted texts for valid doc\n",
    "\n",
    "        valid_doc_ids = list(valid_docs_data.keys())\n",
    "\n",
    "        if not valid_doc_ids:\n",
    "            # print(f\"Warning: No valid document texts found for query {query_id} after checking {len(candidate_doc_ids)} candidates.\")\n",
    "            queries_with_no_valid_docs_count += 1\n",
    "            results[query_id] = candidate_doc_ids # Fallback to original order\n",
    "            continue\n",
    "\n",
    "        # --- Calculate Scores based on Re-ranker Type ---\n",
    "        scores = [] # List to hold the final score for each valid_doc_id\n",
    "        doc_scores_calculated = {} # Map doc_id -> final_score\n",
    "\n",
    "        try:\n",
    "            # --- Bi-Encoder Scoring ---\n",
    "            if reranker_type == 'bi-encoder':\n",
    "                ttype = cfg['text_type']\n",
    "                query_text = query_texts[ttype]\n",
    "                doc_texts = [valid_docs_data[doc_id][ttype] for doc_id in valid_doc_ids]\n",
    "\n",
    "                query_embedding = model.encode(query_text, convert_to_tensor=True, show_progress_bar=False).to(device)\n",
    "                doc_embeddings = model.encode(doc_texts, convert_to_tensor=True, show_progress_bar=False, batch_size=cfg['batch_size']).to(device)\n",
    "                cosine_scores = util.cos_sim(query_embedding, doc_embeddings)[0].cpu().numpy()\n",
    "                doc_scores_calculated = dict(zip(valid_doc_ids, cosine_scores))\n",
    "\n",
    "            # --- Cross-Encoder Scoring ---\n",
    "            elif reranker_type == 'cross-encoder':\n",
    "                ttype = cfg['text_type']\n",
    "                query_text = query_texts[ttype]\n",
    "                # Prepare pairs: [[query, doc1_text], [query, doc2_text], ...]\n",
    "                sentence_pairs = [[query_text, valid_docs_data[doc_id][ttype]] for doc_id in valid_doc_ids]\n",
    "                # Predict returns scores directly\n",
    "                cross_scores = model.predict(sentence_pairs, show_progress_bar=False, batch_size=cfg['batch_size'], convert_to_numpy=True)\n",
    "                doc_scores_calculated = dict(zip(valid_doc_ids, cross_scores))\n",
    "\n",
    "            # --- Hybrid-Text Scoring ---\n",
    "            elif reranker_type == 'hybrid-text':\n",
    "                hybrid_scores_per_doc = {doc_id: [] for doc_id in valid_doc_ids} # {doc_id: [score_t1, score_t2,...]}\n",
    "\n",
    "                for i, ttype in enumerate(cfg['hybrid_text_types']):\n",
    "                    current_query_text = query_texts.get(ttype)\n",
    "                    # Prepare lists for docs that HAVE text for this specific type\n",
    "                    current_doc_texts = []\n",
    "                    current_valid_doc_ids_for_type = []\n",
    "                    for doc_id in valid_doc_ids:\n",
    "                        doc_text = valid_docs_data[doc_id].get(ttype)\n",
    "                        if doc_text: # Only encode if text exists for this type\n",
    "                            current_doc_texts.append(doc_text)\n",
    "                            current_valid_doc_ids_for_type.append(doc_id)\n",
    "\n",
    "                    if not current_query_text or not current_valid_doc_ids_for_type:\n",
    "                        # Assign 0 score for this type if query or all docs lack text\n",
    "                        for doc_id in current_valid_doc_ids_for_type: # Should be empty if query missing\n",
    "                            hybrid_scores_per_doc[doc_id].append(0.0)\n",
    "                        # Need placeholder for docs that were valid overall but missing *this* text type\n",
    "                        for doc_id in valid_doc_ids:\n",
    "                             if doc_id not in current_valid_doc_ids_for_type:\n",
    "                                 # Ensure the list size matches number of types eventually\n",
    "                                  hybrid_scores_per_doc[doc_id].append(0.0) # Append 0 if text missing\n",
    "                        continue # Skip encoding for this type\n",
    "\n",
    "                    # Encode and calculate scores for the current text type\n",
    "                    q_emb = model.encode(current_query_text, convert_to_tensor=True, show_progress_bar=False).to(device)\n",
    "                    d_embs = model.encode(current_doc_texts, convert_to_tensor=True, show_progress_bar=False, batch_size=cfg['batch_size']).to(device)\n",
    "                    sim_scores = util.cos_sim(q_emb, d_embs)[0].cpu().numpy()\n",
    "\n",
    "                    # Map scores back to the correct doc_ids\n",
    "                    scores_map_for_type = dict(zip(current_valid_doc_ids_for_type, sim_scores))\n",
    "                    for doc_id in valid_doc_ids:\n",
    "                        hybrid_scores_per_doc[doc_id].append(scores_map_for_type.get(doc_id, 0.0)) # Append score or 0 if missing\n",
    "\n",
    "                # Combine scores using weights\n",
    "                weights = cfg['hybrid_weights']\n",
    "                for doc_id in valid_doc_ids:\n",
    "                    # Ensure we have scores for all types, even if some were 0\n",
    "                    if len(hybrid_scores_per_doc[doc_id]) == len(weights):\n",
    "                         combined_score = sum(s * w for s, w in zip(hybrid_scores_per_doc[doc_id], weights))\n",
    "                         doc_scores_calculated[doc_id] = combined_score\n",
    "                    else:\n",
    "                         # Should not happen with the logic above, but as a fallback\n",
    "                         print(f\"Warning: Score list length mismatch for doc {doc_id} in hybrid mode. Assigning 0.\")\n",
    "                         doc_scores_calculated[doc_id] = 0.0\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during scoring for query {query_id} using {reranker_type}: {e}\")\n",
    "            # Fallback to original pre-ranked order for this query on error\n",
    "            results[query_id] = candidate_doc_ids\n",
    "            query_encoding_errors += 1\n",
    "            continue\n",
    "\n",
    "        # --- Rank Documents ---\n",
    "        # Create list of (doc_id, score) tuples for sorting\n",
    "        scored_doc_list = list(doc_scores_calculated.items())\n",
    "\n",
    "        # Sort by score in descending order (higher score = better rank)\n",
    "        scored_doc_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get the sorted list of document IDs that were successfully scored\n",
    "        re_ranked_doc_ids = [doc_id for doc_id, score in scored_doc_list]\n",
    "\n",
    "        # --- Combine with original docs that were filtered out ---\n",
    "        # Identify docs from the original list that weren't scored (missing content, etc.)\n",
    "        reranked_set = set(re_ranked_doc_ids)\n",
    "        missing_or_invalid_docs = [doc_id for doc_id in candidate_doc_ids if doc_id not in reranked_set]\n",
    "\n",
    "        # Append the missing/invalid ones to the end\n",
    "        final_ranked_list = re_ranked_doc_ids + missing_or_invalid_docs\n",
    "\n",
    "        # Ensure the final list length matches the original candidate list length\n",
    "        results[query_id] = final_ranked_list[:len(candidate_doc_ids)]\n",
    "\n",
    "\n",
    "    # --- Report Summary ---\n",
    "    print(\"\\n--- Re-ranking Summary ---\")\n",
    "    print(f\"Total queries processed: {len(query_ids)}\")\n",
    "    print(f\"Re-ranker Type Used: {cfg['reranker_type']}\")\n",
    "    if reranker_type == 'hybrid-text':\n",
    "        print(f\"  Hybrid Types: {cfg['hybrid_text_types']}, Weights: {cfg['hybrid_weights']}\")\n",
    "    else:\n",
    "        print(f\"  Text Type Used: {cfg['text_type']}\")\n",
    "\n",
    "    if missing_query_content_count > 0:\n",
    "        print(f\"Warning: Query content missing or empty for {missing_query_content_count} queries (led to fallback).\")\n",
    "    if missing_pre_ranking_count > 0:\n",
    "        print(f\"Warning: Pre-ranking data missing for {missing_pre_ranking_count} queries.\")\n",
    "    if queries_with_no_valid_docs_count > 0:\n",
    "        print(f\"Warning: {queries_with_no_valid_docs_count} queries had no documents with valid content for scoring (led to fallback).\")\n",
    "    if query_encoding_errors > 0:\n",
    "         print(f\"Warning: Scoring failed due to errors for {query_encoding_errors} queries (led to fallback).\")\n",
    "    print(f\"Number of queries in results: {len(results)}\")\n",
    "\n",
    "\n",
    "    # --- Save Results ---\n",
    "    save_json_file(results, output_file)\n",
    "\n",
    "    print(\"\\nRe-ranking complete. Output saved.\")\n",
    "\n",
    "\n",
    "# --- Run the main function ---\n",
    "if __name__ == \"__main__\":\n",
    "    # You can modify the config dictionary here directly before running,\n",
    "    # or load it from a separate JSON file if preferred.\n",
    "    # Example: Change to Cross-Encoder\n",
    "    # config['reranker_type'] = 'cross-encoder'\n",
    "    # config['text_type'] = 'tac1' # Choose appropriate text type for the CE\n",
    "\n",
    "    # Example: Change to Hybrid\n",
    "    # config['reranker_type'] = 'hybrid-text'\n",
    "    # config['hybrid_text_types'] = ['tac1', 'claims']\n",
    "    # config['hybrid_weights'] = [0.7, 0.3]\n",
    "\n",
    "    main(config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
