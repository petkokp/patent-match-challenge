{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petko/projects/patent-match-challenge/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"---------------------------------------------------------------------\n",
    "Block 1 – Imports & global settings                                     \n",
    "---------------------------------------------------------------------\"\"\"\n",
    "import os, json, re, string, time, math, multiprocessing as mp, pickle\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import hnswlib\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer, util as st_util\n",
    "    import torch\n",
    "    DENSE_OK = True\n",
    "except ImportError:\n",
    "    DENSE_OK = False\n",
    "\n",
    "# Re‑seed for reproducibility\n",
    "SEED = 2025\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = Path(\"./datasets\")\n",
    "CACHE_DIR = Path(\".cache\"); CACHE_DIR.mkdir(exist_ok=True)\n",
    "K_SUBMISSION = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"---------------------------------------------------------------------\n",
    "Block 2 – Data loading helpers                                           \n",
    "---------------------------------------------------------------------\"\"\"\n",
    "\n",
    "def load_json(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf‑8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_all():\n",
    "    content = DATA_DIR / \"Content_JSONs\"\n",
    "    citing_train = load_json(content/\"Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TRAIN.json\")\n",
    "    citing_test  = load_json(content/\"Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TEST.json\")\n",
    "    nonciting    = load_json(content/\"Cited_2020_Uncited_2010-2019_Cleaned_Content_22k/CLEANED_CONTENT_DATASET_cited_patents_by_2020_uncited_2010-2019.json\")\n",
    "    mapping      = pd.DataFrame(load_json(DATA_DIR/\"Citation_JSONs/Citation_Train.json\"))\n",
    "    return citing_train, citing_test, nonciting, mapping\n",
    "\n",
    "CITING_TRAIN, CITING_TEST, NONCITING, MAP_DF = load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"---------------------------------------------------------------------\n",
    "Block 3 – Text extraction & basic preprocessing                          \n",
    "---------------------------------------------------------------------\"\"\"\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True) # Needed for word_tokenize\n",
    "from nltk.corpus import stopwords as nltk_sw\n",
    "from nltk.stem import PorterStemmer # Added for stemming\n",
    "\n",
    "STOP_WORDS = set(nltk_sw.words('english')).union({\n",
    "    'claim', 'claims', 'method', 'apparatus',\n",
    "    'embodiment', 'wherein', 'figure', 'system', 'device' # Added a few more common patent terms\n",
    "})\n",
    "\n",
    "TOKENIZER = re.compile(r\"[\\w']{2,}\")\n",
    "STEMMER = PorterStemmer() # Initialize stemmer\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    # Keep hyphens within words, remove other punctuation and digits\n",
    "    text = text.translate(str.maketrans(string.punctuation.replace('-', ''), ' ' * len(string.punctuation.replace('-', ''))))\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Consolidate whitespace\n",
    "    return text\n",
    "\n",
    "@lru_cache(maxsize=1<<15) # Increased cache size slightly\n",
    "def preprocess_and_stem(text: str) -> list[str]:\n",
    "    # Use nltk tokenizer which handles contractions better\n",
    "    # Apply stemming after tokenization and stopword removal\n",
    "    normalized_text = normalize(text)\n",
    "    tokens = [STEMMER.stem(tok) # Apply stemming\n",
    "              for tok in nltk.word_tokenize(normalized_text) # Using nltk tokenizer\n",
    "              if tok.isalnum() and len(tok) > 1 and tok not in STOP_WORDS]\n",
    "    return tokens\n",
    "\n",
    "# Keep original preprocess without stemming for dense model if needed\n",
    "@lru_cache(maxsize=1<<14)\n",
    "def preprocess_original(text: str) -> list[str]:\n",
    "    return [tok for tok in TOKENIZER.findall(normalize(text)) if tok not in STOP_WORDS]\n",
    "\n",
    "TEXT_PARTS = {\n",
    "    \"title\": [\"title\"],\n",
    "    \"abstract\": [\"pa01\"],\n",
    "    \"claim1\": [\"c-en-0001\"],\n",
    "    \"title_abstract_claims\": [\"title\", \"pa01\"] + [f\"c-en-{i:04d}\" for i in range(1, 101)]\n",
    "}\n",
    "\n",
    "def build_corpus(records: list[dict], text_type: str) -> tuple[list[str], list[str]]:\n",
    "    ids, texts = [], []\n",
    "    parts = TEXT_PARTS[text_type]\n",
    "    for rec in records:\n",
    "        doc_id = rec.get('Application_Number','') + rec.get('Application_Category','')\n",
    "        if not doc_id: continue\n",
    "        content = rec.get('Content', {})\n",
    "        segments = [content[k] for k in parts if content.get(k)]\n",
    "        if segments:\n",
    "            texts.append(' '.join(segments))\n",
    "            ids.append(doc_id)\n",
    "    return ids, texts\n",
    "\n",
    "# Parallel preprocessing (disk cached) --------------------------------\n",
    "\n",
    "def cached_tokens(name: str, texts: list[str], stem: bool = True):\n",
    "    path = CACHE_DIR/f\"{name}{'_stemmed' if stem else ''}.pkl\"\n",
    "    if path.exists():\n",
    "        print(f\"Loading cached tokens from: {path}\")\n",
    "        return pickle.load(open(path,'rb'))\n",
    "\n",
    "    print(f\"Preprocessing {'and stemming ' if stem else ''}texts for {name}...\")\n",
    "    func = preprocess_and_stem if stem else preprocess_original\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        tokens = list(tqdm(pool.imap(func, texts, chunksize=100), total=len(texts), desc=f\"Tokenizing {name}\"))\n",
    "    print(f\"Caching tokens to: {path}\")\n",
    "    pickle.dump(tokens, open(path,'wb'))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"---------------------------------------------------------------------\n",
    "Block 4 – Sparse model: BM25 + TF‑IDF                                    \n",
    "---------------------------------------------------------------------\"\"\"\n",
    "class SparseIndexer:\n",
    "    def __init__(self, text_type=\"title_abstract_claims\", max_features=40_000):\n",
    "        self.text_type = text_type\n",
    "        self.max_features = max_features\n",
    "        self.tfidf = None\n",
    "        self.bm25 = None\n",
    "        self.doc_ids = None\n",
    "\n",
    "    # ---------- TF‑IDF ------------\n",
    "    def fit_tfidf(self, texts: list[str]):\n",
    "        vec = TfidfVectorizer(max_features=self.max_features,\n",
    "                              ngram_range=(1,2),\n",
    "                              min_df=2,\n",
    "                              sublinear_tf=True,\n",
    "                              stop_words='english')\n",
    "        self.tfidf = vec.fit_transform(texts)\n",
    "        return vec\n",
    "\n",
    "    # ---------- BM25 --------------\n",
    "    def fit_bm25(self, tokenized_texts: list[list[str]]):\n",
    "        self.bm25 = BM25Okapi(tokenized_texts)\n",
    "        return self.bm25\n",
    "\n",
    "    # ---------- Ranking helpers ---\n",
    "    def rank_tfidf(self, query_vec, top_k):\n",
    "        sims = cosine_similarity(query_vec, self.tfidf, dense_output=False)\n",
    "        return np.asarray(sims.toarray()[0])[:], sims\n",
    "\n",
    "    def rank_bm25(self, query_toks, top_k):\n",
    "        return np.array(self.bm25.get_scores(query_toks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINETUNE DENSE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for fine-tuning with prefixes...\n",
      "Building text lookup maps...\n",
      "Built maps: 6831 citing, 16834 non-citing docs.\n",
      "Split: 6147 train queries, 684 dev queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing citation pairs: 100%|██████████| 8594/8594 [00:00<00:00, 29104.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 7726 training examples.\n",
      "Prepared 684 dev queries with 868 positive relations.\n",
      "Added 4250 negative documents (passages) to the dev corpus.\n",
      "Total dev corpus size: 5100\n",
      "Loading base model: intfloat/e5-large-v2\n",
      "Model max sequence length set to: 512\n",
      "Using MultipleNegativesRankingLoss.\n",
      "Train dataloader created with batch size 8.\n",
      "InformationRetrievalEvaluator configured.\n",
      "\n",
      "--- Starting Fine-Tuning Training ---\n",
      "Steps per epoch: 966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2898' max='2898' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2898/2898 31:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Patent Dev Cos Sim Accuracy@1</th>\n",
       "      <th>Patent Dev Cos Sim Accuracy@3</th>\n",
       "      <th>Patent Dev Cos Sim Accuracy@5</th>\n",
       "      <th>Patent Dev Cos Sim Accuracy@10</th>\n",
       "      <th>Patent Dev Cos Sim Precision@1</th>\n",
       "      <th>Patent Dev Cos Sim Precision@3</th>\n",
       "      <th>Patent Dev Cos Sim Precision@5</th>\n",
       "      <th>Patent Dev Cos Sim Precision@10</th>\n",
       "      <th>Patent Dev Cos Sim Recall@1</th>\n",
       "      <th>Patent Dev Cos Sim Recall@3</th>\n",
       "      <th>Patent Dev Cos Sim Recall@5</th>\n",
       "      <th>Patent Dev Cos Sim Recall@10</th>\n",
       "      <th>Patent Dev Cos Sim Ndcg@10</th>\n",
       "      <th>Patent Dev Cos Sim Mrr@10</th>\n",
       "      <th>Patent Dev Cos Sim Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.679825</td>\n",
       "      <td>0.776316</td>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.232943</td>\n",
       "      <td>0.159357</td>\n",
       "      <td>0.092690</td>\n",
       "      <td>0.361672</td>\n",
       "      <td>0.561964</td>\n",
       "      <td>0.634089</td>\n",
       "      <td>0.740205</td>\n",
       "      <td>0.570572</td>\n",
       "      <td>0.536990</td>\n",
       "      <td>0.518985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>966</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.741228</td>\n",
       "      <td>0.818713</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.249513</td>\n",
       "      <td>0.171637</td>\n",
       "      <td>0.098246</td>\n",
       "      <td>0.384722</td>\n",
       "      <td>0.613109</td>\n",
       "      <td>0.693324</td>\n",
       "      <td>0.783528</td>\n",
       "      <td>0.606097</td>\n",
       "      <td>0.568564</td>\n",
       "      <td>0.547834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.628655</td>\n",
       "      <td>0.716374</td>\n",
       "      <td>0.808480</td>\n",
       "      <td>0.438596</td>\n",
       "      <td>0.237817</td>\n",
       "      <td>0.166959</td>\n",
       "      <td>0.096784</td>\n",
       "      <td>0.376194</td>\n",
       "      <td>0.577827</td>\n",
       "      <td>0.670492</td>\n",
       "      <td>0.771735</td>\n",
       "      <td>0.592413</td>\n",
       "      <td>0.554257</td>\n",
       "      <td>0.534151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.461988</td>\n",
       "      <td>0.668129</td>\n",
       "      <td>0.735380</td>\n",
       "      <td>0.831871</td>\n",
       "      <td>0.461988</td>\n",
       "      <td>0.252924</td>\n",
       "      <td>0.174269</td>\n",
       "      <td>0.101462</td>\n",
       "      <td>0.398733</td>\n",
       "      <td>0.617154</td>\n",
       "      <td>0.697515</td>\n",
       "      <td>0.803363</td>\n",
       "      <td>0.624020</td>\n",
       "      <td>0.583261</td>\n",
       "      <td>0.565443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1932</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.682749</td>\n",
       "      <td>0.748538</td>\n",
       "      <td>0.836257</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.257797</td>\n",
       "      <td>0.176023</td>\n",
       "      <td>0.101608</td>\n",
       "      <td>0.409820</td>\n",
       "      <td>0.632505</td>\n",
       "      <td>0.708894</td>\n",
       "      <td>0.805312</td>\n",
       "      <td>0.632035</td>\n",
       "      <td>0.594077</td>\n",
       "      <td>0.574289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.482456</td>\n",
       "      <td>0.682749</td>\n",
       "      <td>0.748538</td>\n",
       "      <td>0.837719</td>\n",
       "      <td>0.482456</td>\n",
       "      <td>0.257797</td>\n",
       "      <td>0.176316</td>\n",
       "      <td>0.101170</td>\n",
       "      <td>0.419688</td>\n",
       "      <td>0.631603</td>\n",
       "      <td>0.704995</td>\n",
       "      <td>0.804337</td>\n",
       "      <td>0.635349</td>\n",
       "      <td>0.599174</td>\n",
       "      <td>0.580144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.475146</td>\n",
       "      <td>0.675439</td>\n",
       "      <td>0.744152</td>\n",
       "      <td>0.840643</td>\n",
       "      <td>0.475146</td>\n",
       "      <td>0.255361</td>\n",
       "      <td>0.174561</td>\n",
       "      <td>0.101754</td>\n",
       "      <td>0.411769</td>\n",
       "      <td>0.628874</td>\n",
       "      <td>0.700560</td>\n",
       "      <td>0.807505</td>\n",
       "      <td>0.632261</td>\n",
       "      <td>0.594057</td>\n",
       "      <td>0.574908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2898</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.475146</td>\n",
       "      <td>0.685673</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.840643</td>\n",
       "      <td>0.475146</td>\n",
       "      <td>0.259747</td>\n",
       "      <td>0.176316</td>\n",
       "      <td>0.102047</td>\n",
       "      <td>0.411038</td>\n",
       "      <td>0.639352</td>\n",
       "      <td>0.710307</td>\n",
       "      <td>0.809698</td>\n",
       "      <td>0.636180</td>\n",
       "      <td>0.597831</td>\n",
       "      <td>0.579078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 43/43 [00:05<00:00,  7.85it/s]\n",
      "Batches: 100%|██████████| 319/319 [00:41<00:00,  7.70it/s]\n",
      "Corpus Chunks: 100%|██████████| 1/1 [00:42<00:00, 42.35s/it]\n",
      "Batches: 100%|██████████| 43/43 [00:05<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 319/319 [00:42<00:00,  7.53it/s]\n",
      "Corpus Chunks: 100%|██████████| 1/1 [00:43<00:00, 43.29s/it]\n",
      "Batches: 100%|██████████| 43/43 [00:05<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 319/319 [00:42<00:00,  7.54it/s]\n",
      "Corpus Chunks: 100%|██████████| 1/1 [00:43<00:00, 43.29s/it]\n",
      "Batches: 100%|██████████| 43/43 [00:05<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 319/319 [00:42<00:00,  7.55it/s]\n",
      "Corpus Chunks: 100%|██████████| 1/1 [00:43<00:00, 43.22s/it]\n",
      "Batches: 100%|██████████| 43/43 [00:05<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 319/319 [00:42<00:00,  7.55it/s]\n",
      "Corpus Chunks: 100%|██████████| 1/1 [00:43<00:00, 43.20s/it]\n",
      "Batches: 100%|██████████| 43/43 [00:05<00:00,  7.63it/s]\n",
      "Batches: 100%|██████████| 319/319 [00:42<00:00,  7.42it/s]\n",
      "Corpus Chunks: 100%|██████████| 1/1 [00:43<00:00, 43.74s/it]\n",
      "Batches: 100%|██████████| 43/43 [00:05<00:00,  7.63it/s]\n",
      "Batches: 100%|██████████| 319/319 [00:43<00:00,  7.41it/s]\n",
      "Corpus Chunks: 100%|██████████| 1/1 [00:43<00:00, 43.74s/it]\n",
      "Batches: 100%|██████████| 43/43 [00:05<00:00,  7.75it/s]\n",
      "Batches: 100%|██████████| 319/319 [00:42<00:00,  7.55it/s]\n",
      "Corpus Chunks: 100%|██████████| 1/1 [00:43<00:00, 43.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-tuning finished ---\n",
      "Best model saved to: fine_tuned_patent_model\n"
     ]
    }
   ],
   "source": [
    "# %% Block: Fine-tuning Setup\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "    from sentence_transformers import SentenceTransformer, InputExample, losses, models, util as st_util\n",
    "    from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "    FINETUNE_OK = True\n",
    "except ImportError:\n",
    "    print(\"Error: sentence-transformers or torch not installed. Cannot perform fine-tuning.\")\n",
    "    FINETUNE_OK = False\n",
    "\n",
    "if FINETUNE_OK:\n",
    "    # 1. Model Choice\n",
    "    BASE_MODEL_NAME = \"intfloat/e5-large-v2\"\n",
    "    FINETUNED_MODEL_PATH = Path(\"./fine_tuned_patent_model\")\n",
    "    FINETUNED_MODEL_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "    # 2. Data Configuration\n",
    "    TEXT_TYPE_FINETUNE = \"title_abstract_claims\"\n",
    "\n",
    "    # 3. Training Hyperparameters\n",
    "    NUM_EPOCHS = 3\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    EVAL_BATCH_SIZE = 16\n",
    "    WARMUP_STEPS = 100\n",
    "    LEARNING_RATE = 2e-5\n",
    "    EVALUATION_STEPS = 500\n",
    "    MAX_SEQ_LENGTH = 512\n",
    "    USE_AMP = True\n",
    "\n",
    "    # 4. Validation Set Split\n",
    "    VALIDATION_SPLIT_FRACTION = 0.1\n",
    "    SEED = 2025 # Make sure SEED is defined globally earlier\n",
    "\n",
    "    # --- Ensure Core Data is Loaded ---\n",
    "    try:\n",
    "        # Check if necessary variables exist\n",
    "        if 'CITING_TRAIN' not in locals() or 'NONCITING' not in locals() or 'MAP_DF' not in locals():\n",
    "             print(\"Reloading data...\")\n",
    "             CITING_TRAIN, _, NONCITING, MAP_DF = load_all() # Assuming load_all() is defined\n",
    "        if 'build_corpus' not in locals():\n",
    "             raise NameError(\"build_corpus function not defined.\")\n",
    "    except NameError as e:\n",
    "        print(f\"Error: Required data or function not found: {e}\")\n",
    "        FINETUNE_OK = False # Prevent proceeding\n",
    "\n",
    "\n",
    "if FINETUNE_OK:\n",
    "    print(\"Preparing data for fine-tuning with prefixes...\")\n",
    "\n",
    "    # --- Helper functions for prefixes ---\n",
    "    def add_query_prefix(text):\n",
    "        # Ensure text is a string\n",
    "        if not isinstance(text, str):\n",
    "             text = str(text) # Basic conversion if not string\n",
    "        return f\"query: {text}\"\n",
    "\n",
    "    def add_passage_prefix(text):\n",
    "        # Ensure text is a string\n",
    "        if not isinstance(text, str):\n",
    "             text = str(text) # Basic conversion if not string\n",
    "        return f\"passage: {text}\"\n",
    "\n",
    "    # --- Build Text Maps (ID -> Text) - NO PREFIXES HERE ---\n",
    "    print(\"Building text lookup maps...\")\n",
    "    citing_text_map = {}\n",
    "    # Assuming build_corpus is defined and accessible\n",
    "    citing_ids_ft, citing_texts_ft = build_corpus(CITING_TRAIN, TEXT_TYPE_FINETUNE)\n",
    "    for doc_id, text in zip(citing_ids_ft, citing_texts_ft):\n",
    "        citing_text_map[doc_id] = text\n",
    "\n",
    "    nonciting_text_map = {}\n",
    "    nonciting_ids_ft, nonciting_texts_ft = build_corpus(NONCITING, TEXT_TYPE_FINETUNE)\n",
    "    for doc_id, text in zip(nonciting_ids_ft, nonciting_texts_ft):\n",
    "        nonciting_text_map[doc_id] = text\n",
    "    print(f\"Built maps: {len(citing_text_map)} citing, {len(nonciting_text_map)} non-citing docs.\")\n",
    "\n",
    "\n",
    "    # --- Create Positive Training Examples & Dev Set ---\n",
    "    train_examples = []\n",
    "    dev_queries = {} # {query_id: query_text_with_prefix}\n",
    "    dev_corpus = {} # {doc_id: doc_text_with_prefix}\n",
    "    dev_relevant_docs = {} # {query_id: set(relevant_doc_ids)}\n",
    "\n",
    "    # Split citing patents into train and validation sets\n",
    "    all_citing_ids = list(citing_text_map.keys())\n",
    "    random.seed(SEED) # Use the global seed\n",
    "    random.shuffle(all_citing_ids)\n",
    "    split_idx = int(len(all_citing_ids) * (1 - VALIDATION_SPLIT_FRACTION))\n",
    "    train_citing_ids = set(all_citing_ids[:split_idx])\n",
    "    dev_citing_ids = set(all_citing_ids[split_idx:])\n",
    "\n",
    "    print(f\"Split: {len(train_citing_ids)} train queries, {len(dev_citing_ids)} dev queries.\")\n",
    "\n",
    "    processed_pairs = 0\n",
    "    missing_texts = 0\n",
    "    dev_positives_count = 0\n",
    "    # Use MAP_DF for positive pairs\n",
    "    for _, row in tqdm(MAP_DF.iterrows(), total=len(MAP_DF), desc=\"Processing citation pairs\"):\n",
    "        # Make sure column indices/names match your MAP_DF structure\n",
    "        citing_id = row[0]  # Assuming first column is citing ID\n",
    "        cited_id = row[2]   # Assuming third column is cited ID\n",
    "\n",
    "        query_text_orig = citing_text_map.get(citing_id)\n",
    "        # Positive text can come from citing or non-citing corpus\n",
    "        positive_text_orig = citing_text_map.get(cited_id)\n",
    "        if not positive_text_orig:\n",
    "            positive_text_orig = nonciting_text_map.get(cited_id)\n",
    "\n",
    "        if query_text_orig and positive_text_orig:\n",
    "            # <<< --- ADD PREFIXES HERE --- >>>\n",
    "            prefixed_query = add_query_prefix(query_text_orig)\n",
    "            prefixed_positive = add_passage_prefix(positive_text_orig)\n",
    "\n",
    "            if citing_id in train_citing_ids:\n",
    "                # Use prefixed texts for InputExample\n",
    "                train_examples.append(InputExample(texts=[prefixed_query, prefixed_positive]))\n",
    "                processed_pairs += 1\n",
    "            elif citing_id in dev_citing_ids:\n",
    "                # Prepare data for InformationRetrievalEvaluator using prefixed texts\n",
    "                if citing_id not in dev_queries:\n",
    "                    dev_queries[citing_id] = prefixed_query # Store prefixed query\n",
    "                # Add positive doc to corpus and relevant docs (use prefixed text for corpus)\n",
    "                if cited_id not in dev_corpus:\n",
    "                    dev_corpus[cited_id] = prefixed_positive # Store prefixed passage\n",
    "                dev_relevant_docs.setdefault(citing_id, set()).add(cited_id)\n",
    "                dev_positives_count += 1\n",
    "        else:\n",
    "            missing_texts += 1\n",
    "            # Optional: Log which IDs were missing\n",
    "            # if not query_text_orig: print(f\"Missing text for citing_id: {citing_id}\")\n",
    "            # if not positive_text_orig: print(f\"Missing text for cited_id: {cited_id}\")\n",
    "\n",
    "\n",
    "    print(f\"Created {len(train_examples)} training examples.\")\n",
    "    print(f\"Prepared {len(dev_queries)} dev queries with {dev_positives_count} positive relations.\")\n",
    "    if missing_texts > 0:\n",
    "        print(f\"Warning: Skipped {missing_texts} pairs due to missing text for citing or cited patents.\")\n",
    "\n",
    "    # --- Add Negative Examples to Dev Corpus (With Prefix) ---\n",
    "    # We need some *non-relevant* documents in the dev corpus for the evaluator\n",
    "    # Sample from NONCITING corpus.\n",
    "    num_dev_negatives_needed = len(dev_corpus) * 5 # Aim for ~5x more negatives than positives\n",
    "    dev_negatives_added = 0\n",
    "    nonciting_ids_list = list(nonciting_text_map.keys())\n",
    "    random.shuffle(nonciting_ids_list) # Shuffle to get random negatives\n",
    "\n",
    "    for neg_id in nonciting_ids_list:\n",
    "        if dev_negatives_added >= num_dev_negatives_needed:\n",
    "            break\n",
    "        if neg_id not in dev_corpus: # Avoid adding duplicates or existing positives\n",
    "            neg_text = nonciting_text_map.get(neg_id)\n",
    "            if neg_text: # Ensure text exists\n",
    "                 # <<< --- ADD PREFIX HERE --- >>>\n",
    "                 dev_corpus[neg_id] = add_passage_prefix(neg_text) # Add passage prefix\n",
    "                 dev_negatives_added += 1\n",
    "\n",
    "    print(f\"Added {dev_negatives_added} negative documents (passages) to the dev corpus.\")\n",
    "    print(f\"Total dev corpus size: {len(dev_corpus)}\")\n",
    "\n",
    "    # --- Sanity Check ---\n",
    "    if not train_examples:\n",
    "        print(\"Error: No training examples were created. Check data loading and matching.\")\n",
    "        FINETUNE_OK = False\n",
    "    if not dev_queries or not dev_corpus or not dev_relevant_docs:\n",
    "        print(\"Error: Validation set data is incomplete. Check validation split and data processing.\")\n",
    "        FINETUNE_OK = False\n",
    "    # Check if any dev query ID has an empty relevant set (should not happen if processed correctly)\n",
    "    for qid in dev_queries:\n",
    "        if not dev_relevant_docs.get(qid):\n",
    "             print(f\"Warning: Dev query {qid} has no relevant documents listed in dev_relevant_docs.\")\n",
    "\n",
    "\n",
    "# %% Block: Model, Loss, and Evaluator Setup\n",
    "\n",
    "if FINETUNE_OK:\n",
    "    print(f\"Loading base model: {BASE_MODEL_NAME}\")\n",
    "    # Option 1: Use a pre-trained model as is\n",
    "    model = SentenceTransformer(BASE_MODEL_NAME)\n",
    "\n",
    "    # Option 2: Add pooling layer if needed (e.g., if base is just transformer)\n",
    "    # word_embedding_model = models.Transformer(BASE_MODEL_NAME, max_seq_length=MAX_SEQ_LENGTH)\n",
    "    # pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "    # model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "    # Truncate long patent texts if they exceed model capacity\n",
    "    model.max_seq_length = MAX_SEQ_LENGTH\n",
    "    print(f\"Model max sequence length set to: {model.max_seq_length}\")\n",
    "\n",
    "    # --- Loss Function ---\n",
    "    # MultipleNegativesRankingLoss is recommended for training with (anchor, positive) pairs.\n",
    "    # It uses other examples in the batch as negatives.\n",
    "    loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "    print(\"Using MultipleNegativesRankingLoss.\")\n",
    "\n",
    "    # --- Dataloader ---\n",
    "    # NoDuplicatesDataLoader ensures no duplicate texts are in the same batch,\n",
    "    # useful for MNRL as it prevents trivial negative examples.\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=TRAIN_BATCH_SIZE)\n",
    "    print(f\"Train dataloader created with batch size {TRAIN_BATCH_SIZE}.\")\n",
    "\n",
    "    # --- Evaluator ---\n",
    "    # Uses the prepared dev set components\n",
    "    evaluator = InformationRetrievalEvaluator(\n",
    "        queries=dev_queries,              # dict: {query_id: query_text}\n",
    "        corpus=dev_corpus,                # dict: {doc_id: doc_text}\n",
    "        relevant_docs=dev_relevant_docs,  # dict: {query_id: set(relevant_doc_ids)}\n",
    "        batch_size=EVAL_BATCH_SIZE,\n",
    "        main_score_function='cosine',     # How to compare embeddings\n",
    "        score_functions={'cos_sim': st_util.cos_sim},\n",
    "        name='patent_dev',\n",
    "        show_progress_bar=True,\n",
    "        write_csv=True,                   # Saves detailed eval results\n",
    "    )\n",
    "    print(\"InformationRetrievalEvaluator configured.\")\n",
    "\n",
    "\n",
    "# %% Block: Training Execution\n",
    "\n",
    "if FINETUNE_OK:\n",
    "    print(\"\\n--- Starting Fine-Tuning Training ---\")\n",
    "\n",
    "    # Calculate number of steps per epoch if needed for logging/scheduling\n",
    "    steps_per_epoch = math.ceil(len(train_dataloader))\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "    if EVALUATION_STEPS > steps_per_epoch:\n",
    "        print(f\"Warning: EVALUATION_STEPS ({EVALUATION_STEPS}) > steps per epoch ({steps_per_epoch}). Evaluation will happen less than once per epoch.\")\n",
    "\n",
    "\n",
    "    # --- Run Training ---\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, loss)],\n",
    "        evaluator=evaluator,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        evaluation_steps=EVALUATION_STEPS, # Evaluate every N steps\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        optimizer_params={'lr': LEARNING_RATE},\n",
    "        output_path=str(FINETUNED_MODEL_PATH), # Save checkpoints here\n",
    "        save_best_model=True,          # Save the model with the best MAP score on dev\n",
    "        checkpoint_path=str(FINETUNED_MODEL_PATH / \"checkpoints\"),\n",
    "        checkpoint_save_steps=EVALUATION_STEPS * 2, # Save checkpoints less frequently than eval\n",
    "        checkpoint_save_total_limit=3, # Keep only last 3 checkpoints\n",
    "        use_amp=USE_AMP,                 # Enable mixed precision\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Fine-tuning finished ---\")\n",
    "    print(f\"Best model saved to: {FINETUNED_MODEL_PATH}\")\n",
    "\n",
    "    # Optional: Load the best model immediately for use\n",
    "    # print(\"Loading best fine-tuned model...\")\n",
    "    # fine_tuned_model = SentenceTransformer(str(FINETUNED_MODEL_PATH))\n",
    "    # Now you could potentially replace the model in your DenseIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUSH FINETUNED MODEL TO HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from: fine_tuned_patent_model/checkpoints/checkpoint-2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `save_to_hub` method is deprecated and will be removed in a future version of SentenceTransformers. Please use `push_to_hub` instead for future model uploads.\n",
      "model.safetensors: 100%|██████████| 1.34G/1.34G [18:15<00:00, 1.22MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model successfully uploaded to: https://huggingface.co/petkopetkov/e5-large-v2-patent\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"HF_TOKEN\"] = \"YOUR_TOKEN\"\n",
    "\n",
    "repo_name = \"e5-large-v2-patent\"\n",
    "repo_id = f\"petkopetkov/{repo_name}\"\n",
    "\n",
    "model_path = f\"{FINETUNED_MODEL_PATH}/checkpoints/checkpoint-2898\"\n",
    "\n",
    "print(f\"Loading best model from: {model_path}\")\n",
    "best_model = SentenceTransformer(str(model_path))\n",
    "\n",
    "best_model.save_to_hub(\n",
    "    repo_id=repo_id,\n",
    ")\n",
    "print(f\"✅ Model successfully uploaded to: https://huggingface.co/{repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"---------------------------------------------------------------------\n",
    "Block 5 – Dense model & ANN search                                       \n",
    "---------------------------------------------------------------------\"\"\"\n",
    "class DenseIndexer:\n",
    "    def __init__(self,\n",
    "                 model_name=\"petkopetkov/e5-large-v2-patent\", # Default base model\n",
    "                 finetuned_model_path=None,      # Path to fine-tuned model (optional)\n",
    "                 batch_size=256,                 # Adjust based on GPU memory\n",
    "                 ef_construction=200,            # HNSW build parameter\n",
    "                 M=64,                           # HNSW build parameter\n",
    "                 ef_search=300):                 # HNSW search parameter (can be tuned)\n",
    "        if not DENSE_OK:\n",
    "            raise RuntimeError(\"sentence-transformers or torch is not installed. Cannot initialize DenseIndexer.\")\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"DenseIndexer using device: {self.device}\")\n",
    "        \n",
    "        print(f\"Loading model: \", model_name)\n",
    "        \n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name, device=self.device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Sentence Transformer model from '{model_name}': {e}\")\n",
    "            raise # Re-raise error as model loading is critical\n",
    "\n",
    "        # --- Store parameters ---\n",
    "        self.bs = batch_size\n",
    "        self.index = None\n",
    "        self.doc_emb = None # Optionally store embeddings if needed later\n",
    "        self.doc_ids = None\n",
    "        self.ef_construction = ef_construction\n",
    "        self.M = M\n",
    "        self.ef_search = ef_search # Default search ef\n",
    "\n",
    "    # --- Static methods for prefixing (can be called without class instance) ---\n",
    "    @staticmethod\n",
    "    def _add_query_prefix(text: str) -> str:\n",
    "        \"\"\"Adds 'query: ' prefix.\"\"\"\n",
    "        if not isinstance(text, str): text = str(text) # Basic type safety\n",
    "        return f\"query: {text}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_passage_prefix(text: str) -> str:\n",
    "        \"\"\"Adds 'passage: ' prefix.\"\"\"\n",
    "        if not isinstance(text, str): text = str(text) # Basic type safety\n",
    "        return f\"passage: {text}\"\n",
    "\n",
    "    def _embed(self, texts_with_prefix: list[str], desc: str =\"Embedding\") -> np.ndarray:\n",
    "        \"\"\"Internal embedding function expecting prefixed texts.\"\"\"\n",
    "        if not texts_with_prefix:\n",
    "            print(\"Warning: _embed called with empty list of texts.\")\n",
    "            return np.array([], dtype=np.float32).reshape(0, self.model.get_sentence_embedding_dimension())\n",
    "\n",
    "        print(f\"Embedding {len(texts_with_prefix)} texts ({desc})...\")\n",
    "        try:\n",
    "            embeddings = self.model.encode(\n",
    "                texts_with_prefix,\n",
    "                convert_to_numpy=True,\n",
    "                batch_size=self.bs,\n",
    "                show_progress_bar=True,\n",
    "                normalize_embeddings=True,       # E5/GTE models benefit from normalization\n",
    "                output_value='sentence_embedding',\n",
    "                device=self.device\n",
    "            )\n",
    "            # Ensure output is float32 for hnswlib compatibility if needed\n",
    "            return embeddings.astype(np.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during sentence embedding: {e}\")\n",
    "            # Depending on error, might want to return empty or raise\n",
    "            raise # Re-raise error for clarity\n",
    "\n",
    "    def fit(self, doc_texts: list[str], ids: list[str]):\n",
    "        \"\"\"\n",
    "        Builds the HNSW index from the provided document texts.\n",
    "        Automatically adds the 'passage:' prefix.\n",
    "        \"\"\"\n",
    "        if len(doc_texts) != len(ids):\n",
    "            raise ValueError(f\"Number of document texts ({len(doc_texts)}) must match number of IDs ({len(ids)}).\")\n",
    "        if not doc_texts:\n",
    "            print(\"Warning: fit called with empty documents list. Index will be empty.\")\n",
    "            self.doc_ids = []\n",
    "            self.index = None\n",
    "            self.doc_emb = None\n",
    "            return # Nothing to index\n",
    "\n",
    "        self.doc_ids = list(ids) # Store a copy\n",
    "\n",
    "        # --- Add \"passage:\" prefix before embedding ---\n",
    "        print(f\"Adding 'passage:' prefix to {len(doc_texts)} candidate documents...\")\n",
    "        prefixed_doc_texts = [self._add_passage_prefix(text) for text in tqdm(doc_texts, desc=\"Prefixing passages\")]\n",
    "\n",
    "        # --- Embed the prefixed documents ---\n",
    "        emb = self._embed(prefixed_doc_texts, desc=\"Embedding Candidates\")\n",
    "        if emb.shape[0] == 0: # Handle case where embedding failed or returned empty\n",
    "             print(\"Error: Embeddings could not be generated. Index cannot be built.\")\n",
    "             self.index = None\n",
    "             self.doc_emb = None\n",
    "             return\n",
    "\n",
    "        dim = emb.shape[1]\n",
    "        num_elements = emb.shape[0]\n",
    "        print(f\"Building HNSW index (dim={dim}, M={self.M}, ef_construction={self.ef_construction}, num_elements={num_elements})...\")\n",
    "\n",
    "        # --- Initialize and build HNSW index ---\n",
    "        try:\n",
    "            idx = hnswlib.Index(space='cosine', dim=dim) # Cosine distance is 1 - cosine similarity\n",
    "            idx.init_index(max_elements=num_elements, ef_construction=self.ef_construction, M=self.M)\n",
    "            # Add items requires numpy array of indices 0..N-1\n",
    "            idx.add_items(emb, np.arange(num_elements))\n",
    "            idx.set_ef(self.ef_search) # Set default search ef\n",
    "            self.index = idx\n",
    "            self.doc_emb = emb # Store embeddings if needed\n",
    "            print(\"HNSW index built successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error building HNSW index: {e}\")\n",
    "            self.index = None\n",
    "            self.doc_emb = None\n",
    "            raise # Propagate error\n",
    "\n",
    "\n",
    "    def search(self, query_texts: list[str], top_k: int, current_ef_search: int | None = None) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Searches the index for the given query texts.\n",
    "        Automatically adds the 'query:' prefix. Returns labels (indices) and similarities.\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise RuntimeError(\"Index not fitted or failed to build. Call fit() first.\")\n",
    "        if not query_texts:\n",
    "            print(\"Warning: search called with empty query list.\")\n",
    "            return np.array([]), np.array([]) # Return empty arrays\n",
    "\n",
    "        # --- Determine and set ef_search for this query batch ---\n",
    "        search_ef = current_ef_search if current_ef_search is not None else self.ef_search\n",
    "        if search_ef <= 0:\n",
    "             print(f\"Warning: ef_search value ({search_ef}) is invalid, using default: {self.ef_search}\")\n",
    "             search_ef = self.ef_search\n",
    "        try:\n",
    "            self.index.set_ef(search_ef)\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Failed to set ef_search to {search_ef}. Using previous value. Error: {e}\")\n",
    "             # Proceed with the existing ef setting in the index object\n",
    "        print(f\"Searching with ef_search={self.index.ef}...\") # Print actual ef being used\n",
    "\n",
    "        # --- Add \"query:\" prefix before embedding ---\n",
    "        print(f\"Adding 'query:' prefix to {len(query_texts)} search queries...\")\n",
    "        prefixed_query_texts = [self._add_query_prefix(text) for text in tqdm(query_texts, desc=\"Prefixing queries\")]\n",
    "\n",
    "        # --- Embed the prefixed queries ---\n",
    "        q_emb = self._embed(prefixed_query_texts, desc=\"Embedding Queries\")\n",
    "        if q_emb.shape[0] == 0:\n",
    "             print(\"Error: Query embeddings could not be generated.\")\n",
    "             return np.array([]), np.array([])\n",
    "\n",
    "        # --- Perform KNN search ---\n",
    "        print(f\"Performing knn_query for {len(prefixed_query_texts)} queries (top_k={top_k})...\")\n",
    "        try:\n",
    "             # Ensure k is not larger than the number of items in the index\n",
    "             actual_k = min(top_k, self.index.get_current_count())\n",
    "             if actual_k <= 0:\n",
    "                 print(\"Warning: top_k or index count is zero, cannot perform search.\")\n",
    "                 return np.array([]), np.array([])\n",
    "\n",
    "             labels, distances = self.index.knn_query(q_emb, k=actual_k)\n",
    "             # Convert cosine distances (0=identical, 2=opposite) to similarities (1=identical, -1=opposite)\n",
    "             similarities = 1 - distances\n",
    "             print(\"Dense search complete.\")\n",
    "             return labels, similarities\n",
    "        except Exception as e:\n",
    "             print(f\"Error during knn_query: {e}\")\n",
    "             # Return empty arrays or re-raise depending on desired behavior\n",
    "             return np.array([]), np.array([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"---------------------------------------------------------------------\n",
    "Block 6 – Evaluation metrics & RRF fusion                                \n",
    "---------------------------------------------------------------------\"\"\"\n",
    "from sklearn.preprocessing import minmax_scale # For score normalization\n",
    "\n",
    "# --- Evaluation Functions (unchanged) ---\n",
    "def recall_at_k(true_sets, pred_lists, k=100):\n",
    "    # Handle cases where true set might be empty\n",
    "    hits = [(len(t.intersection(p[:k])) / len(t)) if t else (1.0 if not p else 0.0) for t,p in zip(true_sets, pred_lists)]\n",
    "    return np.mean(hits)\n",
    "\n",
    "def average_precision(true, pred, k=100):\n",
    "    if not true:\n",
    "        return 1.0 if not pred else 0.0 # AP is 1 if nothing needed and nothing predicted\n",
    "    score, hits = 0.0, 0\n",
    "    relevant_k = pred[:k] # Consider only top K predictions\n",
    "    for i, p in enumerate(relevant_k):\n",
    "        if p in true:\n",
    "            hits += 1\n",
    "            score += hits / (i + 1.0)\n",
    "    # Normalize by the minimum of k or number of true items\n",
    "    return score / min(len(true), k)\n",
    "\n",
    "def map_at_k(true_sets, pred_lists, k=100):\n",
    "    return np.mean([average_precision(t, p, k) for t, p in zip(true_sets, pred_lists)])\n",
    "\n",
    "# --- Fusion Functions ---\n",
    "\n",
    "# RRF (unchanged)\n",
    "def rrf_fuse(rank_lists: list[list[str]], k=60):\n",
    "    scores = {}\n",
    "    for rlist in rank_lists:\n",
    "        for rank, doc in enumerate(rlist):\n",
    "            # Handle potential empty lists\n",
    "            if doc:\n",
    "                scores[doc] = scores.get(doc, 0) + 1 / (k + rank + 1)\n",
    "    return [d for d, _ in sorted(scores.items(), key=lambda x: -x[1])]\n",
    "\n",
    "# Weighted Fusion\n",
    "def weighted_fuse(score_dicts: list[dict[str, float]], weights: list[float], default_score=0.0):\n",
    "    if len(score_dicts) != len(weights):\n",
    "        raise ValueError(\"Number of score dictionaries must match number of weights.\")\n",
    "\n",
    "    fused_scores = {}\n",
    "    all_docs = set()\n",
    "    for scores in score_dicts:\n",
    "        all_docs.update(scores.keys())\n",
    "\n",
    "    for doc in all_docs:\n",
    "        weighted_score = 0.0\n",
    "        for i, scores in enumerate(score_dicts):\n",
    "            score = scores.get(doc, default_score) # Use default if doc not found by a method\n",
    "            weighted_score += weights[i] * score\n",
    "        fused_scores[doc] = weighted_score\n",
    "\n",
    "    return [d for d, _ in sorted(fused_scores.items(), key=lambda x: -x[1])]\n",
    "\n",
    "# --- Normalization Function ---\n",
    "def normalize_scores(score_dict: dict[str, float]) -> dict[str, float]:\n",
    "    \"\"\"Normalizes scores within a dictionary to [0, 1] range using min-max.\"\"\"\n",
    "    if not score_dict:\n",
    "        return {}\n",
    "    scores = np.array(list(score_dict.values())).reshape(-1, 1)\n",
    "    # Handle case where all scores are the same (avoid division by zero)\n",
    "    if np.all(scores == scores[0]):\n",
    "         normalized = np.full_like(scores, 0.5) # Or 1.0, or 0.0 depending on preference\n",
    "    else:\n",
    "        normalized = minmax_scale(scores)\n",
    "    return {doc: float(norm_score) for doc, norm_score in zip(score_dict.keys(), normalized.flatten())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building corpora...\n",
      "6831 citing docs – 16837 candidate docs\n",
      "Tokenizing candidate documents for BM25...\n",
      "Loading cached tokens from: .cache/nonciting_corpus_stemmed.pkl\n",
      "Tokenizing query documents for BM25...\n",
      "Loading cached tokens from: .cache/citing_queries_stemmed.pkl\n",
      "Fitting BM25Okapi (k1=1.5, b=0.75)...\n",
      "BM25Okapi model fitted.\n",
      "Calculating BM25 scores in parallel using 23 cores (top 100)...\n",
      "Loading cached BM25 ranks from:  RANKS_BM25.pkl\n",
      "Loading cached BM25 scores from:  SCORES_BM25.pkl\n"
     ]
    }
   ],
   "source": [
    "\"\"\"---------------------------------------------------------------------\n",
    "Block 4 – Sparse model: BM25 with rank_bm25 library\n",
    "---------------------------------------------------------------------\"\"\"\n",
    "# Note: This replaces the previous SparseIndexer and the custom BM25Score logic\n",
    "\n",
    "# Parameters (Keep defaults or use ones from your previous tuning if preferred)\n",
    "BM25_K1 = 1.5\n",
    "BM25_B = 0.75\n",
    "TEXT_TYPE_BM25 = \"title_abstract_claims\" # Text parts to use for BM25\n",
    "\n",
    "# 1) Build corpora (IDs are the same, texts are used for tokenization)\n",
    "print(\"Building corpora...\")\n",
    "CITING_IDS, CITING_TEXTS = build_corpus(CITING_TRAIN, TEXT_TYPE_BM25)\n",
    "NON_IDS,   NON_TEXTS     = build_corpus(NONCITING,    TEXT_TYPE_BM25)\n",
    "print(f\"{len(CITING_IDS)} citing docs – {len(NON_IDS)} candidate docs\")\n",
    "\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import math\n",
    "\n",
    "# ... (Keep previous code in the block: imports, BM25 params, corpus building, tokenization) ...\n",
    "\n",
    "# 2) Tokenize corpora (Assuming this is already done and cached)\n",
    "print(\"Tokenizing candidate documents for BM25...\")\n",
    "tokenized_corpus = cached_tokens(\"nonciting_corpus\", NON_TEXTS, stem=True)\n",
    "print(\"Tokenizing query documents for BM25...\")\n",
    "tokenized_queries = cached_tokens(\"citing_queries\", CITING_TEXTS, stem=True)\n",
    "\n",
    "# 3) Fit BM25Okapi model (Same as before)\n",
    "print(f\"Fitting BM25Okapi (k1={BM25_K1}, b={BM25_B})...\")\n",
    "# Make bm25 global ONLY if using the simple access method below,\n",
    "# otherwise pass necessary data to the worker function.\n",
    "# global bm25 # <-- Be cautious with globals in multiprocessing\n",
    "bm25 = BM25Okapi(tokenized_corpus, k1=BM25_K1, b=BM25_B)\n",
    "print(\"BM25Okapi model fitted.\")\n",
    "\n",
    "# --- Worker Function for Parallel Scoring ---\n",
    "# IMPORTANT: BM25Okapi objects themselves aren't easily pickled for multiprocessing.\n",
    "# We pass the necessary components OR rely on a global object (less safe).\n",
    "# A safer approach is to pass the index data if possible, or re-initialize\n",
    "# lightweight components if fitting isn't the bottleneck (but here it is).\n",
    "# Let's TRY accessing the fitted 'bm25' object as a global, but be aware\n",
    "# this might be problematic on some systems/configurations.\n",
    "\n",
    "# Define the worker function *outside* any class, at the top level of the module/script\n",
    "def score_query_batch(query_indices, bm25_model, query_tokens_list, non_ids_list, top_k):\n",
    "    \"\"\"Scores a batch of queries using the provided BM25 model.\"\"\"\n",
    "    results = {}\n",
    "    for i in query_indices:\n",
    "        query_toks = query_tokens_list[i]\n",
    "        try:\n",
    "             # Calculate scores against the *full* corpus\n",
    "            doc_scores = bm25_model.get_scores(query_toks)\n",
    "\n",
    "            # Get top K indices (no need to sort all 17k scores fully)\n",
    "            # Using argpartition is faster than argsort for finding top K\n",
    "            # Get indices of the top K scores (might not be sorted among themselves)\n",
    "            k_th_score_idx = min(top_k, len(doc_scores) -1) # Ensure k is not larger than available docs\n",
    "            if k_th_score_idx < 0: # Handle empty doc_scores case\n",
    "                 top_n_indices = []\n",
    "                 top_n_scores = []\n",
    "            else:\n",
    "                # Efficiently find the indices of the top K scores\n",
    "                top_n_indices = np.argpartition(doc_scores, -top_k)[-top_k:]\n",
    "                # Sort only the top K indices by score\n",
    "                top_n_scores = doc_scores[top_n_indices]\n",
    "                sorted_top_indices = top_n_indices[np.argsort(top_n_scores)[::-1]] # Sort descending\n",
    "                top_n_scores = doc_scores[sorted_top_indices] # Get sorted scores\n",
    "                top_n_indices = sorted_top_indices\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scoring query index {i}: {e}\")\n",
    "            top_n_indices = []\n",
    "            top_n_scores = []\n",
    "\n",
    "        # Store ranks (doc IDs) and scores\n",
    "        query_id = CITING_IDS[i] # Assumes CITING_IDS is accessible\n",
    "        ranks = [non_ids_list[idx] for idx in top_n_indices]\n",
    "        scores = {non_ids_list[idx]: score for idx, score in zip(top_n_indices, top_n_scores)}\n",
    "        results[query_id] = (ranks, scores)\n",
    "    return results\n",
    "\n",
    "\n",
    "# 4) Get BM25 scores and rankings in parallel\n",
    "TOP_K_BM25 = 100\n",
    "N_CORES = mp.cpu_count() - 1 or 1 # Use all but one core, or 1 if only one exists\n",
    "CHUNK_SIZE = math.ceil(len(tokenized_queries) / N_CORES / 4) # Adjust chunk size for progress updates/memory\n",
    "\n",
    "print(f\"Calculating BM25 scores in parallel using {N_CORES} cores (top {TOP_K_BM25})...\")\n",
    "\n",
    "# Prepare arguments for the worker function\n",
    "# We pass the bm25 object directly - this might work on Linux/macOS via fork\n",
    "# but could fail on Windows (spawn) or if the object is too complex.\n",
    "# Also pass lists needed inside the worker.\n",
    "worker_args = (bm25, tokenized_queries, NON_IDS, TOP_K_BM25)\n",
    "\n",
    "# Create batches of query indices\n",
    "query_indices_all = list(range(len(tokenized_queries)))\n",
    "index_batches = [query_indices_all[i:i + CHUNK_SIZE] for i in range(0, len(tokenized_queries), CHUNK_SIZE)]\n",
    "\n",
    "RANKS_BM25_PATH = 'RANKS_BM25.pkl'\n",
    "\n",
    "if os.path.exists(RANKS_BM25_PATH):\n",
    "    print(\"Loading cached BM25 ranks from: \", RANKS_BM25_PATH)\n",
    "    with open(RANKS_BM25_PATH, 'rb') as f:\n",
    "        RANKS_BM25 = pickle.load(f)\n",
    "else:\n",
    "    print(\"BM25 ranks not found, will be calculated.\")\n",
    "    RANKS_BM25 = {}\n",
    "    \n",
    "SCORES_BM25_PATH = 'SCORES_BM25.pkl'\n",
    "    \n",
    "if os.path.exists(SCORES_BM25_PATH):\n",
    "    print(\"Loading cached BM25 scores from: \", SCORES_BM25_PATH)\n",
    "    with open(SCORES_BM25_PATH, 'rb') as f:\n",
    "        SCORES_BM25 = pickle.load(f)\n",
    "else:\n",
    "    print(\"BM25 scores not found, will be calculated.\")\n",
    "    SCORES_BM25 = {}\n",
    "\n",
    "if not bool(RANKS_BM25) and not bool(SCORES_BM25):\n",
    "    # Run in parallel\n",
    "    # Use imap_unordered for potentially faster processing as results arrive\n",
    "    # Need to wrap the worker call with arguments using partial\n",
    "    process_func = partial(score_query_batch, bm25_model=bm25, query_tokens_list=tokenized_queries, non_ids_list=NON_IDS, top_k=TOP_K_BM25)\n",
    "\n",
    "    with mp.Pool(N_CORES) as pool:\n",
    "        with tqdm(total=len(tokenized_queries), desc=\"Parallel BM25 Scoring\") as pbar:\n",
    "            for result_dict in pool.imap_unordered(process_func, index_batches):\n",
    "                for cid, (ranks, scores) in result_dict.items():\n",
    "                    RANKS_BM25[cid] = ranks\n",
    "                    SCORES_BM25[cid] = scores\n",
    "                pbar.update(len(result_dict)) # Update progress bar by number of queries processed in the batch\n",
    "                \n",
    "    with open(RANKS_BM25_PATH, 'wb') as f:\n",
    "        pickle.dump(RANKS_BM25, f)\n",
    "        \n",
    "    with open(SCORES_BM25_PATH, 'wb') as f:\n",
    "        pickle.dump(SCORES_BM25, f)\n",
    "\n",
    "    print(f\"✅ BM25 pre-ranking done (parallel) – top {TOP_K_BM25} docs stored for {len(RANKS_BM25)} queries\")\n",
    "\n",
    "# --- Make sure GT_SETS is defined after CITING_IDS ---\n",
    "TRUE_DICT = {}\n",
    "for _, row in MAP_DF.iterrows():\n",
    "    TRUE_DICT.setdefault(row[0], []).append(row[2])\n",
    "GT_SETS = [set(TRUE_DICT.get(cid, [])) for cid in CITING_IDS] # Moved here or ensure CITING_IDS is available earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense scores not found, will be calculated.\n",
      "Dense ranks not found, will be calculated.\n",
      "DenseIndexer using device: cuda\n",
      "Loading model:  petkopetkov/e5-large-v2-patent\n",
      "Adding 'passage:' prefix to 16837 candidate documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefixing passages: 100%|██████████| 16837/16837 [00:00<00:00, 303731.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 16837 texts (Embedding Candidates)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches: 100%|██████████| 66/66 [09:52<00:00,  8.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building HNSW index (dim=1024, M=64, ef_construction=200, num_elements=16837)...\n",
      "HNSW index built successfully.\n",
      "Retrieving dense neighbours (top 100)...\n",
      "Searching with ef_search=300...\n",
      "Adding 'query:' prefix to 6831 search queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefixing queries: 100%|██████████| 6831/6831 [00:00<00:00, 651773.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 6831 texts (Embedding Queries)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches: 100%|██████████| 27/27 [03:38<00:00,  8.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing knn_query for 6831 queries (top_k=100)...\n",
      "Dense search complete.\n",
      "✅ Dense retrieval done – top 100 docs stored for 6831 queries\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------ #\n",
    "# 5. Dense model (optional)                                          #\n",
    "# ------------------------------------------------------------------ #\n",
    "TOP_K_DENSE = 100 # Retrieve same number as BM25 for fair comparison/fusion\n",
    "TEXT_TYPE_DENSE = \"title_abstract_claims\" # Or maybe just title_abstract?\n",
    "\n",
    "SCORES_DENSE_PATH = 'SCORES_DENSE.pkl'\n",
    "\n",
    "if os.path.exists(SCORES_DENSE_PATH):\n",
    "    print(\"Loading cached dense scores from: \", SCORES_DENSE_PATH)\n",
    "    with open(SCORES_DENSE_PATH, 'rb') as f:\n",
    "        SCORES_DENSE = pickle.load(f)\n",
    "else:\n",
    "    print(\"Dense scores not found, will be calculated.\")\n",
    "    SCORES_DENSE = {}\n",
    "    \n",
    "RANKS_DENSE_PATH = 'RANKS_DENSE.pkl'\n",
    "\n",
    "if os.path.exists(RANKS_DENSE_PATH):\n",
    "    print(\"Loading cached dense ranks from: \", RANKS_DENSE_PATH)\n",
    "    with open(RANKS_DENSE_PATH, 'rb') as f:\n",
    "        SCORES_DENSE = pickle.load(f)\n",
    "else:\n",
    "    print(\"Dense ranks not found, will be calculated.\")\n",
    "    RANKS_DENSE = {}\n",
    "\n",
    "if DENSE_OK and not bool(RANKS_DENSE) and not bool(SCORES_DENSE):\n",
    "    # Ensure NON_IDS and NON_TEXTS are loaded if running blocks separately\n",
    "    if 'NON_IDS' not in locals():\n",
    "        _, NON_TEXTS = build_corpus(NONCITING, TEXT_TYPE_DENSE)\n",
    "        NON_IDS, _   = build_corpus(NONCITING, \"title\") # Need IDs consistent with BM25\n",
    "\n",
    "    # Ensure CITING_IDS and CITING_TEXTS are loaded\n",
    "    if 'CITING_IDS' not in locals():\n",
    "         CITING_IDS, CITING_TEXTS = build_corpus(CITING_TRAIN, TEXT_TYPE_DENSE)\n",
    "\n",
    "    # Initialize and Fit Dense Indexer\n",
    "    de = DenseIndexer(ef_search=300) # Try a higher ef_search\n",
    "    de.fit(NON_TEXTS, NON_IDS)\n",
    "\n",
    "    # Search\n",
    "    print(f\"Retrieving dense neighbours (top {TOP_K_DENSE})...\")\n",
    "    # Use the *same* CITING_TEXTS as used for BM25 queries if TEXT_TYPE_DENSE is the same\n",
    "    # Or rebuild CITING_TEXTS if using a different TEXT_TYPE_DENSE\n",
    "    labels, sims = de.search(CITING_TEXTS, top_k=TOP_K_DENSE)\n",
    "\n",
    "    RANKS_DENSE = {}\n",
    "    SCORES_DENSE = {} # Store scores for weighted fusion\n",
    "    for i, cid in enumerate(CITING_IDS):\n",
    "        doc_indices = labels[i]\n",
    "        doc_similarities = sims[i]\n",
    "        valid_mask = doc_indices < len(NON_IDS) # Ensure indices are valid\n",
    "        RANKS_DENSE[cid] = [NON_IDS[idx] for idx in doc_indices[valid_mask]]\n",
    "        SCORES_DENSE[cid] = {NON_IDS[idx]: score for idx, score in zip(doc_indices[valid_mask], doc_similarities[valid_mask])}\n",
    "        \n",
    "    with open(RANKS_DENSE_PATH, 'wb') as f:\n",
    "        pickle.dump(RANKS_DENSE, f)\n",
    "        \n",
    "    with open(SCORES_DENSE_PATH, 'wb') as f:\n",
    "        pickle.dump(SCORES_DENSE, f)\n",
    "        \n",
    "    print(f\"✅ Dense retrieval done – top {TOP_K_DENSE} docs stored for {len(RANKS_DENSE)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating RRF Fusion ---\n",
      "Running RRF fusion with k=10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF (k=10):   0%|          | 0/6831 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF (k=10): 100%|██████████| 6831/6831 [00:01<00:00, 4426.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRF k=10: Recall@100=0.9795, MAP@100=0.5226\n",
      "Running RRF fusion with k=60...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF (k=60): 100%|██████████| 6831/6831 [00:01<00:00, 4454.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRF k=60: Recall@100=0.9799, MAP@100=0.5025\n",
      "Running RRF fusion with k=120...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RRF (k=120): 100%|██████████| 6831/6831 [00:01<00:00, 4273.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRF k=120: Recall@100=0.9799, MAP@100=0.4982\n",
      "\n",
      "--- Evaluating Weighted Fusion ---\n",
      "Normalizing scores for weighted fusion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing BM25: 100%|██████████| 6831/6831 [00:03<00:00, 1764.10it/s]\n",
      "Normalizing Dense: 100%|██████████| 6831/6831 [00:02<00:00, 2349.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Weighted fusion with alpha=0.10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weighted (a=0.10): 100%|██████████| 6831/6831 [00:02<00:00, 2300.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted alpha=0.1: Recall@100=0.9833, MAP@100=0.5418\n",
      "Running Weighted fusion with alpha=0.30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weighted (a=0.30): 100%|██████████| 6831/6831 [00:02<00:00, 2296.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted alpha=0.3: Recall@100=0.9804, MAP@100=0.5599\n",
      "Running Weighted fusion with alpha=0.50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weighted (a=0.50): 100%|██████████| 6831/6831 [00:02<00:00, 2329.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted alpha=0.5: Recall@100=0.9699, MAP@100=0.5427\n",
      "Running Weighted fusion with alpha=0.70...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weighted (a=0.70): 100%|██████████| 6831/6831 [00:02<00:00, 2304.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted alpha=0.7: Recall@100=0.9291, MAP@100=0.4829\n",
      "Running Weighted fusion with alpha=0.90...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weighted (a=0.90): 100%|██████████| 6831/6831 [00:02<00:00, 2355.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted alpha=0.9: Recall@100=0.8394, MAP@100=0.4156\n",
      "\n",
      "--- Evaluation Summary ---\n",
      "Candidate results: {\n",
      "  \"RRF_k10\": {\n",
      "    \"rec\": 0.9794710388913287,\n",
      "    \"map\": 0.5226457757100408\n",
      "  },\n",
      "  \"RRF_k60\": {\n",
      "    \"rec\": 0.9799346118186697,\n",
      "    \"map\": 0.5025211588576178\n",
      "  },\n",
      "  \"RRF_k120\": {\n",
      "    \"rec\": 0.9799346118186697,\n",
      "    \"map\": 0.49824767724659696\n",
      "  },\n",
      "  \"Weighted_a0.1\": {\n",
      "    \"rec\": 0.9832894158981116,\n",
      "    \"map\": 0.541825415789975\n",
      "  },\n",
      "  \"Weighted_a0.3\": {\n",
      "    \"rec\": 0.9803859854584492,\n",
      "    \"map\": 0.5599194601612584\n",
      "  },\n",
      "  \"Weighted_a0.5\": {\n",
      "    \"rec\": 0.9698701995803445,\n",
      "    \"map\": 0.5426628786945676\n",
      "  },\n",
      "  \"Weighted_a0.7\": {\n",
      "    \"rec\": 0.9290831015468697,\n",
      "    \"map\": 0.4829415898503858\n",
      "  },\n",
      "  \"Weighted_a0.9\": {\n",
      "    \"rec\": 0.8393597813887668,\n",
      "    \"map\": 0.4156319436905769\n",
      "  }\n",
      "}\n",
      "Best config: {'method': 'weighted', 'alpha': 0.3}   MAP@100 = 0.5599\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------ #\n",
    "# 6. Fusion & metric grid                                            #\n",
    "# ------------------------------------------------------------------ #\n",
    "K_EVAL = 100 # Evaluate at K=100\n",
    "BEST_MAP, BEST_CFG, RESULTS = -1, {}, {}\n",
    "\n",
    "# Ensure GT_SETS is available\n",
    "if 'GT_SETS' not in locals():\n",
    "     GT_SETS = [set(TRUE_DICT.get(cid, [])) for cid in CITING_IDS]\n",
    "\n",
    "# --- RRF Evaluation ---\n",
    "print(\"\\n--- Evaluating RRF Fusion ---\")\n",
    "RANK_POOLS_RRF = [r for r in (RANKS_BM25, RANKS_DENSE) if r] # Use rank lists\n",
    "if len(RANK_POOLS_RRF) > 0:\n",
    "    for k_rrf in [10, 60, 120]:\n",
    "        fused_rrf = {}\n",
    "        print(f\"Running RRF fusion with k={k_rrf}...\")\n",
    "        for cid in tqdm(CITING_IDS, desc=f\"RRF (k={k_rrf})\"):\n",
    "            pools = [p.get(cid, []) for p in RANK_POOLS_RRF]\n",
    "            fused_rrf[cid] = rrf_fuse(pools, k=k_rrf)\n",
    "\n",
    "        preds = [fused_rrf.get(cid, [])[:K_EVAL] for cid in CITING_IDS]\n",
    "        rec_k = recall_at_k(GT_SETS, preds, K_EVAL)\n",
    "        map_k = map_at_k(GT_SETS, preds, K_EVAL)\n",
    "        RESULTS[f\"RRF_k{k_rrf}\"] = dict(rec=rec_k, map=map_k)\n",
    "        print(f\"RRF k={k_rrf}: Recall@{K_EVAL}={rec_k:.4f}, MAP@{K_EVAL}={map_k:.4f}\")\n",
    "        if map_k > BEST_MAP:\n",
    "            BEST_MAP, BEST_CFG = map_k, dict(method=\"rrf\", k=k_rrf)\n",
    "else:\n",
    "    print(\"Skipping RRF evaluation as only one retrieval method seems available.\")\n",
    "\n",
    "\n",
    "# --- Weighted Fusion Evaluation ---\n",
    "print(\"\\n--- Evaluating Weighted Fusion ---\")\n",
    "SCORE_POOLS_WEIGHTED = []\n",
    "if SCORES_BM25: SCORE_POOLS_WEIGHTED.append(SCORES_BM25)\n",
    "if SCORES_DENSE: SCORE_POOLS_WEIGHTED.append(SCORES_DENSE)\n",
    "\n",
    "if len(SCORE_POOLS_WEIGHTED) == 2: # Only makes sense for 2 methods currently\n",
    "    # Normalize scores per query *before* fusion\n",
    "    print(\"Normalizing scores for weighted fusion...\")\n",
    "    NORM_SCORES_BM25 = {cid: normalize_scores(SCORES_BM25.get(cid, {})) for cid in tqdm(CITING_IDS, desc=\"Normalizing BM25\")}\n",
    "    NORM_SCORES_DENSE = {cid: normalize_scores(SCORES_DENSE.get(cid, {})) for cid in tqdm(CITING_IDS, desc=\"Normalizing Dense\")}\n",
    "\n",
    "    for alpha in [0.1, 0.3, 0.5, 0.7, 0.9]: # Alpha = weight for BM25\n",
    "        weights = [alpha, 1 - alpha]\n",
    "        fused_weighted = {}\n",
    "        print(f\"Running Weighted fusion with alpha={alpha:.2f}...\")\n",
    "        for cid in tqdm(CITING_IDS, desc=f\"Weighted (a={alpha:.2f})\"):\n",
    "            scores_to_fuse = [\n",
    "                NORM_SCORES_BM25.get(cid, {}),\n",
    "                NORM_SCORES_DENSE.get(cid, {})\n",
    "            ]\n",
    "            fused_weighted[cid] = weighted_fuse(scores_to_fuse, weights, default_score=0.0) # Use 0 as default for missing docs\n",
    "\n",
    "        preds = [fused_weighted.get(cid, [])[:K_EVAL] for cid in CITING_IDS]\n",
    "        rec_k = recall_at_k(GT_SETS, preds, K_EVAL)\n",
    "        map_k = map_at_k(GT_SETS, preds, K_EVAL)\n",
    "        result_key = f\"Weighted_a{alpha:.1f}\"\n",
    "        RESULTS[result_key] = dict(rec=rec_k, map=map_k)\n",
    "        print(f\"Weighted alpha={alpha:.1f}: Recall@{K_EVAL}={rec_k:.4f}, MAP@{K_EVAL}={map_k:.4f}\")\n",
    "        if map_k > BEST_MAP:\n",
    "            BEST_MAP, BEST_CFG = map_k, dict(method=\"weighted\", alpha=alpha)\n",
    "elif len(SCORE_POOLS_WEIGHTED) == 1:\n",
    "     print(\"Skipping Weighted fusion evaluation as only one retrieval method has scores.\")\n",
    "     # Evaluate the single method\n",
    "     method_name = \"BM25\" if SCORES_BM25 else \"Dense\"\n",
    "     single_method_ranks = RANKS_BM25 if SCORES_BM25 else RANKS_DENSE\n",
    "     preds = [single_method_ranks.get(cid, [])[:K_EVAL] for cid in CITING_IDS]\n",
    "     rec_k = recall_at_k(GT_SETS, preds, K_EVAL)\n",
    "     map_k = map_at_k(GT_SETS, preds, K_EVAL)\n",
    "     RESULTS[method_name] = dict(rec=rec_k, map=map_k)\n",
    "     print(f\"Single Method ({method_name}): Recall@{K_EVAL}={rec_k:.4f}, MAP@{K_EVAL}={map_k:.4f}\")\n",
    "     if map_k > BEST_MAP:\n",
    "        BEST_MAP, BEST_CFG = map_k, dict(method=method_name)\n",
    "else:\n",
    "    print(\"Skipping Weighted fusion evaluation as no scores were generated.\")\n",
    "\n",
    "\n",
    "# --- Final Results ---\n",
    "print(\"\\n--- Evaluation Summary ---\")\n",
    "print(\"Candidate results:\", json.dumps(RESULTS, indent=2))\n",
    "if BEST_CFG:\n",
    "    print(f\"Best config: {BEST_CFG}   MAP@{K_EVAL} = {BEST_MAP:.4f}\")\n",
    "else:\n",
    "    print(\"No valid configurations were evaluated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RANKS_BM25 = 'TEST_RANKS_BM25.pkl'\n",
    "    \n",
    "with open(TEST_RANKS_BM25, 'wb') as f:\n",
    "    pickle.dump(TEST_RANKS_BM25, f)\n",
    "    \n",
    "TEST_RANKS_DENSE = 'TEST_RANKS_DENSE.pkl'\n",
    "\n",
    "with open(TEST_RANKS_DENSE, 'wb') as f:\n",
    "    pickle.dump(TEST_RANKS_DENSE, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating final predictions for test set ---\n",
      "Using text_type for prediction: title_abstract_claims\n",
      "Loaded 1000 test queries.\n",
      "Performing BM25 ranking for test queries (retrieving top 100)...\n",
      "Tokenizing test queries for BM25...\n",
      "Loading cached tokens from: .cache/citing_test_queries_stemmed_stemmed.pkl\n",
      "Calculating BM25 scores for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BM25 Scoring (Test): 100%|██████████| 1000/1000 [26:51<00:00,  1.61s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 ranking complete for 1000 test queries.\n",
      "Performing Dense retrieval for test queries (retrieving top 100)...\n",
      "Adding 'query:' prefix to test queries for Dense search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefixing test queries: 100%|██████████| 1000/1000 [00:00<00:00, 443138.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching with ef_search=300...\n",
      "Adding 'query:' prefix to 1000 search queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefixing queries: 100%|██████████| 1000/1000 [00:00<00:00, 303056.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1000 texts (Embedding Queries)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:27<00:00,  6.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing knn_query for 1000 queries (top_k=100)...\n",
      "Dense search complete.\n",
      "Mapping dense results to document IDs...\n",
      "Dense retrieval complete for 1000 test queries.\n",
      "Applying final ranking strategy: weighted\n",
      "Warning: Best config method 'weighted' could not be applied or prerequisites missing.\n",
      "Falling back to BM25 rankings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fallback BM25 (Test): 100%|██████████| 1000/1000 [00:00<00:00, 739214.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ranking strategy applied: bm25 (fallback)\n",
      "✅ Wrote prediction1.json with predictions for 1000 queries.\n",
      "   ✅ All 1000 queries have 100 predictions (or max available).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Generating final predictions for test set ---\")\n",
    "\n",
    "# 1. Load Test Data & Define Text Representation\n",
    "# Use a consistent text representation, potentially guided by BEST_CFG or fixed\n",
    "# TEXT_TYPE_PREDICT = BEST_CFG.get('text_type', 'title_abstract_claims') # Example: Get from config\n",
    "TEXT_TYPE_PREDICT = \"title_abstract_claims\" # Or fix it: \"title_abstract_claims\"\n",
    "print(f\"Using text_type for prediction: {TEXT_TYPE_PREDICT}\")\n",
    "try:\n",
    "    # Assuming build_corpus is available\n",
    "    TEST_IDS, TEST_TEXTS = build_corpus(CITING_TEST, TEXT_TYPE_PREDICT)\n",
    "    if not TEST_IDS:\n",
    "        raise ValueError(\"Failed to extract Test IDs and Texts. Check CITING_TEST data and build_corpus.\")\n",
    "    print(f\"Loaded {len(TEST_IDS)} test queries.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test data: {e}\")\n",
    "    # Exit or handle gracefully if test data is crucial\n",
    "    TEST_IDS, TEST_TEXTS = [], [] # Ensure variables exist but are empty\n",
    "\n",
    "\n",
    "# 2. BM25 Ranking for Test Set (No prefix changes needed here)\n",
    "\n",
    "TEST_RANKS_BM25_PATH = 'TEST_RANKS_BM25.pkl'\n",
    "\n",
    "if os.path.exists(TEST_RANKS_BM25_PATH):\n",
    "    print(\"Loading cached test BM25 ranks from: \", SCORES_DENSE_PATH)\n",
    "    with open(TEST_RANKS_BM25_PATH, 'rb') as f:\n",
    "        TEST_RANKS_BM25 = pickle.load(f)\n",
    "else:\n",
    "    print(\"Dense scores not found, will be calculated.\")\n",
    "    TEST_RANKS_BM25 = {}\n",
    "    \n",
    "if not bool(TEST_RANKS_BM25):\n",
    "    BM25_RETRIEVAL_DEPTH = 100 # How many candidates to retrieve with BM25\n",
    "    print(f\"Performing BM25 ranking for test queries (retrieving top {BM25_RETRIEVAL_DEPTH})...\")\n",
    "    try:\n",
    "        if 'bm25' not in globals() or bm25 is None:\n",
    "            print(\"BM25 model ('bm25') not found or not fitted. Skipping BM25 ranking.\")\n",
    "        elif not TEST_IDS:\n",
    "            print(\"No test queries loaded. Skipping BM25 ranking.\")\n",
    "        else:\n",
    "            # Tokenize test queries using the stemmed preprocessor used for BM25 training\n",
    "            # Assuming preprocess_and_stem and cached_tokens are available\n",
    "            print(\"Tokenizing test queries for BM25...\")\n",
    "            tokenized_test_queries = cached_tokens(\"citing_test_queries_stemmed\", TEST_TEXTS, stem=True)\n",
    "\n",
    "            if len(tokenized_test_queries) != len(TEST_IDS):\n",
    "                print(\"Warning: Mismatch between number of tokenized queries and test IDs.\")\n",
    "                # Decide how to handle: skip, error out, or try to proceed\n",
    "\n",
    "            print(\"Calculating BM25 scores for test set...\")\n",
    "            for i, query_toks in enumerate(tqdm(tokenized_test_queries, desc=\"BM25 Scoring (Test)\")):\n",
    "                if i >= len(TEST_IDS): break # Safety break if lists mismatch\n",
    "                cid = TEST_IDS[i]\n",
    "                try:\n",
    "                    # Get scores for *all* documents from the fitted bm25 model\n",
    "                    doc_scores = bm25.get_scores(query_toks)\n",
    "                    # Get top N indices (more efficient than sorting all)\n",
    "                    num_candidates = len(doc_scores)\n",
    "                    actual_depth = min(BM25_RETRIEVAL_DEPTH, num_candidates)\n",
    "                    if actual_depth > 0:\n",
    "                        # Efficiently get indices of top N scores\n",
    "                        top_n_indices = np.argpartition(doc_scores, -actual_depth)[-actual_depth:]\n",
    "                        # Sort only the top N indices by score\n",
    "                        top_n_scores = doc_scores[top_n_indices]\n",
    "                        sorted_top_indices = top_n_indices[np.argsort(top_n_scores)[::-1]] # Descending scores\n",
    "                        # Store ranked doc IDs from NON_IDS list\n",
    "                        TEST_RANKS_BM25[cid] = [NON_IDS[idx] for idx in sorted_top_indices if idx < len(NON_IDS)]\n",
    "                    else:\n",
    "                        TEST_RANKS_BM25[cid] = []\n",
    "                except Exception as e:\n",
    "                    print(f\"Error scoring BM25 for test query ID {cid}: {e}\")\n",
    "                    TEST_RANKS_BM25[cid] = [] # Assign empty list on error for this query\n",
    "            print(f\"BM25 ranking complete for {len(TEST_RANKS_BM25)} test queries.\")\n",
    "\n",
    "    except NameError as e:\n",
    "        print(f\"Error during BM25 test ranking setup (required function/variable missing?): {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during BM25 test ranking: {e}\")\n",
    "\n",
    "TEST_RANKS_DENSE_PATH = 'TEST_RANKS_DENSE.pkl'\n",
    "\n",
    "if os.path.exists(TEST_RANKS_DENSE_PATH):\n",
    "    print(\"Loading cached test dense ranks from: \", SCORES_DENSE_PATH)\n",
    "    with open(TEST_RANKS_DENSE_PATH, 'rb') as f:\n",
    "        TEST_RANKS_DENSE = pickle.load(f)\n",
    "else:\n",
    "    print(\"Dense scores not found, will be calculated.\")\n",
    "    TEST_RANKS_DENSE = {}\n",
    "\n",
    "DENSE_RETRIEVAL_DEPTH = 100 # How many candidates to retrieve with Dense model\n",
    "\n",
    "if DENSE_OK and not bool(TEST_RANKS_DENSE):\n",
    "    print(f\"Performing Dense retrieval for test queries (retrieving top {DENSE_RETRIEVAL_DEPTH})...\")\n",
    "    try:\n",
    "        if 'de' not in globals() or de is None or de.index is None:\n",
    "             print(\"DenseIndexer 'de' not found, not fitted, or index is missing. Skipping Dense ranking.\")\n",
    "             DENSE_OK = False # Ensure Dense fusion is skipped later\n",
    "        elif not TEST_IDS:\n",
    "             print(\"No test queries loaded. Skipping Dense ranking.\")\n",
    "             DENSE_OK = False\n",
    "        else:\n",
    "            # --- Add \"query:\" prefix to TEST_TEXTS before searching ---\n",
    "            print(\"Adding 'query:' prefix to test queries for Dense search...\")\n",
    "            # Ensure the prefix function/method is accessible\n",
    "            try:\n",
    "                 # Use the static method from DenseIndexer class definition\n",
    "                 prefixed_test_texts = [DenseIndexer._add_query_prefix(text) for text in tqdm(TEST_TEXTS, desc=\"Prefixing test queries\")]\n",
    "            except NameError:\n",
    "                 # Fallback if DenseIndexer class definition isn't in scope (should be unlikely)\n",
    "                 def _add_query_prefix_local(text): return f\"query: {text}\"\n",
    "                 prefixed_test_texts = [_add_query_prefix_local(text) for text in tqdm(TEST_TEXTS, desc=\"Prefixing test queries (local)\")]\n",
    "\n",
    "\n",
    "            # --- Perform search using the DenseIndexer's search method ---\n",
    "            # Assuming 'de' is the fitted DenseIndexer instance\n",
    "            # Pass the *prefixed* queries\n",
    "            labels, sims = de.search(\n",
    "                prefixed_test_texts,\n",
    "                top_k=DENSE_RETRIEVAL_DEPTH,\n",
    "                current_ef_search=300 # Optional: override default ef_search here if needed\n",
    "            )\n",
    "\n",
    "            if len(labels) != len(TEST_IDS):\n",
    "                 print(\"Warning: Number of dense results doesn't match number of test queries.\")\n",
    "                 # Handle mismatch if necessary\n",
    "\n",
    "            # Map indices back to document IDs\n",
    "            print(\"Mapping dense results to document IDs...\")\n",
    "            for i, cid in enumerate(TEST_IDS):\n",
    "                if i < len(labels):\n",
    "                    doc_indices = labels[i]\n",
    "                    # Ensure indices are valid before mapping to NON_IDS\n",
    "                    valid_mask = (doc_indices >= 0) & (doc_indices < len(NON_IDS)) # Check bounds\n",
    "                    valid_indices = doc_indices[valid_mask]\n",
    "                    TEST_RANKS_DENSE[cid] = [NON_IDS[idx] for idx in valid_indices]\n",
    "                else:\n",
    "                     TEST_RANKS_DENSE[cid] = [] # Assign empty list if results are missing for this query\n",
    "            print(f\"Dense retrieval complete for {len(TEST_RANKS_DENSE)} test queries.\")\n",
    "\n",
    "    except NameError as e:\n",
    "        print(f\"Error during Dense test ranking setup (required object 'de' missing?): {e}\")\n",
    "        DENSE_OK = False # Disable dense fusion if setup fails\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Dense test retrieval: {e}\")\n",
    "        DENSE_OK = False # Disable dense fusion on error\n",
    "\n",
    "\n",
    "# 4. Fusion / Selection based on BEST_CFG\n",
    "final_pred = {}\n",
    "# Determine fusion method from BEST_CFG, default to BM25 if config missing or invalid\n",
    "default_method = 'bm25' if TEST_RANKS_BM25 else ('dense' if TEST_RANKS_DENSE else 'none')\n",
    "fusion_method = BEST_CFG.get('method', default_method) if isinstance(BEST_CFG, dict) else default_method\n",
    "\n",
    "# Ensure necessary rank lists are available for the chosen method\n",
    "can_do_rrf = fusion_method == 'rrf' and DENSE_OK and TEST_RANKS_BM25 and TEST_RANKS_DENSE\n",
    "can_do_dense = fusion_method == 'dense' and DENSE_OK and TEST_RANKS_DENSE\n",
    "can_do_bm25 = fusion_method == 'bm25' and TEST_RANKS_BM25\n",
    "\n",
    "print(f\"Applying final ranking strategy: {fusion_method}\")\n",
    "\n",
    "if can_do_rrf:\n",
    "    fuse_k = BEST_CFG.get('k', 60) # Get RRF k from config, default to 60\n",
    "    print(f\"Using RRF fusion with k={fuse_k}\")\n",
    "    # Assuming rrf_fuse function is available\n",
    "    for cid in tqdm(TEST_IDS, desc=\"RRF Fusion (Test)\"):\n",
    "        pools = [TEST_RANKS_BM25.get(cid, []), TEST_RANKS_DENSE.get(cid, [])]\n",
    "        # Ensure rrf_fuse handles empty lists gracefully\n",
    "        final_pred[cid] = rrf_fuse(pools, k=fuse_k)[:K_SUBMISSION]\n",
    "\n",
    "elif can_do_dense:\n",
    "    print(\"Using Dense rankings only.\")\n",
    "    for cid in tqdm(TEST_IDS, desc=\"Dense Selection (Test)\"):\n",
    "        final_pred[cid] = TEST_RANKS_DENSE.get(cid, [])[:K_SUBMISSION]\n",
    "\n",
    "elif can_do_bm25:\n",
    "     print(\"Using BM25 rankings only.\")\n",
    "     for cid in tqdm(TEST_IDS, desc=\"BM25 Selection (Test)\"):\n",
    "        final_pred[cid] = TEST_RANKS_BM25.get(cid, [])[:K_SUBMISSION]\n",
    "\n",
    "else:\n",
    "    # Fallback strategy if chosen method failed or prerequisites missing\n",
    "    print(f\"Warning: Best config method '{fusion_method}' could not be applied or prerequisites missing.\")\n",
    "    if TEST_RANKS_BM25:\n",
    "        print(\"Falling back to BM25 rankings.\")\n",
    "        fusion_method = 'bm25 (fallback)'\n",
    "        for cid in tqdm(TEST_IDS, desc=\"Fallback BM25 (Test)\"):\n",
    "            final_pred[cid] = TEST_RANKS_BM25.get(cid, [])[:K_SUBMISSION]\n",
    "    elif TEST_RANKS_DENSE and DENSE_OK:\n",
    "         print(\"Falling back to Dense rankings.\")\n",
    "         fusion_method = 'dense (fallback)'\n",
    "         for cid in tqdm(TEST_IDS, desc=\"Fallback Dense (Test)\"):\n",
    "            final_pred[cid] = TEST_RANKS_DENSE.get(cid, [])[:K_SUBMISSION]\n",
    "    else:\n",
    "         print(\"Error: No rankings available (BM25 or Dense) to generate predictions.\")\n",
    "         fusion_method = 'none'\n",
    "         for cid in TEST_IDS:\n",
    "            final_pred[cid] = [] # Assign empty list if no method worked\n",
    "\n",
    "print(f\"Final ranking strategy applied: {fusion_method}\")\n",
    "\n",
    "\n",
    "# 5. Save Predictions\n",
    "output_filename = \"prediction1.json\"\n",
    "try:\n",
    "    # Ensure all query IDs from TEST_IDS are present in the output\n",
    "    for cid in TEST_IDS:\n",
    "        if cid not in final_pred:\n",
    "            print(f\"Warning: Test query ID {cid} missing from final predictions. Adding empty list.\")\n",
    "            final_pred[cid] = []\n",
    "\n",
    "    # Final check: ensure values are lists\n",
    "    for cid in final_pred:\n",
    "        if not isinstance(final_pred[cid], list):\n",
    "             print(f\"Warning: Prediction for {cid} is not a list. Converting.\")\n",
    "             final_pred[cid] = list(final_pred[cid]) # Attempt conversion\n",
    "\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_pred, f, indent=2) # Use indent for readability\n",
    "    print(f\"✅ Wrote {output_filename} with predictions for {len(final_pred)} queries.\")\n",
    "\n",
    "    # Sanity check: count queries with fewer than K_SUBMISSION results\n",
    "    short_preds = sum(1 for cid in final_pred if len(final_pred.get(cid, [])) < K_SUBMISSION)\n",
    "    zero_preds = sum(1 for cid in final_pred if not final_pred.get(cid, []))\n",
    "    if zero_preds > 0:\n",
    "         print(f\"   ⚠️ Warning: {zero_preds} queries have ZERO predictions.\")\n",
    "    elif short_preds > 0:\n",
    "        print(f\"   ⚠️ Warning: {short_preds} queries have fewer than {K_SUBMISSION} predictions.\")\n",
    "    else:\n",
    "        print(f\"   ✅ All {len(final_pred)} queries have {K_SUBMISSION} predictions (or max available).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error writing prediction file '{output_filename}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
