{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11357490,"sourceType":"datasetVersion","datasetId":7030319}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## libraries","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:47:23.285198Z","iopub.execute_input":"2025-04-10T21:47:23.285870Z","iopub.status.idle":"2025-04-10T21:47:26.176024Z","shell.execute_reply.started":"2025-04-10T21:47:23.285847Z","shell.execute_reply":"2025-04-10T21:47:26.175241Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!cp -r /kaggle/input/ir-dataset/* /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:47:35.052570Z","iopub.execute_input":"2025-04-10T21:47:35.053374Z","iopub.status.idle":"2025-04-10T21:47:35.370519Z","shell.execute_reply.started":"2025-04-10T21:47:35.053343Z","shell.execute_reply":"2025-04-10T21:47:35.369784Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"##  task 1","metadata":{}},{"cell_type":"code","source":"! gdown 'https://drive.google.com/uc?id=1cbXtZBzBMRmLsBX4W08pCWnK0dJNYTun'\n! tar -xvzf /kaggle/working/citation_mapping.tar.gz\n!gdown 'https://drive.google.com/uc?id=1DHw-DdRB8xjTqvK-jU3Sol_-8VG4ACZK'\n!tar -xvzf /kaggle/working/embeddings.tar.gz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:48:04.757255Z","iopub.execute_input":"2025-04-10T21:48:04.757557Z","iopub.status.idle":"2025-04-10T21:48:21.535590Z","shell.execute_reply.started":"2025-04-10T21:48:04.757533Z","shell.execute_reply":"2025-04-10T21:48:21.534860Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1cbXtZBzBMRmLsBX4W08pCWnK0dJNYTun\nTo: /kaggle/working/citation_mapping.tar.gz\n100%|█████████████████████████████████████████| 264k/264k [00:00<00:00, 107MB/s]\nCitation_JSONs/\nCitation_JSONs/Citation_Train.json\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1DHw-DdRB8xjTqvK-jU3Sol_-8VG4ACZK\nFrom (redirected): https://drive.google.com/uc?id=1DHw-DdRB8xjTqvK-jU3Sol_-8VG4ACZK&confirm=t&uuid=508432eb-8c35-451f-a466-0c2e8c234a9f\nTo: /kaggle/working/embeddings.tar.gz\n100%|████████████████████████████████████████| 316M/316M [00:04<00:00, 73.0MB/s]\nembeddings_precalculated_docs/\nembeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TA.npy\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_TA.json\nembeddings_precalculated_docs/embeddings_PatentSBERTa_mean_claims.npy\nembeddings_precalculated_docs/app_ids_PatentSBERTa_mean_claims.json\nembeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TAC.npy\nembeddings_precalculated_docs/app_ids_PatentSBERTa_mean_TAC.json\nembeddings_precalculated_docs/embeddings_all-MiniLM-L6-v2_mean_TA.npy\nembeddings_precalculated_docs/app_ids_all-MiniLM-L6-v2_mean_TA.json\nembeddings_precalculated_docs/embeddings_all-MiniLM-L6-v2_mean_claims.npy\nembeddings_precalculated_docs/app_ids_all-MiniLM-L6-v2_mean_claims.json\nembeddings_precalculated_docs/embeddings_all-MiniLM-L6-v2_mean_TAC.npy\nembeddings_precalculated_docs/app_ids_all-MiniLM-L6-v2_mean_TAC.json\nembeddings_precalculated_train/\nembeddings_precalculated_train/embeddings_PatentSBERTa_mean_TA.npy\nembeddings_precalculated_train/app_ids_PatentSBERTa_mean_TA.json\nembeddings_precalculated_train/embeddings_PatentSBERTa_mean_claims.npy\nembeddings_precalculated_train/app_ids_PatentSBERTa_mean_claims.json\nembeddings_precalculated_train/embeddings_PatentSBERTa_mean_TAC.npy\nembeddings_precalculated_train/app_ids_PatentSBERTa_mean_TAC.json\nembeddings_precalculated_train/embeddings_all-MiniLM-L6-v2_mean_TA.npy\nembeddings_precalculated_train/app_ids_all-MiniLM-L6-v2_mean_TA.json\nembeddings_precalculated_train/embeddings_all-MiniLM-L6-v2_mean_claims.npy\nembeddings_precalculated_train/app_ids_all-MiniLM-L6-v2_mean_claims.json\nembeddings_precalculated_train/embeddings_all-MiniLM-L6-v2_mean_TAC.npy\nembeddings_precalculated_train/app_ids_all-MiniLM-L6-v2_mean_TAC.json\nembeddings_precalculated_test/\nembeddings_precalculated_test/embeddings_PatentSBERTa_mean_TA.npy\nembeddings_precalculated_test/app_ids_PatentSBERTa_mean_TA.json\nembeddings_precalculated_test/embeddings_PatentSBERTa_mean_claims.npy\nembeddings_precalculated_test/app_ids_PatentSBERTa_mean_claims.json\nembeddings_precalculated_test/embeddings_PatentSBERTa_mean_TAC.npy\nembeddings_precalculated_test/app_ids_PatentSBERTa_mean_TAC.json\nembeddings_precalculated_test/embeddings_all-MiniLM-L6-v2_mean_TA.npy\nembeddings_precalculated_test/app_ids_all-MiniLM-L6-v2_mean_TA.json\nembeddings_precalculated_test/embeddings_all-MiniLM-L6-v2_mean_claims.npy\nembeddings_precalculated_test/app_ids_all-MiniLM-L6-v2_mean_claims.json\nembeddings_precalculated_test/embeddings_all-MiniLM-L6-v2_mean_TAC.npy\nembeddings_precalculated_test/app_ids_all-MiniLM-L6-v2_mean_TAC.json\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport torch\nimport faiss\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\n\n# ----------------------------\n# Model Settings and Parameters\n# ----------------------------\nMODEL_NAME = \"PatentSBERTa\"  # changed from \"all-MiniLM-L6-v2\" to \"PatentSBERTa\"\nCONTENT_TYPE = \"TA\"          # e.g., \"TA\", \"claims\", or \"TAC\"\nPOOLING = \"mean\"             # pooling strategy\n# Set QUERY_SET to either \"train\", \"test\", or \"split\"\nQUERY_SET = \"test\"  \nSAVE_RESULTS = True\nTOP_N = 100\nK_VALUE = 10  # for evaluation (used in some functions)\nMETRICS_TYPE = \"all\"\nSPLIT_RATIO = 0.8  # For \"split\" mode: fraction for training\n\n# ----------------------------\n# Paths and Files\n# ----------------------------\nBASE_DIR = \"/kaggle/working/\"\nDOC_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings_precalculated_docs\")\nTRAIN_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings_precalculated_train\")\nTEST_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings_precalculated_test\")\nOUTPUT_DIR = os.path.join(BASE_DIR, \"results\")\nCITATION_FILE = os.path.join(BASE_DIR, \"Citation_JSONs/Citation_Train.json\")\n\n# Primary content type (\"TA\")\nCONTENT_TYPE_1 = \"TA\"\nDOC_EMBEDDING_FILE_1 = os.path.join(DOC_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_1}.npy\")\nDOC_APP_IDS_FILE_1 = os.path.join(DOC_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_1}.json\")\nQUERY_EMBEDDING_DIR = TRAIN_EMBEDDING_DIR if QUERY_SET != \"test\" else TEST_EMBEDDING_DIR\nQUERY_EMBEDDING_FILE_1 = os.path.join(QUERY_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_1}.npy\")\nQUERY_APP_IDS_FILE_1 = os.path.join(QUERY_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_1}.json\")\n\n# Secondary content type (e.g., \"claims\")\nCONTENT_TYPE_2 = \"claims\"\nDOC_EMBEDDING_FILE_2 = os.path.join(DOC_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_2}.npy\")\nDOC_APP_IDS_FILE_2 = os.path.join(DOC_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_2}.json\")\nQUERY_EMBEDDING_FILE_2 = os.path.join(QUERY_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_2}.npy\")\nQUERY_APP_IDS_FILE_2 = os.path.join(QUERY_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_2}.json\")\n\n# ----------------------------\n# Device Setup\n# ----------------------------\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ----------------------------\n# Utility Functions\n# ----------------------------\ndef load_embeddings_and_ids(embedding_file, app_ids_file):\n    print(f\"Loading embeddings from {embedding_file}\")\n    embeddings = torch.from_numpy(np.load(embedding_file))\n    with open(app_ids_file, 'r') as f:\n        app_ids = json.load(f)\n    print(f\"Loaded {len(embeddings)} embeddings and {len(app_ids)} app_ids\")\n    return embeddings, app_ids\n\ndef citation_to_citing_to_cited_dict(citations):\n    citing_to_cited_dict = {}\n    for citation in citations:\n        citing_to_cited_dict.setdefault(citation[0], []).append(citation[2])\n    return citing_to_cited_dict\n\ndef get_true_and_predicted(citing_to_cited_dict, recommendations_dict):\n    true_labels = []\n    predicted_labels = []\n    not_in_citation_mapping = 0\n    for citing_id in recommendations_dict.keys():\n        if citing_id in citing_to_cited_dict:\n            true_labels.append(citing_to_cited_dict[citing_id])\n            predicted_labels.append(recommendations_dict[citing_id])\n        else:\n            print(citing_id, \"not in citation mapping\")\n            not_in_citation_mapping += 1\n    return true_labels, predicted_labels, not_in_citation_mapping\n\n# ----------------------------\n# Metric Functions\n# ----------------------------\ndef recall_at_k(true, pred, k):\n    true_set = set(true)\n    if len(true_set) == 0:\n        return 0\n    return len([d for d in pred[:k] if d in true_set]) / len(true_set)\n\ndef precision_at_k(true, pred, k):\n    true_set = set(true)\n    return len([d for d in pred[:k] if d in true_set]) / k\n\ndef average_precision(true, pred):\n    true_set = set(true)\n    if len(true_set) == 0:\n        return 0\n    score = 0.0\n    num_hits = 0.0\n    for i, doc in enumerate(pred):\n        if doc in true_set:\n            num_hits += 1\n            score += num_hits / (i + 1)\n    return score / len(true_set)\n\ndef compute_map(true_labels, predicted_labels):\n    aps = [average_precision(t, p) for t, p in zip(true_labels, predicted_labels)]\n    return np.mean(aps) if aps else 0\n\ndef balanced_accuracy(true, pred, total_docs):\n    true_set = set(true)\n    predicted_set = set(pred)\n    TP = len(true_set & predicted_set)\n    FP = len(predicted_set) - TP\n    FN = len(true_set) - TP\n    TN = total_docs - len(true_set) - FP\n    if (TP + FN) == 0 or (TN + FP) == 0:\n        return 0\n    sensitivity = TP / (TP + FN)\n    specificity = TN / (TN + FP)\n    return (sensitivity + specificity) / 2\n\n# ----------------------------\n# Build FAISS Index Functions\n# ----------------------------\ndef build_faiss_index(embeddings_np):\n    # Ensure embeddings are float32 and normalized for cosine similarity via inner product.\n    embeddings_np = embeddings_np.astype('float32')\n    faiss.normalize_L2(embeddings_np)\n    index = faiss.IndexFlatIP(embeddings_np.shape[1])\n    index.add(embeddings_np)\n    return index\n\ndef faiss_search(index, query_embeddings_np, top_n):\n    query_embeddings_np = query_embeddings_np.astype('float32')\n    faiss.normalize_L2(query_embeddings_np)\n    distances, indices = index.search(query_embeddings_np, top_n)\n    return distances, indices\n\n# ----------------------------\n# Load Embeddings and IDs\n# ----------------------------\ndoc_embeddings_1, doc_app_ids_1 = load_embeddings_and_ids(DOC_EMBEDDING_FILE_1, DOC_APP_IDS_FILE_1)\nquery_embeddings_1, query_app_ids_1 = load_embeddings_and_ids(QUERY_EMBEDDING_FILE_1, QUERY_APP_IDS_FILE_1)\ndoc_embeddings_1 = doc_embeddings_1.to(device)\nquery_embeddings_1 = query_embeddings_1.to(device)\n\nif os.path.exists(DOC_EMBEDDING_FILE_2) and os.path.exists(QUERY_EMBEDDING_FILE_2):\n    doc_embeddings_2, doc_app_ids_2 = load_embeddings_and_ids(DOC_EMBEDDING_FILE_2, DOC_APP_IDS_FILE_2)\n    query_embeddings_2, query_app_ids_2 = load_embeddings_and_ids(QUERY_EMBEDDING_FILE_2, QUERY_APP_IDS_FILE_2)\n    doc_embeddings_2 = doc_embeddings_2.to(device)\n    query_embeddings_2 = query_embeddings_2.to(device)\nelse:\n    print(\"Secondary embeddings not found; using primary only.\")\n    doc_embeddings_2, doc_app_ids_2 = doc_embeddings_1, doc_app_ids_1\n    query_embeddings_2, query_app_ids_2 = query_embeddings_1, query_app_ids_1\n\nassert query_app_ids_1 == query_app_ids_2, \"Mismatch between primary and secondary query app_ids!\"\n\n# ----------------------------\n# Align Document Embeddings\n# ----------------------------\n# Only use documents that are common between both embedding types.\ncommon_doc_ids = list(set(doc_app_ids_1) & set(doc_app_ids_2))\nindices_1 = [doc_app_ids_1.index(doc_id) for doc_id in common_doc_ids]\nindices_2 = [doc_app_ids_2.index(doc_id) for doc_id in common_doc_ids]\ndoc_embeddings_1_aligned = doc_embeddings_1[indices_1]\ndoc_embeddings_2_aligned = doc_embeddings_2[indices_2]\n\n# ----------------------------\n# Compute Rankings using FAISS (Task 1: TA method)\n# ----------------------------\n# Convert tensors to numpy arrays (on CPU) and ensure type float32.\ndoc_emb_1_np = doc_embeddings_1_aligned.cpu().numpy()\nquery_emb_1_np = query_embeddings_1.cpu().numpy()\n\n# Build FAISS index and search.\nindex_TA = build_faiss_index(doc_emb_1_np)\n_, I_TA = faiss_search(index_TA, query_emb_1_np, TOP_N)\nresults_TA = {}\nfor i, q_id in enumerate(query_app_ids_1):\n    results_TA[q_id] = [common_doc_ids[idx] for idx in I_TA[i]]\n\n# ----------------------------\n# Compare Query IDs with Prediction Keys\n# ----------------------------\ntest_query_ids = set(query_app_ids_1)\nprediction_keys = set(results_TA.keys())\nif test_query_ids != prediction_keys:\n    missing_in_predictions = test_query_ids - prediction_keys\n    extra_in_predictions = prediction_keys - test_query_ids\n    print(\"Mismatch found!\")\n    if missing_in_predictions:\n        print(\"The following query IDs are missing in predictions:\", missing_in_predictions)\n    if extra_in_predictions:\n        print(\"The following keys are in predictions but not in the test query IDs:\", extra_in_predictions)\nelse:\n    print(\"All test query IDs are present in the predictions!\")\n\n# ----------------------------\n# Evaluation on Training and Testing (Validation) Splits for Task 1\n# ----------------------------\nif QUERY_SET == \"split\":\n    # Split queries into training and testing sets\n    num_queries = len(query_app_ids_1)\n    indices = np.arange(num_queries)\n    np.random.shuffle(indices)\n    split_point = int(num_queries * SPLIT_RATIO)\n    train_indices = indices[:split_point]\n    test_indices = indices[split_point:]\n    \n    train_query_emb1 = query_embeddings_1[train_indices]\n    train_query_ids = [query_app_ids_1[i] for i in train_indices]\n    test_query_emb1 = query_embeddings_1[test_indices]\n    test_query_ids = [query_app_ids_1[i] for i in test_indices]\n    \n    # Use FAISS search for training and testing splits (using TA method)\n    train_query_emb1_np = train_query_emb1.cpu().numpy()\n    _, I_train = faiss_search(index_TA, train_query_emb1_np, TOP_N)\n    results_train = {}\n    for i, q_id in enumerate(train_query_ids):\n        results_train[q_id] = [common_doc_ids[idx] for idx in I_train[i]]\n    \n    test_query_emb1_np = test_query_emb1.cpu().numpy()\n    _, I_test = faiss_search(index_TA, test_query_emb1_np, TOP_N)\n    results_test = {}\n    for i, q_id in enumerate(test_query_ids):\n        results_test[q_id] = [common_doc_ids[idx] for idx in I_test[i]]\n    \n    # Load citation mapping for evaluation\n    with open(CITATION_FILE, 'r') as f:\n        citations = json.load(f)\n    citing_to_cited_dict = citation_to_citing_to_cited_dict(citations)\n    \n    total_docs = len(common_doc_ids)\n    \n    # --- Task 1 Evaluation ---\n    # For Task 1, compute Recall@10, @20, @50, and @100; MAP; balanced accuracy and precision (top 100)\n    # Training split\n    true_labels_train, predicted_labels_train, _ = get_true_and_predicted(citing_to_cited_dict, results_train)\n    recall_train_10 = np.mean([recall_at_k(t, p, 10) for t, p in zip(true_labels_train, predicted_labels_train)])\n    recall_train_20 = np.mean([recall_at_k(t, p, 20) for t, p in zip(true_labels_train, predicted_labels_train)])\n    recall_train_50 = np.mean([recall_at_k(t, p, 50) for t, p in zip(true_labels_train, predicted_labels_train)])\n    recall_train_100 = np.mean([recall_at_k(t, p, 100) for t, p in zip(true_labels_train, predicted_labels_train)])\n    map_train = compute_map(true_labels_train, predicted_labels_train)\n    bal_acc_train = np.mean([balanced_accuracy(t, p[:100], total_docs) for t, p in zip(true_labels_train, predicted_labels_train)])\n    prec_train = np.mean([precision_at_k(t, p, 100) for t, p in zip(true_labels_train, predicted_labels_train)])\n    print(\"\\nTask 1 Metrics (Training Split):\")\n    print(f\"Recall@10: {recall_train_10:.4f}\")\n    print(f\"Recall@20: {recall_train_20:.4f}\")\n    print(f\"Recall@50: {recall_train_50:.4f}\")\n    print(f\"Recall@100: {recall_train_100:.4f}\")\n    print(f\"Mean Average Precision: {map_train:.4f}\")\n    print(f\"Balanced Accuracy (top 100): {bal_acc_train:.4f}\")\n    print(f\"Precision (top 100): {prec_train:.4f}\")\n    \n    # Testing split\n    true_labels_test, predicted_labels_test, _ = get_true_and_predicted(citing_to_cited_dict, results_test)\n    recall_test_10 = np.mean([recall_at_k(t, p, 10) for t, p in zip(true_labels_test, predicted_labels_test)])\n    recall_test_20 = np.mean([recall_at_k(t, p, 20) for t, p in zip(true_labels_test, predicted_labels_test)])\n    recall_test_50 = np.mean([recall_at_k(t, p, 50) for t, p in zip(true_labels_test, predicted_labels_test)])\n    recall_test_100 = np.mean([recall_at_k(t, p, 100) for t, p in zip(true_labels_test, predicted_labels_test)])\n    map_test = compute_map(true_labels_test, predicted_labels_test)\n    bal_acc_test = np.mean([balanced_accuracy(t, p[:100], total_docs) for t, p in zip(true_labels_test, predicted_labels_test)])\n    prec_test = np.mean([precision_at_k(t, p, 100) for t, p in zip(true_labels_test, predicted_labels_test)])\n    print(\"\\nTask 1 Metrics (Testing Split):\")\n    print(f\"Recall@10: {recall_test_10:.4f}\")\n    print(f\"Recall@20: {recall_test_20:.4f}\")\n    print(f\"Recall@50: {recall_test_50:.4f}\")\n    print(f\"Recall@100: {recall_test_100:.4f}\")\n    print(f\"Mean Average Precision: {map_test:.4f}\")\n    print(f\"Balanced Accuracy (top 100): {bal_acc_test:.4f}\")\n    print(f\"Precision (top 100): {prec_test:.4f}\")\n\n# ----------------------------\n# Save Outputs (Task 1 Only)\n# ----------------------------\nif SAVE_RESULTS:\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    task1_output_file = os.path.join(OUTPUT_DIR, \"prediction1.json\")\n    with open(task1_output_file, 'w') as f:\n        json.dump(results_TA, f)\n    print(f\"\\nSaved Task 1 output to {task1_output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:48:21.537413Z","iopub.execute_input":"2025-04-10T21:48:21.537660Z","iopub.status.idle":"2025-04-10T21:48:26.581178Z","shell.execute_reply.started":"2025-04-10T21:48:21.537638Z","shell.execute_reply":"2025-04-10T21:48:26.580427Z"}},"outputs":[{"name":"stdout","text":"Loading embeddings from /kaggle/working/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TA.npy\nLoaded 16837 embeddings and 16837 app_ids\nLoading embeddings from /kaggle/working/embeddings_precalculated_test/embeddings_PatentSBERTa_mean_TA.npy\nLoaded 1000 embeddings and 1000 app_ids\nLoading embeddings from /kaggle/working/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_claims.npy\nLoaded 16834 embeddings and 16834 app_ids\nLoading embeddings from /kaggle/working/embeddings_precalculated_test/embeddings_PatentSBERTa_mean_claims.npy\nLoaded 1000 embeddings and 1000 app_ids\nAll test query IDs are present in the predictions!\n\nSaved Task 1 output to /kaggle/working/results/prediction1.json\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## task 2 ","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport torch\nfrom sentence_transformers import SentenceTransformer, models, util\nfrom tqdm import tqdm\n# Removed: import argparse\nfrom pathlib import Path\nimport types # Used for SimpleNamespace if preferred\n\nconfig = {\n    # --- Data Files ---\n    'base_dir': '.', # Base directory containing the data files.\n    'query_list_file': 'test_queries.json', # Path to the JSON file with query IDs (relative to base_dir). REQUIRED.\n    'pre_ranking_file': 'shuffled_pre_ranking.json', # Path to the initial ranking JSON (relative to base_dir).\n    'queries_content_file': 'queries_content_with_features.json', # Path to queries content JSON (relative to base_dir).\n    'documents_content_file': 'documents_content_with_features.json', # Path to documents content JSON (relative to base_dir).\n    'output_file': 'prediction2.json', # Path to save the re-ranked prediction JSON (relative to base_dir).\n\n    # --- Model and Text Settings ---\n    'model_name': 'AI-Growth-Lab/PatentSBERTa', # Sentence Transformer model name.\n    'pooling': 'mean', # Pooling strategy (Note: may be overridden by model config). Choices: 'mean', 'max', 'cls'\n    'text_type': 'TA', # Type of text content. Choices: 'TA', 'claims', 'tac1', 'description', 'full', 'features'\n    'max_length': 512, # Max sequence length for the model.\n\n    # --- Execution Settings ---\n    'batch_size': 32, # Batch size for encoding document texts.\n    'device': None # Device: 'cuda', 'cpu', or None (auto-detect).\n}\n\n# --- Auto-detect device if not specified ---\nif config['device'] is None:\n    config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\nelif config['device'] == 'cuda' and not torch.cuda.is_available():\n    print(\"Warning: CUDA requested but not available. Using CPU.\")\n    config['device'] = 'cpu'\n\n# ----------------------------\n# Utility Functions\n# ----------------------------\n\ndef load_json_file(file_path):\n    \"\"\"Load JSON data from a file\"\"\"\n    print(f\"Loading JSON from: {file_path}\")\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        print(f\"Successfully loaded {len(data)} items.\")\n        return data\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n        return None\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred loading {file_path}: {e}\")\n        return None\n\ndef save_json_file(data, file_path):\n    \"\"\"Save data to a JSON file\"\"\"\n    print(f\"Saving JSON to: {file_path}\")\n    try:\n        # Ensure the directory exists before saving\n        output_dir = os.path.dirname(file_path)\n        if output_dir: # Check if dirname returned a non-empty string\n             os.makedirs(output_dir, exist_ok=True)\n        with open(file_path, 'w', encoding='utf-8') as f:\n            json.dump(data, f, indent=2)\n        print(f\"Successfully saved data to {file_path}\")\n    except Exception as e:\n        print(f\"An error occurred saving to {file_path}: {e}\")\n\ndef load_content_data(file_path):\n    \"\"\"Load content data from a JSON file and create a FAN to Content mapping.\"\"\"\n    data = load_json_file(file_path)\n    if data is None:\n        return {}\n\n    content_dict = {}\n    key_options = ['FAN', 'Application_Number'] # Handle potential key variations\n\n    for item in data:\n        fan_key = None\n        for key in key_options:\n            if key in item:\n                # Sometimes Application_Number needs Application_Category appended\n                if key == 'Application_Number' and 'Application_Category' in item:\n                   fan_key = item[key] + item.get('Application_Category', '') # Safely get category\n                else:\n                   fan_key = item[key]\n                break # Found a key, stop looking\n\n        if fan_key and 'Content' in item:\n             content_dict[fan_key] = item['Content']\n        # else:\n        #     print(f\"Warning: Could not find FAN key or Content in item: {item.keys()}\")\n\n    print(f\"Created content dictionary with {len(content_dict)} entries.\")\n    return content_dict\n\n\ndef extract_text(content_dict, text_type=\"TA\"):\n    \"\"\"Extract text from patent content based on text_type\"\"\"\n    if not isinstance(content_dict, dict):\n        # print(f\"Warning: Invalid content_dict provided (type: {type(content_dict)}), expected dict.\")\n        return \"\"\n\n    text_parts = []\n\n    # Note: The original argparse choices included 'TAC', but the function uses 'tac1'.\n    # Adjust config['text_type'] if 'tac1' was intended instead of 'TAC'.\n    if text_type in [\"TA\", \"tac1\", \"full\", \"title_abstract\"]:\n        text_parts.append(content_dict.get(\"title\", \"\"))\n        text_parts.append(content_dict.get(\"pa01\", \"\")) # Abstract\n\n    if text_type in [\"claims\", \"tac1\", \"full\"]:\n        claims = []\n        first_claim = None\n        # Sort keys to approximate claim order, although keys aren't guaranteed sequential\n        sorted_keys = sorted([key for key in content_dict if key.startswith('c-')])\n        for key in sorted_keys:\n            claim_text = content_dict.get(key, \"\")\n            if claim_text:\n                claims.append(claim_text)\n                if first_claim is None and text_type == \"tac1\":\n                    first_claim = claim_text\n\n        if text_type == \"claims\" or text_type == \"full\":\n            text_parts.extend(claims)\n        elif text_type == \"tac1\" and first_claim:\n            text_parts.append(first_claim)\n\n    if text_type in [\"description\", \"full\"]:\n        # Add description paragraphs (keys starting with 'p')\n        desc_parts = []\n        # Sort keys to approximate paragraph order\n        sorted_keys = sorted([key for key in content_dict if key.startswith('p')])\n        for key in sorted_keys:\n             desc_parts.append(content_dict.get(key,\"\"))\n        text_parts.extend(desc_parts)\n\n    if text_type == \"features\":\n        # Extract LLM features if present\n        text_parts.append(content_dict.get(\"features\", \"\"))\n\n    # Join non-empty parts with a space\n    return \" \".join(filter(None, text_parts)).strip()\n\n\n# ----------------------------\n# Main Re-ranking Logic\n# ----------------------------\n\n# Changed function signature to accept config dictionary\ndef main(cfg):\n    # --- Device Setup ---\n    # Use device from the config dictionary\n    device = torch.device(cfg['device'])\n    print(f\"Using device: {device}\")\n\n    # --- Construct Full Paths ---\n    # Use base_dir from the config dictionary\n    def get_full_path(path):\n        if os.path.isabs(path):\n            return path\n        # Use cfg['base_dir'] instead of args.base_dir\n        return os.path.join(cfg['base_dir'], path)\n\n    # Use paths from the config dictionary\n    query_list_file = get_full_path(cfg['query_list_file'])\n    pre_ranking_file = get_full_path(cfg['pre_ranking_file'])\n    queries_content_file = get_full_path(cfg['queries_content_file'])\n    documents_content_file = get_full_path(cfg['documents_content_file'])\n    output_file = get_full_path(cfg['output_file'])\n\n    # --- Load Data ---\n    query_ids = load_json_file(query_list_file)\n    pre_ranking_data = load_json_file(pre_ranking_file)\n    queries_content = load_content_data(queries_content_file)\n    documents_content = load_content_data(documents_content_file)\n\n    if not query_ids or not pre_ranking_data or not queries_content or not documents_content:\n        print(\"Error: Failed to load one or more essential data files. Exiting.\")\n        return\n\n    # --- Load Model ---\n    # Use model_name from the config dictionary\n    print(f\"Loading SentenceTransformer model: {cfg['model_name']}\")\n    try:\n        # Define model architecture if needed (e.g., for specific pooling)\n        # word_embedding_model = models.Transformer(cfg['model_name'], max_seq_length=cfg['max_length'])\n        # pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=cfg['pooling']) # Use cfg['pooling']\n        # model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device=device)\n\n        # Simpler loading if default pooling (mean) is okay or model config handles it\n        model = SentenceTransformer(cfg['model_name'], device=device)\n        model.max_seq_length = cfg['max_length'] # Set max length using cfg['max_length']\n        print(\"Model loaded successfully.\")\n    except Exception as e:\n        print(f\"Error loading model {cfg['model_name']}: {e}\")\n        return\n\n    # --- Re-ranking Process ---\n    # Use text_type from the config dictionary\n    print(f\"Starting re-ranking for {len(query_ids)} queries using '{cfg['text_type']}' content...\")\n    results = {}\n    missing_query_content = 0\n    missing_pre_ranking = 0\n    queries_with_no_valid_docs = 0\n\n    for query_id in tqdm(query_ids, desc=\"Processing queries\"):\n        # 1. Get Query Content\n        query_content_dict = queries_content.get(query_id)\n        if not query_content_dict:\n            # print(f\"Warning: Content not found for query {query_id}\")\n            missing_query_content += 1\n            results[query_id] = [] # Assign empty list if query content missing\n            continue\n\n        # Use text_type from config\n        query_text = extract_text(query_content_dict, cfg['text_type'])\n        if not query_text:\n            # print(f\"Warning: Extracted text is empty for query {query_id} with type '{cfg['text_type']}'\")\n            missing_query_content += 1\n            results[query_id] = []\n            continue\n\n        # 2. Get Candidate Documents\n        candidate_doc_ids = pre_ranking_data.get(query_id)\n        if not candidate_doc_ids:\n            # print(f\"Warning: Pre-ranking not found for query {query_id}\")\n            missing_pre_ranking += 1\n            results[query_id] = []\n            continue\n\n        # 3. Get Candidate Document Content\n        doc_texts = []\n        valid_doc_ids_for_query = []\n        missing_docs_count = 0\n        for doc_id in candidate_doc_ids:\n            doc_content_dict = documents_content.get(doc_id)\n            if not doc_content_dict:\n                # print(f\"Warning: Content not found for document {doc_id} (query {query_id})\")\n                missing_docs_count += 1\n                continue\n\n            # Use text_type from config\n            doc_text = extract_text(doc_content_dict, cfg['text_type'])\n            if doc_text:\n                doc_texts.append(doc_text)\n                valid_doc_ids_for_query.append(doc_id)\n            else:\n                 # print(f\"Warning: Extracted text is empty for document {doc_id} with type '{cfg['text_type']}' (query {query_id})\")\n                 missing_docs_count += 1\n\n\n        if not valid_doc_ids_for_query:\n            # print(f\"Warning: No valid document texts found for query {query_id} after checking {len(candidate_doc_ids)} candidates.\")\n            queries_with_no_valid_docs += 1\n            results[query_id] = [] # Assign empty list if no valid docs\n            continue\n\n        # 4. Generate Embeddings (On-the-fly)\n        try:\n            # Use batch_size from config\n            query_embedding = model.encode(\n                query_text,\n                convert_to_tensor=True,\n                show_progress_bar=False,\n                batch_size=1 # Batch size for query is usually 1\n            )\n            doc_embeddings = model.encode(\n                doc_texts,\n                convert_to_tensor=True,\n                show_progress_bar=False,\n                batch_size=cfg['batch_size'] # Use cfg['batch_size']\n            )\n        except Exception as e:\n            print(f\"Error during encoding for query {query_id}: {e}\")\n            results[query_id] = candidate_doc_ids # Fallback to original order on error\n            continue\n\n\n        # 5. Calculate Similarities\n        # Ensure embeddings are on the same device for cosine similarity\n        query_embedding = query_embedding.to(device)\n        doc_embeddings = doc_embeddings.to(device)\n\n        cosine_scores = util.cos_sim(query_embedding, doc_embeddings)[0] # Get the first row of scores\n        cosine_scores = cosine_scores.cpu().numpy() # Move scores to CPU for sorting\n\n        # 6. Rank Documents\n        # Combine scores with their original valid doc_ids\n        doc_scores = list(zip(valid_doc_ids_for_query, cosine_scores))\n\n        # Sort by score in descending order\n        doc_scores.sort(key=lambda x: x[1], reverse=True)\n\n        # Get the sorted list of document IDs\n        re_ranked_doc_ids = [doc_id for doc_id, score in doc_scores]\n\n        # If some original docs were missing content, append their IDs at the end\n        # (or handle differently if needed - e.g., exclude them)\n        original_candidate_set = set(candidate_doc_ids)\n        reranked_set = set(re_ranked_doc_ids)\n        missing_from_reranked = list(original_candidate_set - reranked_set)\n        final_ranked_list = re_ranked_doc_ids + missing_from_reranked\n\n        results[query_id] = final_ranked_list[:len(candidate_doc_ids)] # Ensure max length is original candidate count\n\n\n    # --- Report Missing Data ---\n    print(\"\\n--- Re-ranking Summary ---\")\n    print(f\"Total queries processed: {len(query_ids)}\")\n    if missing_query_content > 0:\n        print(f\"Warning: Content missing or empty for {missing_query_content} queries.\")\n    if missing_pre_ranking > 0:\n        print(f\"Warning: Pre-ranking data missing for {missing_pre_ranking} queries.\")\n    if queries_with_no_valid_docs > 0:\n        print(f\"Warning: {queries_with_no_valid_docs} queries had no valid documents with content.\")\n    print(f\"Number of queries in results: {len(results)}\")\n\n\n    # --- Save Results ---\n    # Use output_file from config\n    save_json_file(results, output_file)\n\n    print(\"\\nRe-ranking complete.\")\n\nmain(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T21:48:26.582160Z","iopub.execute_input":"2025-04-10T21:48:26.582402Z","iopub.status.idle":"2025-04-10T21:48:34.046658Z","shell.execute_reply.started":"2025-04-10T21:48:26.582385Z","shell.execute_reply":"2025-04-10T21:48:34.045780Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading JSON from: ./test_queries.json\nSuccessfully loaded 10 items.\nLoading JSON from: ./shuffled_pre_ranking.json\nSuccessfully loaded 30 items.\nLoading JSON from: ./queries_content_with_features.json\nSuccessfully loaded 30 items.\nCreated content dictionary with 30 entries.\nLoading JSON from: ./documents_content_with_features.json\nSuccessfully loaded 900 items.\nCreated content dictionary with 900 entries.\nLoading SentenceTransformer model: AI-Growth-Lab/PatentSBERTa\nModel loaded successfully.\nStarting re-ranking for 10 queries using 'TA' content...\n","output_type":"stream"},{"name":"stderr","text":"Processing queries: 100%|██████████| 10/10 [00:05<00:00,  1.80it/s]","output_type":"stream"},{"name":"stdout","text":"\n--- Re-ranking Summary ---\nTotal queries processed: 10\nNumber of queries in results: 10\nSaving JSON to: ./prediction2.json\nSuccessfully saved data to ./prediction2.json\n\nRe-ranking complete.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"    ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}