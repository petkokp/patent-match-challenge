{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11357490,"sourceType":"datasetVersion","datasetId":7030319}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## libraries","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:02:31.932347Z","iopub.execute_input":"2025-04-10T22:02:31.932661Z","iopub.status.idle":"2025-04-10T22:02:34.831118Z","shell.execute_reply.started":"2025-04-10T22:02:31.932637Z","shell.execute_reply":"2025-04-10T22:02:34.830154Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!cp -r /kaggle/input/ir-dataset/* /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:02:34.832561Z","iopub.execute_input":"2025-04-10T22:02:34.832765Z","iopub.status.idle":"2025-04-10T22:02:35.158843Z","shell.execute_reply.started":"2025-04-10T22:02:34.832747Z","shell.execute_reply":"2025-04-10T22:02:35.158131Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"##  task 1","metadata":{}},{"cell_type":"code","source":"! gdown 'https://drive.google.com/uc?id=1cbXtZBzBMRmLsBX4W08pCWnK0dJNYTun'\n! tar -xvzf /kaggle/working/citation_mapping.tar.gz\n!gdown 'https://drive.google.com/uc?id=1DHw-DdRB8xjTqvK-jU3Sol_-8VG4ACZK'\n!tar -xvzf /kaggle/working/embeddings.tar.gz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:02:38.489600Z","iopub.execute_input":"2025-04-10T22:02:38.489908Z","iopub.status.idle":"2025-04-10T22:02:52.087555Z","shell.execute_reply.started":"2025-04-10T22:02:38.489883Z","shell.execute_reply":"2025-04-10T22:02:52.086825Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1cbXtZBzBMRmLsBX4W08pCWnK0dJNYTun\nTo: /kaggle/working/citation_mapping.tar.gz\n100%|████████████████████████████████████████| 264k/264k [00:00<00:00, 91.4MB/s]\nCitation_JSONs/\nCitation_JSONs/Citation_Train.json\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1DHw-DdRB8xjTqvK-jU3Sol_-8VG4ACZK\nFrom (redirected): https://drive.google.com/uc?id=1DHw-DdRB8xjTqvK-jU3Sol_-8VG4ACZK&confirm=t&uuid=6536d8e2-cd6f-4bac-8e42-e9e5f5607340\nTo: /kaggle/working/embeddings.tar.gz\n100%|█████████████████████████████████████████| 316M/316M [00:01<00:00, 256MB/s]\nembeddings_precalculated_docs/\nembeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TA.npy\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"embeddings_precalculated_docs/app_ids_PatentSBERTa_mean_TA.json\nembeddings_precalculated_docs/embeddings_PatentSBERTa_mean_claims.npy\nembeddings_precalculated_docs/app_ids_PatentSBERTa_mean_claims.json\nembeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TAC.npy\nembeddings_precalculated_docs/app_ids_PatentSBERTa_mean_TAC.json\nembeddings_precalculated_docs/embeddings_all-MiniLM-L6-v2_mean_TA.npy\nembeddings_precalculated_docs/app_ids_all-MiniLM-L6-v2_mean_TA.json\nembeddings_precalculated_docs/embeddings_all-MiniLM-L6-v2_mean_claims.npy\nembeddings_precalculated_docs/app_ids_all-MiniLM-L6-v2_mean_claims.json\nembeddings_precalculated_docs/embeddings_all-MiniLM-L6-v2_mean_TAC.npy\nembeddings_precalculated_docs/app_ids_all-MiniLM-L6-v2_mean_TAC.json\nembeddings_precalculated_train/\nembeddings_precalculated_train/embeddings_PatentSBERTa_mean_TA.npy\nembeddings_precalculated_train/app_ids_PatentSBERTa_mean_TA.json\nembeddings_precalculated_train/embeddings_PatentSBERTa_mean_claims.npy\nembeddings_precalculated_train/app_ids_PatentSBERTa_mean_claims.json\nembeddings_precalculated_train/embeddings_PatentSBERTa_mean_TAC.npy\nembeddings_precalculated_train/app_ids_PatentSBERTa_mean_TAC.json\nembeddings_precalculated_train/embeddings_all-MiniLM-L6-v2_mean_TA.npy\nembeddings_precalculated_train/app_ids_all-MiniLM-L6-v2_mean_TA.json\nembeddings_precalculated_train/embeddings_all-MiniLM-L6-v2_mean_claims.npy\nembeddings_precalculated_train/app_ids_all-MiniLM-L6-v2_mean_claims.json\nembeddings_precalculated_train/embeddings_all-MiniLM-L6-v2_mean_TAC.npy\nembeddings_precalculated_train/app_ids_all-MiniLM-L6-v2_mean_TAC.json\nembeddings_precalculated_test/\nembeddings_precalculated_test/embeddings_PatentSBERTa_mean_TA.npy\nembeddings_precalculated_test/app_ids_PatentSBERTa_mean_TA.json\nembeddings_precalculated_test/embeddings_PatentSBERTa_mean_claims.npy\nembeddings_precalculated_test/app_ids_PatentSBERTa_mean_claims.json\nembeddings_precalculated_test/embeddings_PatentSBERTa_mean_TAC.npy\nembeddings_precalculated_test/app_ids_PatentSBERTa_mean_TAC.json\nembeddings_precalculated_test/embeddings_all-MiniLM-L6-v2_mean_TA.npy\nembeddings_precalculated_test/app_ids_all-MiniLM-L6-v2_mean_TA.json\nembeddings_precalculated_test/embeddings_all-MiniLM-L6-v2_mean_claims.npy\nembeddings_precalculated_test/app_ids_all-MiniLM-L6-v2_mean_claims.json\nembeddings_precalculated_test/embeddings_all-MiniLM-L6-v2_mean_TAC.npy\nembeddings_precalculated_test/app_ids_all-MiniLM-L6-v2_mean_TAC.json\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"## task 1","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport torch\nimport faiss\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\n\n# ----------------------------\n# Model Settings and Parameters\n# ----------------------------\n# Model and content configuration. (PatentSBERTa has been chosen for its performance on patent texts.)\nMODEL_NAME = \"PatentSBERTa\"  # SentenceTransformer model optimized for patent text\nCONTENT_TYPE = \"TA\"          # Content type to use (e.g., \"TA\" for Title-Abstract, \"claims\", or \"TAC\")\nPOOLING = \"mean\"             # Pooling strategy for sentence embeddings\nQUERY_SET = \"test\"           # Choose between \"train\", \"test\", or \"split\" set for queries\nSAVE_RESULTS = True          # Flag to control whether to save outputs\nTOP_N = 100                  # Top N retrieval results to fetch\nK_VALUE = 10                 # Parameter used in evaluation functions\nMETRICS_TYPE = \"all\"         # Specify which metrics to calculate\nSPLIT_RATIO = 0.8            # Split ratio for training (used when QUERY_SET is \"split\")\n\n# ----------------------------\n# Paths and Files\n# ----------------------------\n# Define base directories and construct file paths for pre-computed embeddings and output results.\nBASE_DIR = \"/kaggle/working/\"\nDOC_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings_precalculated_docs\")\nTRAIN_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings_precalculated_train\")\nTEST_EMBEDDING_DIR = os.path.join(BASE_DIR, \"embeddings_precalculated_test\")\nOUTPUT_DIR = os.path.join(BASE_DIR, \"results\")\nCITATION_FILE = os.path.join(BASE_DIR, \"Citation_JSONs/Citation_Train.json\")\n\n# Define primary content type files (e.g., Title-Abstract)\nCONTENT_TYPE_1 = \"TA\"\nDOC_EMBEDDING_FILE_1 = os.path.join(DOC_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_1}.npy\")\nDOC_APP_IDS_FILE_1 = os.path.join(DOC_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_1}.json\")\nQUERY_EMBEDDING_DIR = TRAIN_EMBEDDING_DIR if QUERY_SET != \"test\" else TEST_EMBEDDING_DIR\nQUERY_EMBEDDING_FILE_1 = os.path.join(QUERY_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_1}.npy\")\nQUERY_APP_IDS_FILE_1 = os.path.join(QUERY_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_1}.json\")\n\n# Define secondary content type files (e.g., claims) for additional analysis if available\nCONTENT_TYPE_2 = \"claims\"\nDOC_EMBEDDING_FILE_2 = os.path.join(DOC_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_2}.npy\")\nDOC_APP_IDS_FILE_2 = os.path.join(DOC_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_2}.json\")\nQUERY_EMBEDDING_FILE_2 = os.path.join(QUERY_EMBEDDING_DIR, f\"embeddings_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_2}.npy\")\nQUERY_APP_IDS_FILE_2 = os.path.join(QUERY_EMBEDDING_DIR, f\"app_ids_{MODEL_NAME}_{POOLING}_{CONTENT_TYPE_2}.json\")\n\n# ----------------------------\n# Device Setup\n# ----------------------------\n# Set device to GPU if available for performance, otherwise fall back to CPU.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ----------------------------\n# Utility Functions\n# ----------------------------\ndef load_embeddings_and_ids(embedding_file, app_ids_file):\n    \"\"\"\n    Load embeddings from a NumPy file and corresponding application IDs from a JSON file.\n    The embeddings are loaded as a PyTorch tensor.\n    \"\"\"\n    print(f\"Loading embeddings from {embedding_file}\")\n    embeddings = torch.from_numpy(np.load(embedding_file))\n    with open(app_ids_file, 'r') as f:\n        app_ids = json.load(f)\n    print(f\"Loaded {len(embeddings)} embeddings and {len(app_ids)} app_ids\")\n    return embeddings, app_ids\n\ndef citation_to_citing_to_cited_dict(citations):\n    \"\"\"\n    Convert a list of citation relationships into a dictionary mapping each citing document to\n    the list of cited document IDs.\n    \"\"\"\n    citing_to_cited_dict = {}\n    for citation in citations:\n        citing_to_cited_dict.setdefault(citation[0], []).append(citation[2])\n    return citing_to_cited_dict\n\ndef get_true_and_predicted(citing_to_cited_dict, recommendations_dict):\n    \"\"\"\n    For each citing document in recommendations, retrieve the true list of cited documents.\n    Returns lists of true and predicted labels along with a count of unmatched queries.\n    \"\"\"\n    true_labels = []\n    predicted_labels = []\n    not_in_citation_mapping = 0\n    for citing_id in recommendations_dict.keys():\n        if citing_id in citing_to_cited_dict:\n            true_labels.append(citing_to_cited_dict[citing_id])\n            predicted_labels.append(recommendations_dict[citing_id])\n        else:\n            print(citing_id, \"not in citation mapping\")\n            not_in_citation_mapping += 1\n    return true_labels, predicted_labels, not_in_citation_mapping\n\n# ----------------------------\n# Metric Functions\n# ----------------------------\ndef recall_at_k(true, pred, k):\n    \"\"\"\n    Calculate recall at k: the proportion of true relevant documents found in the top k predictions.\n    \"\"\"\n    true_set = set(true)\n    if len(true_set) == 0:\n        return 0\n    return len([d for d in pred[:k] if d in true_set]) / len(true_set)\n\ndef precision_at_k(true, pred, k):\n    \"\"\"\n    Calculate precision at k: the proportion of top k retrieved documents that are relevant.\n    \"\"\"\n    true_set = set(true)\n    return len([d for d in pred[:k] if d in true_set]) / k\n\ndef average_precision(true, pred):\n    \"\"\"\n    Compute the average precision score for a single query.\n    \"\"\"\n    true_set = set(true)\n    if len(true_set) == 0:\n        return 0\n    score = 0.0\n    num_hits = 0.0\n    for i, doc in enumerate(pred):\n        if doc in true_set:\n            num_hits += 1\n            score += num_hits / (i + 1)\n    return score / len(true_set)\n\ndef compute_map(true_labels, predicted_labels):\n    \"\"\"\n    Compute the mean average precision (MAP) across queries.\n    \"\"\"\n    aps = [average_precision(t, p) for t, p in zip(true_labels, predicted_labels)]\n    return np.mean(aps) if aps else 0\n\ndef balanced_accuracy(true, pred, total_docs):\n    \"\"\"\n    Calculate balanced accuracy, defined as the average of sensitivity and specificity.\n    This function uses the total document count to infer true negatives.\n    \"\"\"\n    true_set = set(true)\n    predicted_set = set(pred)\n    TP = len(true_set & predicted_set)\n    FP = len(predicted_set) - TP\n    FN = len(true_set) - TP\n    TN = total_docs - len(true_set) - FP\n    if (TP + FN) == 0 or (TN + FP) == 0:\n        return 0\n    sensitivity = TP / (TP + FN)\n    specificity = TN / (TN + FP)\n    return (sensitivity + specificity) / 2\n\n# ----------------------------\n# Build FAISS Index Functions\n# ----------------------------\ndef build_faiss_index(embeddings_np):\n    \"\"\"\n    Build a FAISS index for inner product (cosine similarity when vectors are L2 normalized).\n    The embeddings are converted to float32 and normalized before indexing.\n    \"\"\"\n    embeddings_np = embeddings_np.astype('float32')\n    faiss.normalize_L2(embeddings_np)  # Normalize to unit norm to enable cosine similarity via inner product.\n    index = faiss.IndexFlatIP(embeddings_np.shape[1])\n    index.add(embeddings_np)\n    return index\n\ndef faiss_search(index, query_embeddings_np, top_n):\n    \"\"\"\n    Search the FAISS index with query embeddings.\n    Normalizes the queries and returns the top_n indices along with their similarity scores.\n    \"\"\"\n    query_embeddings_np = query_embeddings_np.astype('float32')\n    faiss.normalize_L2(query_embeddings_np)\n    distances, indices = index.search(query_embeddings_np, top_n)\n    return distances, indices\n\n# ----------------------------\n# Load Embeddings and IDs for Primary and Secondary Content Types\n# ----------------------------\ndoc_embeddings_1, doc_app_ids_1 = load_embeddings_and_ids(DOC_EMBEDDING_FILE_1, DOC_APP_IDS_FILE_1)\nquery_embeddings_1, query_app_ids_1 = load_embeddings_and_ids(QUERY_EMBEDDING_FILE_1, QUERY_APP_IDS_FILE_1)\ndoc_embeddings_1 = doc_embeddings_1.to(device)\nquery_embeddings_1 = query_embeddings_1.to(device)\n\nif os.path.exists(DOC_EMBEDDING_FILE_2) and os.path.exists(QUERY_EMBEDDING_FILE_2):\n    doc_embeddings_2, doc_app_ids_2 = load_embeddings_and_ids(DOC_EMBEDDING_FILE_2, DOC_APP_IDS_FILE_2)\n    query_embeddings_2, query_app_ids_2 = load_embeddings_and_ids(QUERY_EMBEDDING_FILE_2, QUERY_APP_IDS_FILE_2)\n    doc_embeddings_2 = doc_embeddings_2.to(device)\n    query_embeddings_2 = query_embeddings_2.to(device)\nelse:\n    print(\"Secondary embeddings not found; using primary only.\")\n    doc_embeddings_2, doc_app_ids_2 = doc_embeddings_1, doc_app_ids_1\n    query_embeddings_2, query_app_ids_2 = query_embeddings_1, query_app_ids_1\n\nassert query_app_ids_1 == query_app_ids_2, \"Mismatch between primary and secondary query app_ids!\"\n\n# ----------------------------\n# Align Document Embeddings\n# ----------------------------\n# Use only documents that are present in both primary and secondary embedding sets.\ncommon_doc_ids = list(set(doc_app_ids_1) & set(doc_app_ids_2))\nindices_1 = [doc_app_ids_1.index(doc_id) for doc_id in common_doc_ids]\nindices_2 = [doc_app_ids_2.index(doc_id) for doc_id in common_doc_ids]\ndoc_embeddings_1_aligned = doc_embeddings_1[indices_1]\ndoc_embeddings_2_aligned = doc_embeddings_2[indices_2]\n\n# ----------------------------\n# Compute Rankings using FAISS (Task 1: TA method)\n# ----------------------------\n# Convert aligned tensor embeddings to numpy arrays (on CPU) for FAISS.\ndoc_emb_1_np = doc_embeddings_1_aligned.cpu().numpy()\nquery_emb_1_np = query_embeddings_1.cpu().numpy()\n\n# Build the FAISS index with the document embeddings and perform the search.\nindex_TA = build_faiss_index(doc_emb_1_np)\n_, I_TA = faiss_search(index_TA, query_emb_1_np, TOP_N)\n\n# Associate query IDs with the ranked document IDs.\nresults_TA = {}\nfor i, q_id in enumerate(query_app_ids_1):\n    results_TA[q_id] = [common_doc_ids[idx] for idx in I_TA[i]]\n\n# ----------------------------\n# Validate Query IDs in Predictions\n# ----------------------------\n# Check that all test query IDs are present in the predictions.\ntest_query_ids = set(query_app_ids_1)\nprediction_keys = set(results_TA.keys())\nif test_query_ids != prediction_keys:\n    missing_in_predictions = test_query_ids - prediction_keys\n    extra_in_predictions = prediction_keys - test_query_ids\n    print(\"Mismatch found!\")\n    if missing_in_predictions:\n        print(\"The following query IDs are missing in predictions:\", missing_in_predictions)\n    if extra_in_predictions:\n        print(\"The following keys are in predictions but not in the test query IDs:\", extra_in_predictions)\nelse:\n    print(\"All test query IDs are present in the predictions!\")\n\n# ----------------------------\n# Evaluation on Training and Testing (Validation) Splits for Task 1\n# ----------------------------\nif QUERY_SET == \"split\":\n    # Split the query set into training and testing parts.\n    num_queries = len(query_app_ids_1)\n    indices = np.arange(num_queries)\n    np.random.shuffle(indices)\n    split_point = int(num_queries * SPLIT_RATIO)\n    train_indices = indices[:split_point]\n    test_indices = indices[split_point:]\n    \n    train_query_emb1 = query_embeddings_1[train_indices]\n    train_query_ids = [query_app_ids_1[i] for i in train_indices]\n    test_query_emb1 = query_embeddings_1[test_indices]\n    test_query_ids = [query_app_ids_1[i] for i in test_indices]\n    \n    # Run FAISS search for training and testing splits using the TA method.\n    train_query_emb1_np = train_query_emb1.cpu().numpy()\n    _, I_train = faiss_search(index_TA, train_query_emb1_np, TOP_N)\n    results_train = {}\n    for i, q_id in enumerate(train_query_ids):\n        results_train[q_id] = [common_doc_ids[idx] for idx in I_train[i]]\n    \n    test_query_emb1_np = test_query_emb1.cpu().numpy()\n    _, I_test = faiss_search(index_TA, test_query_emb1_np, TOP_N)\n    results_test = {}\n    for i, q_id in enumerate(test_query_ids):\n        results_test[q_id] = [common_doc_ids[idx] for idx in I_test[i]]\n    \n    # Load the citation mappings for evaluation.\n    with open(CITATION_FILE, 'r') as f:\n        citations = json.load(f)\n    citing_to_cited_dict = citation_to_citing_to_cited_dict(citations)\n    \n    total_docs = len(common_doc_ids)\n    \n    # --- Task 1 Evaluation ---\n    # Compute evaluation metrics such as Recall@K, MAP, balanced accuracy, and precision for training split.\n    true_labels_train, predicted_labels_train, _ = get_true_and_predicted(citing_to_cited_dict, results_train)\n    recall_train_10 = np.mean([recall_at_k(t, p, 10) for t, p in zip(true_labels_train, predicted_labels_train)])\n    recall_train_20 = np.mean([recall_at_k(t, p, 20) for t, p in zip(true_labels_train, predicted_labels_train)])\n    recall_train_50 = np.mean([recall_at_k(t, p, 50) for t, p in zip(true_labels_train, predicted_labels_train)])\n    recall_train_100 = np.mean([recall_at_k(t, p, 100) for t, p in zip(true_labels_train, predicted_labels_train)])\n    map_train = compute_map(true_labels_train, predicted_labels_train)\n    bal_acc_train = np.mean([balanced_accuracy(t, p[:100], total_docs) for t, p in zip(true_labels_train, predicted_labels_train)])\n    prec_train = np.mean([precision_at_k(t, p, 100) for t, p in zip(true_labels_train, predicted_labels_train)])\n    print(\"\\nTask 1 Metrics (Training Split):\")\n    print(f\"Recall@10: {recall_train_10:.4f}\")\n    print(f\"Recall@20: {recall_train_20:.4f}\")\n    print(f\"Recall@50: {recall_train_50:.4f}\")\n    print(f\"Recall@100: {recall_train_100:.4f}\")\n    print(f\"Mean Average Precision: {map_train:.4f}\")\n    print(f\"Balanced Accuracy (top 100): {bal_acc_train:.4f}\")\n    print(f\"Precision (top 100): {prec_train:.4f}\")\n    \n    # Compute metrics for testing split.\n    true_labels_test, predicted_labels_test, _ = get_true_and_predicted(citing_to_cited_dict, results_test)\n    recall_test_10 = np.mean([recall_at_k(t, p, 10) for t, p in zip(true_labels_test, predicted_labels_test)])\n    recall_test_20 = np.mean([recall_at_k(t, p, 20) for t, p in zip(true_labels_test, predicted_labels_test)])\n    recall_test_50 = np.mean([recall_at_k(t, p, 50) for t, p in zip(true_labels_test, predicted_labels_test)])\n    recall_test_100 = np.mean([recall_at_k(t, p, 100) for t, p in zip(true_labels_test, predicted_labels_test)])\n    map_test = compute_map(true_labels_test, predicted_labels_test)\n    bal_acc_test = np.mean([balanced_accuracy(t, p[:100], total_docs) for t, p in zip(true_labels_test, predicted_labels_test)])\n    prec_test = np.mean([precision_at_k(t, p, 100) for t, p in zip(true_labels_test, predicted_labels_test)])\n    print(\"\\nTask 1 Metrics (Testing Split):\")\n    print(f\"Recall@10: {recall_test_10:.4f}\")\n    print(f\"Recall@20: {recall_test_20:.4f}\")\n    print(f\"Recall@50: {recall_test_50:.4f}\")\n    print(f\"Recall@100: {recall_test_100:.4f}\")\n    print(f\"Mean Average Precision: {map_test:.4f}\")\n    print(f\"Balanced Accuracy (top 100): {bal_acc_test:.4f}\")\n    print(f\"Precision (top 100): {prec_test:.4f}\")\n\n# ----------------------------\n# Save Outputs (Task 1 Only)\n# ----------------------------\nif SAVE_RESULTS:\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    task1_output_file = os.path.join(OUTPUT_DIR, \"prediction1.json\")\n    with open(task1_output_file, 'w') as f:\n        json.dump(results_TA, f)\n    print(f\"\\nSaved Task 1 output to {task1_output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:03:07.035960Z","iopub.execute_input":"2025-04-10T22:03:07.036648Z","iopub.status.idle":"2025-04-10T22:03:12.200378Z","shell.execute_reply.started":"2025-04-10T22:03:07.036620Z","shell.execute_reply":"2025-04-10T22:03:12.199672Z"}},"outputs":[{"name":"stdout","text":"Loading embeddings from /kaggle/working/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_TA.npy\nLoaded 16837 embeddings and 16837 app_ids\nLoading embeddings from /kaggle/working/embeddings_precalculated_test/embeddings_PatentSBERTa_mean_TA.npy\nLoaded 1000 embeddings and 1000 app_ids\nLoading embeddings from /kaggle/working/embeddings_precalculated_docs/embeddings_PatentSBERTa_mean_claims.npy\nLoaded 16834 embeddings and 16834 app_ids\nLoading embeddings from /kaggle/working/embeddings_precalculated_test/embeddings_PatentSBERTa_mean_claims.npy\nLoaded 1000 embeddings and 1000 app_ids\nAll test query IDs are present in the predictions!\n\nSaved Task 1 output to /kaggle/working/results/prediction1.json\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## task 2","metadata":{}},{"cell_type":"code","source":"###############################\n# Re-ranking with SentenceTransformer\n###############################\nimport os\nimport json\nimport numpy as np\nimport torch\nfrom sentence_transformers import SentenceTransformer, models, util\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport types  # Used for SimpleNamespace if preferred\n\n# ----------------------------\n# Configuration Dictionary\n# ----------------------------\n# Defines file paths, model parameters, and execution settings.\nconfig = {\n    # --- Data Files ---\n    'base_dir': '.',  # Base directory containing the data files.\n    'query_list_file': 'test_queries.json',  # JSON file with list of query IDs.\n    'pre_ranking_file': 'shuffled_pre_ranking.json',  # Pre-ranked candidate document IDs.\n    'queries_content_file': 'queries_content_with_features.json',  # JSON file containing query details.\n    'documents_content_file': 'documents_content_with_features.json',  # JSON file containing document details.\n    'output_file': 'prediction2.json',  # File to save the re-ranked results.\n\n    # --- Model and Text Settings ---\n    'model_name': 'AI-Growth-Lab/PatentSBERTa',  # SentenceTransformer model to use.\n    'pooling': 'mean',  # Pooling strategy (e.g., 'mean', 'max', 'cls')\n    'text_type': 'TA',  # The section of the patent text to use for embedding.\n    'max_length': 512,  # Maximum token length for input sequences.\n\n    # --- Execution Settings ---\n    'batch_size': 32,  # Batch size for encoding documents.\n    'device': None  # Device to run on: 'cuda', 'cpu', or None (auto-detect).\n}\n\n# Auto-detect device if not specified in config.\nif config['device'] is None:\n    config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\nelif config['device'] == 'cuda' and not torch.cuda.is_available():\n    print(\"Warning: CUDA requested but not available. Using CPU.\")\n    config['device'] = 'cpu'\n\n# ----------------------------\n# Utility Functions for Data I/O and Processing\n# ----------------------------\n\ndef load_json_file(file_path):\n    \"\"\"\n    Loads a JSON file and returns the parsed data.\n    Reports errors if the file is missing or cannot be decoded.\n    \"\"\"\n    print(f\"Loading JSON from: {file_path}\")\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n        print(f\"Successfully loaded {len(data)} items.\")\n        return data\n    except FileNotFoundError:\n        print(f\"Error: File not found at {file_path}\")\n        return None\n    except json.JSONDecodeError:\n        print(f\"Error: Could not decode JSON from {file_path}\")\n        return None\n    except Exception as e:\n        print(f\"An unexpected error occurred loading {file_path}: {e}\")\n        return None\n\ndef save_json_file(data, file_path):\n    \"\"\"\n    Saves the provided data as a JSON file. Creates the output directory if it does not exist.\n    \"\"\"\n    print(f\"Saving JSON to: {file_path}\")\n    try:\n        output_dir = os.path.dirname(file_path)\n        if output_dir:  # Create directory if needed.\n            os.makedirs(output_dir, exist_ok=True)\n        with open(file_path, 'w', encoding='utf-8') as f:\n            json.dump(data, f, indent=2)\n        print(f\"Successfully saved data to {file_path}\")\n    except Exception as e:\n        print(f\"An error occurred saving to {file_path}: {e}\")\n\ndef load_content_data(file_path):\n    \"\"\"\n    Loads patent content from a JSON file and builds a mapping from a unique identifier (FAN or Application_Number)\n    to the document content.\n    \"\"\"\n    data = load_json_file(file_path)\n    if data is None:\n        return {}\n\n    content_dict = {}\n    # Keys to try in order for a unique identifier.\n    key_options = ['FAN', 'Application_Number']\n\n    for item in data:\n        fan_key = None\n        for key in key_options:\n            if key in item:\n                # Optionally append Application_Category for Application_Number.\n                if key == 'Application_Number' and 'Application_Category' in item:\n                   fan_key = item[key] + item.get('Application_Category', '')\n                else:\n                   fan_key = item[key]\n                break  # Stop searching after finding a valid key.\n\n        if fan_key and 'Content' in item:\n             content_dict[fan_key] = item['Content']\n        # Uncomment below to get warnings for missing keys.\n        # else:\n        #     print(f\"Warning: Could not find FAN key or Content in item: {item.keys()}\")\n\n    print(f\"Created content dictionary with {len(content_dict)} entries.\")\n    return content_dict\n\ndef extract_text(content_dict, text_type=\"TA\"):\n    \"\"\"\n    Extracts and concatenates specific sections of text from the patent content based on the requested text_type.\n    Depending on text_type, it may include title, abstract, claims, description, or features.\n    \"\"\"\n    if not isinstance(content_dict, dict):\n        return \"\"\n\n    text_parts = []\n\n    # For Title-Abstract based types.\n    if text_type in [\"TA\", \"tac1\", \"full\", \"title_abstract\"]:\n        text_parts.append(content_dict.get(\"title\", \"\"))\n        text_parts.append(content_dict.get(\"pa01\", \"\"))  # Typically holds the abstract.\n\n    # For claims or a combination type.\n    if text_type in [\"claims\", \"tac1\", \"full\"]:\n        claims = []\n        first_claim = None\n        # Sort claim keys to preserve an approximate order.\n        sorted_keys = sorted([key for key in content_dict if key.startswith('c-')])\n        for key in sorted_keys:\n            claim_text = content_dict.get(key, \"\")\n            if claim_text:\n                claims.append(claim_text)\n                if first_claim is None and text_type == \"tac1\":\n                    first_claim = claim_text\n        if text_type in [\"claims\", \"full\"]:\n            text_parts.extend(claims)\n        elif text_type == \"tac1\" and first_claim:\n            text_parts.append(first_claim)\n\n    # For descriptions or full text types.\n    if text_type in [\"description\", \"full\"]:\n        desc_parts = []\n        # Sort paragraph keys for natural reading order.\n        sorted_keys = sorted([key for key in content_dict if key.startswith('p')])\n        for key in sorted_keys:\n             desc_parts.append(content_dict.get(key, \"\"))\n        text_parts.extend(desc_parts)\n\n    # For features, directly include if present.\n    if text_type == \"features\":\n        text_parts.append(content_dict.get(\"features\", \"\"))\n\n    # Combine non-empty text parts into a single string.\n    return \" \".join(filter(None, text_parts)).strip()\n\n# ----------------------------\n# Main Re-ranking Logic\n# ----------------------------\ndef main(cfg):\n    \"\"\"\n    Main function for the re-ranking process.\n    Steps:\n      1. Setup device and full path resolution using the configuration.\n      2. Load query IDs, pre-ranking results, and content for both queries and documents.\n      3. Load the SentenceTransformer model for on-the-fly embedding generation.\n      4. For each query, extract text, generate embeddings, calculate cosine similarity scores with candidate documents,\n         and then re-rank the candidates.\n      5. Handle missing data (queries or documents) robustly.\n      6. Save the final re-ranked results to disk.\n    \"\"\"\n    # --- Device Setup ---\n    device = torch.device(cfg['device'])\n    print(f\"Using device: {device}\")\n\n    # --- Construct Full Paths for Files ---\n    def get_full_path(path):\n        return path if os.path.isabs(path) else os.path.join(cfg['base_dir'], path)\n\n    query_list_file = get_full_path(cfg['query_list_file'])\n    pre_ranking_file = get_full_path(cfg['pre_ranking_file'])\n    queries_content_file = get_full_path(cfg['queries_content_file'])\n    documents_content_file = get_full_path(cfg['documents_content_file'])\n    output_file = get_full_path(cfg['output_file'])\n\n    # --- Load Data Files ---\n    query_ids = load_json_file(query_list_file)\n    pre_ranking_data = load_json_file(pre_ranking_file)\n    queries_content = load_content_data(queries_content_file)\n    documents_content = load_content_data(documents_content_file)\n\n    if not query_ids or not pre_ranking_data or not queries_content or not documents_content:\n        print(\"Error: Failed to load one or more essential data files. Exiting.\")\n        return\n\n    # --- Load SentenceTransformer Model ---\n    print(f\"Loading SentenceTransformer model: {cfg['model_name']}\")\n    try:\n        # Uncomment and modify below if you need a custom architecture; otherwise, default settings are used.\n        # word_embedding_model = models.Transformer(cfg['model_name'], max_seq_length=cfg['max_length'])\n        # pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=cfg['pooling'])\n        # model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device=device)\n\n        # Simpler loading when default pooling is sufficient or handled by the model.\n        model = SentenceTransformer(cfg['model_name'], device=device)\n        model.max_seq_length = cfg['max_length']  # Set maximum token length\n        print(\"Model loaded successfully.\")\n    except Exception as e:\n        print(f\"Error loading model {cfg['model_name']}: {e}\")\n        return\n\n    # --- Re-ranking Process ---\n    print(f\"Starting re-ranking for {len(query_ids)} queries using '{cfg['text_type']}' content...\")\n    results = {}\n    missing_query_content = 0\n    missing_pre_ranking = 0\n    queries_with_no_valid_docs = 0\n\n    for query_id in tqdm(query_ids, desc=\"Processing queries\"):\n        # 1. Extract Query Content and Text\n        query_content_dict = queries_content.get(query_id)\n        if not query_content_dict:\n            missing_query_content += 1\n            results[query_id] = []  # No content available; assign empty result.\n            continue\n\n        query_text = extract_text(query_content_dict, cfg['text_type'])\n        if not query_text:\n            missing_query_content += 1\n            results[query_id] = []\n            continue\n\n        # 2. Retrieve Candidate Document IDs for the Query\n        candidate_doc_ids = pre_ranking_data.get(query_id)\n        if not candidate_doc_ids:\n            missing_pre_ranking += 1\n            results[query_id] = []\n            continue\n\n        # 3. Collect Content for Candidate Documents\n        doc_texts = []\n        valid_doc_ids_for_query = []\n        missing_docs_count = 0\n        for doc_id in candidate_doc_ids:\n            doc_content_dict = documents_content.get(doc_id)\n            if not doc_content_dict:\n                missing_docs_count += 1\n                continue\n\n            doc_text = extract_text(doc_content_dict, cfg['text_type'])\n            if doc_text:\n                doc_texts.append(doc_text)\n                valid_doc_ids_for_query.append(doc_id)\n            else:\n                missing_docs_count += 1\n\n        if not valid_doc_ids_for_query:\n            queries_with_no_valid_docs += 1\n            results[query_id] = []  # No valid document text found.\n            continue\n\n        # 4. Generate Embeddings for the Query and Candidate Documents\n        try:\n            query_embedding = model.encode(\n                query_text,\n                convert_to_tensor=True,\n                show_progress_bar=False,\n                batch_size=1  # Usually one query at a time.\n            )\n            doc_embeddings = model.encode(\n                doc_texts,\n                convert_to_tensor=True,\n                show_progress_bar=False,\n                batch_size=cfg['batch_size']\n            )\n        except Exception as e:\n            print(f\"Error during encoding for query {query_id}: {e}\")\n            results[query_id] = candidate_doc_ids  # Fallback to original ranking.\n            continue\n\n        # 5. Calculate Cosine Similarity Scores\n        query_embedding = query_embedding.to(device)\n        doc_embeddings = doc_embeddings.to(device)\n        cosine_scores = util.cos_sim(query_embedding, doc_embeddings)[0]\n        cosine_scores = cosine_scores.cpu().numpy()\n\n        # 6. Re-rank Documents Based on Similarity\n        doc_scores = list(zip(valid_doc_ids_for_query, cosine_scores))\n        # Sort candidate documents by descending similarity score.\n        doc_scores.sort(key=lambda x: x[1], reverse=True)\n        re_ranked_doc_ids = [doc_id for doc_id, score in doc_scores]\n\n        # Append any candidate docs missing content at the end to preserve original candidate count.\n        original_candidate_set = set(candidate_doc_ids)\n        reranked_set = set(re_ranked_doc_ids)\n        missing_from_reranked = list(original_candidate_set - reranked_set)\n        final_ranked_list = re_ranked_doc_ids + missing_from_reranked\n\n        results[query_id] = final_ranked_list[:len(candidate_doc_ids)]\n\n    # --- Report Data Issues ---\n    print(\"\\n--- Re-ranking Summary ---\")\n    print(f\"Total queries processed: {len(query_ids)}\")\n    if missing_query_content > 0:\n        print(f\"Warning: Content missing or empty for {missing_query_content} queries.\")\n    if missing_pre_ranking > 0:\n        print(f\"Warning: Pre-ranking data missing for {missing_pre_ranking} queries.\")\n    if queries_with_no_valid_docs > 0:\n        print(f\"Warning: {queries_with_no_valid_docs} queries had no valid documents with content.\")\n    print(f\"Number of queries in results: {len(results)}\")\n\n    # --- Save the Final Re-ranked Results ---\n    save_json_file(results, output_file)\n    print(\"\\nRe-ranking complete.\")\n\n# Execute the re-ranking procedure using the provided configuration.\nmain(config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T22:03:14.897688Z","iopub.execute_input":"2025-04-10T22:03:14.898293Z","iopub.status.idle":"2025-04-10T22:03:22.405783Z","shell.execute_reply.started":"2025-04-10T22:03:14.898269Z","shell.execute_reply":"2025-04-10T22:03:22.404881Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading JSON from: ./test_queries.json\nSuccessfully loaded 10 items.\nLoading JSON from: ./shuffled_pre_ranking.json\nSuccessfully loaded 30 items.\nLoading JSON from: ./queries_content_with_features.json\nSuccessfully loaded 30 items.\nCreated content dictionary with 30 entries.\nLoading JSON from: ./documents_content_with_features.json\nSuccessfully loaded 900 items.\nCreated content dictionary with 900 entries.\nLoading SentenceTransformer model: AI-Growth-Lab/PatentSBERTa\nModel loaded successfully.\nStarting re-ranking for 10 queries using 'TA' content...\n","output_type":"stream"},{"name":"stderr","text":"Processing queries: 100%|██████████| 10/10 [00:05<00:00,  1.77it/s]","output_type":"stream"},{"name":"stdout","text":"\n--- Re-ranking Summary ---\nTotal queries processed: 10\nNumber of queries in results: 10\nSaving JSON to: ./prediction2.json\nSuccessfully saved data to ./prediction2.json\n\nRe-ranking complete.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}