{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading JSON from: ./test_queries.json\n",
      "Successfully loaded 10 items.\n",
      "Loading JSON from: ./shuffled_pre_ranking.json\n",
      "Successfully loaded 30 items.\n",
      "Loading JSON from: ./queries_content_with_features.json\n",
      "Successfully loaded 30 items.\n",
      "Created content dictionary with 30 entries.\n",
      "Loading JSON from: ./documents_content_with_features.json\n",
      "Successfully loaded 900 items.\n",
      "Created content dictionary with 900 entries.\n",
      "Loading SentenceTransformer model: AI-Growth-Lab/PatentSBERTa\n",
      "Model loaded successfully.\n",
      "Starting re-ranking for 10 queries using 'TA' content...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 10/10 [00:01<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Re-ranking Summary ---\n",
      "Total queries processed: 10\n",
      "Number of queries in results: 10\n",
      "Saving JSON to: ./prediction2.json\n",
      "Successfully saved data to ./prediction2.json\n",
      "\n",
      "Re-ranking complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, models, util\n",
    "from tqdm import tqdm\n",
    "# Removed: import argparse\n",
    "from pathlib import Path\n",
    "import types # Used for SimpleNamespace if preferred\n",
    "\n",
    "config = {\n",
    "    # --- Data Files ---\n",
    "    'base_dir': '.', # Base directory containing the data files.\n",
    "    'query_list_file': 'test_queries.json', # Path to the JSON file with query IDs (relative to base_dir). REQUIRED.\n",
    "    'pre_ranking_file': 'shuffled_pre_ranking.json', # Path to the initial ranking JSON (relative to base_dir).\n",
    "    'queries_content_file': 'queries_content_with_features.json', # Path to queries content JSON (relative to base_dir).\n",
    "    'documents_content_file': 'documents_content_with_features.json', # Path to documents content JSON (relative to base_dir).\n",
    "    'output_file': 'prediction2.json', # Path to save the re-ranked prediction JSON (relative to base_dir).\n",
    "\n",
    "    # --- Model and Text Settings ---\n",
    "    'model_name': 'AI-Growth-Lab/PatentSBERTa', # Sentence Transformer model name.\n",
    "    'pooling': 'mean', # Pooling strategy (Note: may be overridden by model config). Choices: 'mean', 'max', 'cls'\n",
    "    'text_type': 'TA', # Type of text content. Choices: 'TA', 'claims', 'tac1', 'description', 'full', 'features'\n",
    "    'max_length': 512, # Max sequence length for the model.\n",
    "\n",
    "    # --- Execution Settings ---\n",
    "    'batch_size': 32, # Batch size for encoding document texts.\n",
    "    'device': None # Device: 'cuda', 'cpu', or None (auto-detect).\n",
    "}\n",
    "\n",
    "# --- Auto-detect device if not specified ---\n",
    "if config['device'] is None:\n",
    "    config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "elif config['device'] == 'cuda' and not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA requested but not available. Using CPU.\")\n",
    "    config['device'] = 'cpu'\n",
    "\n",
    "# ----------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load JSON data from a file\"\"\"\n",
    "    print(f\"Loading JSON from: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Successfully loaded {len(data)} items.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_json_file(data, file_path):\n",
    "    \"\"\"Save data to a JSON file\"\"\"\n",
    "    print(f\"Saving JSON to: {file_path}\")\n",
    "    try:\n",
    "        # Ensure the directory exists before saving\n",
    "        output_dir = os.path.dirname(file_path)\n",
    "        if output_dir: # Check if dirname returned a non-empty string\n",
    "             os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"Successfully saved data to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred saving to {file_path}: {e}\")\n",
    "\n",
    "def load_content_data(file_path):\n",
    "    \"\"\"Load content data from a JSON file and create a FAN to Content mapping.\"\"\"\n",
    "    data = load_json_file(file_path)\n",
    "    if data is None:\n",
    "        return {}\n",
    "\n",
    "    content_dict = {}\n",
    "    key_options = ['FAN', 'Application_Number'] # Handle potential key variations\n",
    "\n",
    "    for item in data:\n",
    "        fan_key = None\n",
    "        for key in key_options:\n",
    "            if key in item:\n",
    "                # Sometimes Application_Number needs Application_Category appended\n",
    "                if key == 'Application_Number' and 'Application_Category' in item:\n",
    "                   fan_key = item[key] + item.get('Application_Category', '') # Safely get category\n",
    "                else:\n",
    "                   fan_key = item[key]\n",
    "                break # Found a key, stop looking\n",
    "\n",
    "        if fan_key and 'Content' in item:\n",
    "             content_dict[fan_key] = item['Content']\n",
    "        # else:\n",
    "        #     print(f\"Warning: Could not find FAN key or Content in item: {item.keys()}\")\n",
    "\n",
    "    print(f\"Created content dictionary with {len(content_dict)} entries.\")\n",
    "    return content_dict\n",
    "\n",
    "\n",
    "def extract_text(content_dict, text_type=\"TA\"):\n",
    "    \"\"\"Extract text from patent content based on text_type\"\"\"\n",
    "    if not isinstance(content_dict, dict):\n",
    "        # print(f\"Warning: Invalid content_dict provided (type: {type(content_dict)}), expected dict.\")\n",
    "        return \"\"\n",
    "\n",
    "    text_parts = []\n",
    "\n",
    "    # Note: The original argparse choices included 'TAC', but the function uses 'tac1'.\n",
    "    # Adjust config['text_type'] if 'tac1' was intended instead of 'TAC'.\n",
    "    if text_type in [\"TA\", \"tac1\", \"full\", \"title_abstract\"]:\n",
    "        text_parts.append(content_dict.get(\"title\", \"\"))\n",
    "        text_parts.append(content_dict.get(\"pa01\", \"\")) # Abstract\n",
    "\n",
    "    if text_type in [\"claims\", \"tac1\", \"full\"]:\n",
    "        claims = []\n",
    "        first_claim = None\n",
    "        # Sort keys to approximate claim order, although keys aren't guaranteed sequential\n",
    "        sorted_keys = sorted([key for key in content_dict if key.startswith('c-')])\n",
    "        for key in sorted_keys:\n",
    "            claim_text = content_dict.get(key, \"\")\n",
    "            if claim_text:\n",
    "                claims.append(claim_text)\n",
    "                if first_claim is None and text_type == \"tac1\":\n",
    "                    first_claim = claim_text\n",
    "\n",
    "        if text_type == \"claims\" or text_type == \"full\":\n",
    "            text_parts.extend(claims)\n",
    "        elif text_type == \"tac1\" and first_claim:\n",
    "            text_parts.append(first_claim)\n",
    "\n",
    "    if text_type in [\"description\", \"full\"]:\n",
    "        # Add description paragraphs (keys starting with 'p')\n",
    "        desc_parts = []\n",
    "        # Sort keys to approximate paragraph order\n",
    "        sorted_keys = sorted([key for key in content_dict if key.startswith('p')])\n",
    "        for key in sorted_keys:\n",
    "             desc_parts.append(content_dict.get(key,\"\"))\n",
    "        text_parts.extend(desc_parts)\n",
    "\n",
    "    if text_type == \"features\":\n",
    "        # Extract LLM features if present\n",
    "        text_parts.append(content_dict.get(\"features\", \"\"))\n",
    "\n",
    "    # Join non-empty parts with a space\n",
    "    return \" \".join(filter(None, text_parts)).strip()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main Re-ranking Logic\n",
    "# ----------------------------\n",
    "\n",
    "# Changed function signature to accept config dictionary\n",
    "def main(cfg):\n",
    "    # --- Device Setup ---\n",
    "    # Use device from the config dictionary\n",
    "    device = torch.device(cfg['device'])\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # --- Construct Full Paths ---\n",
    "    # Use base_dir from the config dictionary\n",
    "    def get_full_path(path):\n",
    "        if os.path.isabs(path):\n",
    "            return path\n",
    "        # Use cfg['base_dir'] instead of args.base_dir\n",
    "        return os.path.join(cfg['base_dir'], path)\n",
    "\n",
    "    # Use paths from the config dictionary\n",
    "    query_list_file = get_full_path(cfg['query_list_file'])\n",
    "    pre_ranking_file = get_full_path(cfg['pre_ranking_file'])\n",
    "    queries_content_file = get_full_path(cfg['queries_content_file'])\n",
    "    documents_content_file = get_full_path(cfg['documents_content_file'])\n",
    "    output_file = get_full_path(cfg['output_file'])\n",
    "\n",
    "    # --- Load Data ---\n",
    "    query_ids = load_json_file(query_list_file)\n",
    "    pre_ranking_data = load_json_file(pre_ranking_file)\n",
    "    queries_content = load_content_data(queries_content_file)\n",
    "    documents_content = load_content_data(documents_content_file)\n",
    "\n",
    "    if not query_ids or not pre_ranking_data or not queries_content or not documents_content:\n",
    "        print(\"Error: Failed to load one or more essential data files. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- Load Model ---\n",
    "    # Use model_name from the config dictionary\n",
    "    print(f\"Loading SentenceTransformer model: {cfg['model_name']}\")\n",
    "    try:\n",
    "        # Define model architecture if needed (e.g., for specific pooling)\n",
    "        # word_embedding_model = models.Transformer(cfg['model_name'], max_seq_length=cfg['max_length'])\n",
    "        # pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode=cfg['pooling']) # Use cfg['pooling']\n",
    "        # model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device=device)\n",
    "\n",
    "        # Simpler loading if default pooling (mean) is okay or model config handles it\n",
    "        model = SentenceTransformer(cfg['model_name'], device=device)\n",
    "        model.max_seq_length = cfg['max_length'] # Set max length using cfg['max_length']\n",
    "        print(\"Model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {cfg['model_name']}: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Re-ranking Process ---\n",
    "    # Use text_type from the config dictionary\n",
    "    print(f\"Starting re-ranking for {len(query_ids)} queries using '{cfg['text_type']}' content...\")\n",
    "    results = {}\n",
    "    missing_query_content = 0\n",
    "    missing_pre_ranking = 0\n",
    "    queries_with_no_valid_docs = 0\n",
    "\n",
    "    for query_id in tqdm(query_ids, desc=\"Processing queries\"):\n",
    "        # 1. Get Query Content\n",
    "        query_content_dict = queries_content.get(query_id)\n",
    "        if not query_content_dict:\n",
    "            # print(f\"Warning: Content not found for query {query_id}\")\n",
    "            missing_query_content += 1\n",
    "            results[query_id] = [] # Assign empty list if query content missing\n",
    "            continue\n",
    "\n",
    "        # Use text_type from config\n",
    "        query_text = extract_text(query_content_dict, cfg['text_type'])\n",
    "        if not query_text:\n",
    "            # print(f\"Warning: Extracted text is empty for query {query_id} with type '{cfg['text_type']}'\")\n",
    "            missing_query_content += 1\n",
    "            results[query_id] = []\n",
    "            continue\n",
    "\n",
    "        # 2. Get Candidate Documents\n",
    "        candidate_doc_ids = pre_ranking_data.get(query_id)\n",
    "        if not candidate_doc_ids:\n",
    "            # print(f\"Warning: Pre-ranking not found for query {query_id}\")\n",
    "            missing_pre_ranking += 1\n",
    "            results[query_id] = []\n",
    "            continue\n",
    "\n",
    "        # 3. Get Candidate Document Content\n",
    "        doc_texts = []\n",
    "        valid_doc_ids_for_query = []\n",
    "        missing_docs_count = 0\n",
    "        for doc_id in candidate_doc_ids:\n",
    "            doc_content_dict = documents_content.get(doc_id)\n",
    "            if not doc_content_dict:\n",
    "                # print(f\"Warning: Content not found for document {doc_id} (query {query_id})\")\n",
    "                missing_docs_count += 1\n",
    "                continue\n",
    "\n",
    "            # Use text_type from config\n",
    "            doc_text = extract_text(doc_content_dict, cfg['text_type'])\n",
    "            if doc_text:\n",
    "                doc_texts.append(doc_text)\n",
    "                valid_doc_ids_for_query.append(doc_id)\n",
    "            else:\n",
    "                 # print(f\"Warning: Extracted text is empty for document {doc_id} with type '{cfg['text_type']}' (query {query_id})\")\n",
    "                 missing_docs_count += 1\n",
    "\n",
    "\n",
    "        if not valid_doc_ids_for_query:\n",
    "            # print(f\"Warning: No valid document texts found for query {query_id} after checking {len(candidate_doc_ids)} candidates.\")\n",
    "            queries_with_no_valid_docs += 1\n",
    "            results[query_id] = [] # Assign empty list if no valid docs\n",
    "            continue\n",
    "\n",
    "        # 4. Generate Embeddings (On-the-fly)\n",
    "        try:\n",
    "            # Use batch_size from config\n",
    "            query_embedding = model.encode(\n",
    "                query_text,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=1 # Batch size for query is usually 1\n",
    "            )\n",
    "            doc_embeddings = model.encode(\n",
    "                doc_texts,\n",
    "                convert_to_tensor=True,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=cfg['batch_size'] # Use cfg['batch_size']\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during encoding for query {query_id}: {e}\")\n",
    "            results[query_id] = candidate_doc_ids # Fallback to original order on error\n",
    "            continue\n",
    "\n",
    "\n",
    "        # 5. Calculate Similarities\n",
    "        # Ensure embeddings are on the same device for cosine similarity\n",
    "        query_embedding = query_embedding.to(device)\n",
    "        doc_embeddings = doc_embeddings.to(device)\n",
    "\n",
    "        cosine_scores = util.cos_sim(query_embedding, doc_embeddings)[0] # Get the first row of scores\n",
    "        cosine_scores = cosine_scores.cpu().numpy() # Move scores to CPU for sorting\n",
    "\n",
    "        # 6. Rank Documents\n",
    "        # Combine scores with their original valid doc_ids\n",
    "        doc_scores = list(zip(valid_doc_ids_for_query, cosine_scores))\n",
    "\n",
    "        # Sort by score in descending order\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get the sorted list of document IDs\n",
    "        re_ranked_doc_ids = [doc_id for doc_id, score in doc_scores]\n",
    "\n",
    "        # If some original docs were missing content, append their IDs at the end\n",
    "        # (or handle differently if needed - e.g., exclude them)\n",
    "        original_candidate_set = set(candidate_doc_ids)\n",
    "        reranked_set = set(re_ranked_doc_ids)\n",
    "        missing_from_reranked = list(original_candidate_set - reranked_set)\n",
    "        final_ranked_list = re_ranked_doc_ids + missing_from_reranked\n",
    "\n",
    "        results[query_id] = final_ranked_list[:len(candidate_doc_ids)] # Ensure max length is original candidate count\n",
    "\n",
    "\n",
    "    # --- Report Missing Data ---\n",
    "    print(\"\\n--- Re-ranking Summary ---\")\n",
    "    print(f\"Total queries processed: {len(query_ids)}\")\n",
    "    if missing_query_content > 0:\n",
    "        print(f\"Warning: Content missing or empty for {missing_query_content} queries.\")\n",
    "    if missing_pre_ranking > 0:\n",
    "        print(f\"Warning: Pre-ranking data missing for {missing_pre_ranking} queries.\")\n",
    "    if queries_with_no_valid_docs > 0:\n",
    "        print(f\"Warning: {queries_with_no_valid_docs} queries had no valid documents with content.\")\n",
    "    print(f\"Number of queries in results: {len(results)}\")\n",
    "\n",
    "\n",
    "    # --- Save Results ---\n",
    "    # Use output_file from config\n",
    "    save_json_file(results, output_file)\n",
    "\n",
    "    print(\"\\nRe-ranking complete.\")\n",
    "\n",
    "main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
