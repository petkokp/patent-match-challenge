{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, models, util\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time # To measure experiment time\n",
    "import types # Used for SimpleNamespace if preferred\n",
    "import re\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "# Base config - specific parameters will be overridden by experiments\n",
    "config = {\n",
    "    # --- Data Files ---\n",
    "    'base_dir': '.',\n",
    "    'query_list_file': 'test_queries.json',\n",
    "    'pre_ranking_file': 'shuffled_pre_ranking.json',\n",
    "    'queries_content_file': 'queries_content_with_features.json',\n",
    "    'documents_content_file': 'documents_content_with_features.json',\n",
    "    # 'qrels_file': 'train_gold_mapping.json', # REMOVED - Not needed for prediction generation\n",
    "\n",
    "    # --- Default Model/Text Settings (can be overridden in experiments) ---\n",
    "    'reranker_type': 'bi-encoder',\n",
    "    'bi_encoder_model': 'AI-Growth-Lab/PatentSBERTa',\n",
    "    'cross_encoder_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "    'text_type': 'tac1',\n",
    "    'max_length': 512,\n",
    "\n",
    "    # --- Execution Settings ---\n",
    "    'batch_size': 32,\n",
    "    'device': None,\n",
    "\n",
    "    # --- Output Settings ---\n",
    "    'save_individual_predictions': True, # Keep True to save output for each experiment\n",
    "    'output_file_prefix': 'prediction_exp', # Prefix for individual prediction files\n",
    "}\n",
    "\n",
    "# --- Experiments to Run ---\n",
    "# Define different configurations to generate predictions for\n",
    "experiments = [\n",
    "    # --- Baseline Bi-Encoders ---\n",
    "    {\n",
    "        'exp_id': 'BiEnc_PatentSBERTa_tac1',\n",
    "        'reranker_type': 'bi-encoder',\n",
    "        'bi_encoder_model': 'AI-Growth-Lab/PatentSBERTa',\n",
    "        'text_type': 'tac1',\n",
    "    },\n",
    "    {\n",
    "        'exp_id': 'BiEnc_MPNet_tac1',\n",
    "        'reranker_type': 'bi-encoder',\n",
    "        'bi_encoder_model': 'all-mpnet-base-v2',\n",
    "        'text_type': 'tac1',\n",
    "    },\n",
    "    # --- Test Claims with Bi-Encoders ---\n",
    "    {\n",
    "        'exp_id': 'BiEnc_PatentSBERTa_claims',\n",
    "        'reranker_type': 'bi-encoder',\n",
    "        'bi_encoder_model': 'AI-Growth-Lab/PatentSBERTa',\n",
    "        'text_type': 'claims',\n",
    "    },\n",
    "    {\n",
    "        'exp_id': 'BiEnc_MPNet_claims',\n",
    "        'reranker_type': 'bi-encoder',\n",
    "        'bi_encoder_model': 'all-mpnet-base-v2',\n",
    "        'text_type': 'claims',\n",
    "    },\n",
    "    # --- Test QA Bi-Encoder ---\n",
    "    {\n",
    "        'exp_id': 'BiEnc_MultiQA_tac1',\n",
    "        'reranker_type': 'bi-encoder',\n",
    "        'bi_encoder_model': 'multi-qa-mpnet-base-dot-v1',\n",
    "        'text_type': 'tac1',\n",
    "    },\n",
    "    # --- Baseline Cross-Encoder ---\n",
    "    {\n",
    "        'exp_id': 'CrossEnc_L6_tac1',\n",
    "        'reranker_type': 'cross-encoder',\n",
    "        'cross_encoder_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "        'text_type': 'tac1',\n",
    "    },\n",
    "    # --- Test Claims/TA with Cross-Encoder ---\n",
    "    {\n",
    "        'exp_id': 'CrossEnc_L6_claims',\n",
    "        'reranker_type': 'cross-encoder',\n",
    "        'cross_encoder_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "        'text_type': 'claims',\n",
    "     },\n",
    "     {\n",
    "        'exp_id': 'CrossEnc_L6_TA',\n",
    "        'reranker_type': 'cross-encoder',\n",
    "        'cross_encoder_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "        'text_type': 'TA',\n",
    "     },\n",
    "]\n",
    "\n",
    "\n",
    "# --- Auto-detect device ---\n",
    "if config['device'] is None:\n",
    "    config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "elif config['device'] == 'cuda' and not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA requested but not available. Using CPU.\")\n",
    "    config['device'] = 'cpu'\n",
    "\n",
    "# ----------------------------\n",
    "# Utility Functions\n",
    "# ----------------------------\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load JSON data from a file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f: data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError: print(f\"Error: File not found at {file_path}\"); return None\n",
    "    except json.JSONDecodeError: print(f\"Error: Could not decode JSON from {file_path}\"); return None\n",
    "    except Exception as e: print(f\"An unexpected error occurred loading {file_path}: {e}\"); return None\n",
    "\n",
    "def save_json_file(data, file_path):\n",
    "    \"\"\"Save data to a JSON file\"\"\"\n",
    "    print(f\"Saving predictions to: {file_path}\")\n",
    "    try:\n",
    "        output_dir = os.path.dirname(file_path)\n",
    "        if output_dir: os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(file_path, 'w', encoding='utf-8') as f: json.dump(data, f, indent=2)\n",
    "    except Exception as e: print(f\"An error occurred saving to {file_path}: {e}\")\n",
    "\n",
    "def load_content_data(file_path):\n",
    "    \"\"\"Load content data from a JSON file and create a FAN/AppNum to Content mapping.\"\"\"\n",
    "    data = load_json_file(file_path)\n",
    "    if data is None: return {}\n",
    "    print(f\"Processing content file: {os.path.basename(file_path)}\")\n",
    "    content_dict = {}\n",
    "    key_options = ['FAN', 'Application_Number']\n",
    "    # Ensure keys are strings during loading/processing for consistency\n",
    "    for item in tqdm(data, desc=\"Loading content\", leave=False):\n",
    "        fan_key = None\n",
    "        temp_key_val = None # Store the original key value before potential modification\n",
    "        for key_name in key_options:\n",
    "            if key_name in item:\n",
    "                temp_key_val = item[key_name]\n",
    "                if key_name == 'Application_Number' and 'Application_Category' in item:\n",
    "                   fan_key = str(temp_key_val) + str(item.get('Application_Category', '')) # Ensure string concat\n",
    "                else:\n",
    "                   fan_key = str(temp_key_val) # Ensure key is string\n",
    "                break\n",
    "        if fan_key and 'Content' in item:\n",
    "             content_dict[fan_key] = item['Content'] # Key is now guaranteed string\n",
    "    return content_dict\n",
    "\n",
    "def extract_text(content_dict, text_type=\"TA\"):\n",
    "    \"\"\"Extract text from patent content based on text_type\"\"\"\n",
    "    if not isinstance(content_dict, dict): return \"\"\n",
    "    text_parts = []\n",
    "    # Standard types\n",
    "    if text_type in [\"TA\", \"tac1\", \"full\", \"title_abstract\"]:\n",
    "        text_parts.append(content_dict.get(\"title\", \"\"))\n",
    "        text_parts.append(content_dict.get(\"pa01\", \"\")) # Abstract\n",
    "    if text_type in [\"claims\", \"tac1\", \"full\"]:\n",
    "        claims, first_claim = [], None\n",
    "        claim_keys = [key for key in content_dict if key.startswith('c-')]\n",
    "        def get_sort_key(key_string):\n",
    "            parts = key_string.split('-', 1); return int(parts[1]) if len(parts) == 2 and parts[1].isdigit() else float('inf')\n",
    "        sorted_keys = sorted(claim_keys, key=get_sort_key)\n",
    "        for key in sorted_keys:\n",
    "            claim_text = content_dict.get(key, \"\")\n",
    "            if claim_text:\n",
    "                claims.append(claim_text)\n",
    "                if first_claim is None and text_type == \"tac1\": first_claim = claim_text\n",
    "        if text_type == \"claims\" or text_type == \"full\": text_parts.extend(claims)\n",
    "        elif text_type == \"tac1\" and first_claim: text_parts.append(first_claim)\n",
    "    if text_type in [\"description\", \"full\"]:\n",
    "        desc_parts = []\n",
    "        desc_keys = [key for key in content_dict if key.startswith('p')]\n",
    "        def get_p_sort_key(key_string):\n",
    "             parts = key_string.split('-', 1); return int(parts[1]) if len(parts) == 2 and parts[1].isdigit() else float('inf')\n",
    "        sorted_keys = sorted(desc_keys, key=get_p_sort_key)\n",
    "        for key in sorted_keys: desc_parts.append(content_dict.get(key,\"\"))\n",
    "        text_parts.extend(desc_parts)\n",
    "    if text_type == \"features\": text_parts.append(content_dict.get(\"features\", \"\"))\n",
    "    result = \" \".join(filter(None, text_parts)).strip()\n",
    "    return result\n",
    "\n",
    "# --- REMOVED Evaluation Functions ---\n",
    "# load_qrels, calculate_average_precision, calculate_recall_at_k, evaluate_ranking deleted\n",
    "\n",
    "# --- REMOVED Print Results Table function ---\n",
    "# print_results_table deleted\n",
    "\n",
    "# ----------------------------\n",
    "# Core Re-ranking Function\n",
    "# ----------------------------\n",
    "def perform_reranking(exp_config, query_ids, pre_ranking_data, queries_content, documents_content):\n",
    "    \"\"\"Performs the re-ranking for a specific experiment configuration.\"\"\"\n",
    "    # This function remains largely the same, but no longer needs to worry about qrels/metrics\n",
    "\n",
    "    device = torch.device(exp_config['device'])\n",
    "    reranker_type = exp_config['reranker_type']\n",
    "    text_type = exp_config['text_type']\n",
    "    model = None\n",
    "    model_name = \"\"\n",
    "\n",
    "    # --- Load Model ---\n",
    "    try:\n",
    "        if reranker_type == 'bi-encoder':\n",
    "            model_name = exp_config.get('bi_encoder_model')\n",
    "            if not model_name: raise ValueError(\"bi_encoder_model must be specified\")\n",
    "            print(f\"Loading Bi-Encoder model: {model_name}...\")\n",
    "            model = SentenceTransformer(model_name, device=device)\n",
    "            model.max_seq_length = exp_config['max_length']\n",
    "\n",
    "        elif reranker_type == 'cross-encoder':\n",
    "            model_name = exp_config.get('cross_encoder_model')\n",
    "            if not model_name: raise ValueError(\"cross_encoder_model must be specified\")\n",
    "            print(f\"Loading Cross-Encoder model: {model_name}...\")\n",
    "            model = CrossEncoder(model_name, device=device, max_length=exp_config['max_length'])\n",
    "        else:\n",
    "             raise ValueError(f\"Invalid reranker_type: {reranker_type}\")\n",
    "        print(f\"Model '{model_name}' loaded.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading model '{model_name}' for experiment '{exp_config.get('exp_id', 'N/A')}': {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Re-ranking Process ---\n",
    "    results = {}\n",
    "    pbar = tqdm(query_ids, desc=f\"Re-ranking ({exp_config.get('exp_id', 'N/A')})\", leave=False)\n",
    "    # query_ids are strings here\n",
    "    for query_id in pbar:\n",
    "        candidate_doc_ids = pre_ranking_data.get(query_id, []) # query_id is string, doc_ids are strings\n",
    "        query_content_dict = queries_content.get(query_id) # query_id is string\n",
    "\n",
    "        if not candidate_doc_ids: results[query_id] = []; continue\n",
    "        if not query_content_dict: results[query_id] = candidate_doc_ids; continue\n",
    "\n",
    "        query_text = extract_text(query_content_dict, text_type)\n",
    "        if not query_text: results[query_id] = candidate_doc_ids; continue\n",
    "\n",
    "        valid_docs_texts = {}\n",
    "        for doc_id in candidate_doc_ids: # doc_id is string\n",
    "            doc_content_dict = documents_content.get(doc_id) # doc_id is string\n",
    "            if doc_content_dict:\n",
    "                doc_text = extract_text(doc_content_dict, text_type)\n",
    "                if doc_text: valid_docs_texts[doc_id] = doc_text # Key doc_id is string\n",
    "\n",
    "        valid_doc_ids = list(valid_docs_texts.keys()) # These are strings\n",
    "        if not valid_doc_ids: results[query_id] = candidate_doc_ids; continue\n",
    "\n",
    "        # --- Calculate Scores ---\n",
    "        doc_scores_calculated = {}\n",
    "        try:\n",
    "            doc_texts_for_scoring = [valid_docs_texts[doc_id] for doc_id in valid_doc_ids]\n",
    "\n",
    "            if reranker_type == 'bi-encoder':\n",
    "                query_embedding = model.encode(query_text, convert_to_tensor=True, show_progress_bar=False).to(device)\n",
    "                doc_embeddings = model.encode(doc_texts_for_scoring, convert_to_tensor=True, show_progress_bar=False, batch_size=exp_config['batch_size']).to(device)\n",
    "                if query_embedding is None or doc_embeddings is None or len(doc_embeddings) == 0: raise RuntimeError(\"Embedding generation failed\")\n",
    "                if query_embedding.shape[0] == 0 or doc_embeddings.shape[0] == 0: raise RuntimeError(\"Embedding tensor is empty\")\n",
    "                cosine_scores = util.cos_sim(query_embedding, doc_embeddings)[0].cpu().numpy()\n",
    "                doc_scores_calculated = dict(zip(valid_doc_ids, cosine_scores))\n",
    "\n",
    "            elif reranker_type == 'cross-encoder':\n",
    "                sentence_pairs = [[query_text, doc_text] for doc_text in doc_texts_for_scoring]\n",
    "                cross_scores = model.predict(sentence_pairs, show_progress_bar=False, batch_size=exp_config['batch_size'], convert_to_numpy=True)\n",
    "                doc_scores_calculated = dict(zip(valid_doc_ids, cross_scores))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during scoring query {query_id} in exp {exp_config.get('exp_id', 'N/A')}: {type(e).__name__} - {e}\")\n",
    "            results[query_id] = candidate_doc_ids; continue\n",
    "\n",
    "        # --- Rank Documents ---\n",
    "        min_score = min(doc_scores_calculated.values()) if doc_scores_calculated else 0\n",
    "        fallback_score = min_score - 1 if min_score > -float('inf') else -float('inf')\n",
    "        scored_doc_list = []\n",
    "        processed_docs = set()\n",
    "        for doc_id, score in doc_scores_calculated.items(): # doc_id is string\n",
    "            scored_doc_list.append((doc_id, float(score)))\n",
    "            processed_docs.add(doc_id)\n",
    "        for doc_id in candidate_doc_ids: # doc_id is string\n",
    "            if doc_id not in processed_docs: scored_doc_list.append((doc_id, fallback_score))\n",
    "        scored_doc_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        final_ranked_list = [doc_id for doc_id, score in scored_doc_list] # List of strings\n",
    "        results[query_id] = final_ranked_list[:len(candidate_doc_ids)] # Keys (query_id) are strings\n",
    "\n",
    "    del model\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    return results # Keys are strings, values are lists of strings\n",
    "\n",
    "# ----------------------------\n",
    "# Main Execution Logic\n",
    "# ----------------------------\n",
    "def main(base_cfg, experiments_to_run):\n",
    "    print(f\"Base device requested: {base_cfg.get('device', 'None specified')}\")\n",
    "    print(f\"Using effective device: {config['device']}\") # Show auto-detected device\n",
    "\n",
    "    # --- Construct Full Paths ---\n",
    "    def get_full_path(path): return path if os.path.isabs(path) else os.path.join(base_cfg['base_dir'], path)\n",
    "    query_list_file = get_full_path(base_cfg['query_list_file'])\n",
    "    pre_ranking_file = get_full_path(base_cfg['pre_ranking_file'])\n",
    "    queries_content_file = get_full_path(base_cfg['queries_content_file'])\n",
    "    documents_content_file = get_full_path(base_cfg['documents_content_file'])\n",
    "    # qrels_file_path REMOVED\n",
    "\n",
    "    # --- Load Shared Data ---\n",
    "    print(\"\\nLoading shared data...\")\n",
    "    query_ids_raw = load_json_file(query_list_file)\n",
    "    pre_ranking_data_raw = load_json_file(pre_ranking_file)\n",
    "    queries_content_raw = load_content_data(queries_content_file)\n",
    "    documents_content_raw = load_content_data(documents_content_file)\n",
    "    # qrels REMOVED\n",
    "\n",
    "    # --- Ensure IDs/Keys are Strings ---\n",
    "    if query_ids_raw is None: print(\"\\nError: Failed to load query_list_file. Exiting.\"); return\n",
    "    query_ids = [str(qid) for qid in query_ids_raw]\n",
    "    print(f\"Loaded and processed {len(query_ids)} query IDs (as strings).\")\n",
    "\n",
    "    if pre_ranking_data_raw is None: print(\"\\nError: Failed to load pre_ranking_file. Exiting.\"); return\n",
    "    pre_ranking_data = {str(k): list(map(str, v)) for k, v in pre_ranking_data_raw.items()}\n",
    "    print(f\"Processed {len(pre_ranking_data)} pre-ranking entries (keys/docs as strings).\")\n",
    "\n",
    "    if queries_content_raw is None or documents_content_raw is None:\n",
    "         print(\"\\nError: Failed to load content files. Exiting.\"); return\n",
    "    queries_content = {str(k): v for k, v in queries_content_raw.items()}\n",
    "    documents_content = {str(k): v for k, v in documents_content_raw.items()}\n",
    "    print(\"Ensured content dictionary keys are strings.\")\n",
    "    # ------------------------------------\n",
    "\n",
    "    if not all([pre_ranking_data, queries_content, documents_content]):\n",
    "        print(\"\\nError: Failed to load one or more data files (pre-ranking, content). Exiting.\")\n",
    "        return\n",
    "\n",
    "    # --- Run Experiments ---\n",
    "    print(f\"\\nStarting {len(experiments_to_run)} experiments to generate prediction files...\")\n",
    "\n",
    "    for i, exp_params in enumerate(experiments_to_run):\n",
    "        exp_id = exp_params.get('exp_id', f'exp_{i+1}')\n",
    "        print(f\"\\n--- Running Experiment: {exp_id} ---\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        run_config = base_cfg.copy()\n",
    "        run_config.update(exp_params)\n",
    "\n",
    "        # Perform re-ranking\n",
    "        predictions = perform_reranking(\n",
    "            run_config,\n",
    "            query_ids,\n",
    "            pre_ranking_data,\n",
    "            queries_content,\n",
    "            documents_content\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Experiment {exp_id} finished in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "        if predictions is None:\n",
    "            print(f\"Experiment {exp_id} failed during re-ranking. No prediction file generated.\")\n",
    "        else:\n",
    "            # Save predictions if enabled\n",
    "            if run_config.get('save_individual_predictions', False):\n",
    "                 pred_filename = f\"{run_config.get('output_file_prefix', 'pred')}_{exp_id}.json\"\n",
    "                 pred_filepath = get_full_path(pred_filename)\n",
    "                 save_json_file(predictions, pred_filepath) # Save the generated predictions\n",
    "            else:\n",
    "                 print(f\"Skipping saving prediction file for {exp_id} as 'save_individual_predictions' is False.\")\n",
    "\n",
    "        # Metrics calculation and storage REMOVED\n",
    "        # Results table printing REMOVED\n",
    "\n",
    "    print(\"\\nAll experiments complete.\")\n",
    "\n",
    "# --- Run the main function ---\n",
    "if __name__ == \"__main__\":\n",
    "    main(config, experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
