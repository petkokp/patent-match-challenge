{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patent Match Challenge\n",
    "\n",
    "## Dimitrije Zdrale, Clement Marie, Petko Petkov\n",
    "\n",
    "This notebook is structured to handle patent similarity and re-ranking experiments using various transformer-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - pre-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "import scipy.sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    print(\"Sentence Transformers and Torch loaded.\")\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Sentence Transformers or Torch not found. Dense embedding methods will be skipped.\")\n",
    "    print(\"Install them (`pip install sentence-transformers torch`) to enable.\")\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "print(\"Downloading NLTK data (if necessary)...\")\n",
    "nltk_packages = ['wordnet', 'stopwords', 'punkt']\n",
    "for package in nltk_packages:\n",
    "    try:\n",
    "        if package == 'punkt':\n",
    "            nltk.data.find(f'tokenizers/{package}')\n",
    "        else:\n",
    "             nltk.data.find(f'corpora/{package}')\n",
    "    except:\n",
    "        try:\n",
    "           print(f\"Downloading NLTK package '{package}'...\")\n",
    "           nltk.download(package, quiet=True)\n",
    "           print(f\"NLTK package '{package}' downloaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading NLTK package '{package}': {e}\")\n",
    "print(\"NLTK check complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing function performs several steps to clean and normalize text data:\n",
    "\n",
    "- Converts text to lowercase.\n",
    "- Removes punctuation, digits, and isolated single letters.\n",
    "- Normalizes whitespace.\n",
    "- Tokenizes text using NLTK.\n",
    "- Removes stopwords, using either standard NLTK stopwords or an extended set including domain-specific terms.\n",
    "- Applies lemmatization by default, with optional stemming if specified (`use_stemming=True`).\n",
    "- Filters out short tokens (less than 3 characters).\n",
    "\n",
    "This setup is designed to prepare technical or patent-like text for downstream NLP tasks by reducing noise and focusing on meaningful terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = set([\n",
    "    'claim', 'claims', 'claimed', 'method', 'system', 'device', 'apparatus', 'assembly', 'unit',\n",
    "    'comprising', 'comprises', 'thereof', 'wherein', 'said', 'thereby', 'herein', 'accordance',\n",
    "    'invention', 'present', 'related', 'relates', 'figure', 'fig', 'example', 'examples',\n",
    "    'embodiment', 'embodiments', 'accordance', 'therein', 'associated', 'provided', 'configured',\n",
    "    'includes', 'including', 'based', 'least', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten',\n",
    "    'first', 'second', 'third', 'fourth', 'fifth', 'etc', 'eg', 'ie',\n",
    "    'may', 'further', 'also', 'within', 'upon', 'used', 'using', 'use', 'capable', 'adapted',\n",
    "    'generally', 'typically', 'respectively', 'particularly', 'preferably', 'various', 'such',\n",
    "    'described', 'disclosed', 'illustrated', 'shown',\n",
    "    'portion', 'member', 'element', 'surface', 'axis', 'position', 'direction', 'side', 'end', 'top', 'bottom',\n",
    "    'lower', 'upper', 'inner', 'outer', 'rear', 'front', 'lateral',\n",
    "    'set', 'provide', 'generate', 'control', 'controlling', 'operation', 'value', 'signal', 'process', 'data',\n",
    "    'group', 'range', 'level', 'time', 'number', 'result', 'type', 'form', 'part', 'manner', 'step'\n",
    "])\n",
    "all_stopwords = stop_words.union(custom_stopwords)\n",
    "\n",
    "\n",
    "def preprocess_text(text, use_stemming=False, use_custom_stopwords=True):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # Remove single letters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Normalize whitespace\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    current_stopwords = all_stopwords if use_custom_stopwords else stop_words\n",
    "\n",
    "    if use_stemming:\n",
    "        processed_tokens = [stemmer.stem(word) for word in tokens if word not in current_stopwords and len(word) > 2]\n",
    "    else:\n",
    "        processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in current_stopwords and len(word) > 2]\n",
    "\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a `create_corpus` helper function so we can extract and preprocess the data. It extracts and optionally preprocesses textual content from a structured corpus:\n",
    "- Supports multiple `text_type` options: `'title'`, `'abstract'`, `'claim1'`, `'claims'`, `'description'`, `'fulltext'`, or combinations like `'title_abstract'`.\n",
    "- Skips documents with missing identifiers or required text sections.\n",
    "- Optionally applies text preprocessing (e.g., lemmatization/stemming, stopword removal) unless `config['method'] == 'dense'`.\n",
    "- Tracks and reports the number of documents skipped.\n",
    "- Returns a list of dictionaries with `'id'` and `'text'` keys for each valid document.\n",
    "\n",
    "This function is designed to flexibly extract and clean specific sections of patent documents for downstream analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "            contents = json.load(file)\n",
    "        return contents\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_corpus(corpus, text_type, preprocess=False, config={}):\n",
    "    if not corpus:\n",
    "        print(f\"Warning: Attempting to create corpus from empty or None input for '{text_type}'.\")\n",
    "        return []\n",
    "\n",
    "    app_ids = []\n",
    "    texts = []\n",
    "    cnt = 0\n",
    "\n",
    "    print(f\"Creating corpus for text_type: '{text_type}'...\")\n",
    "\n",
    "    required_parts = []\n",
    "    if 'title' in text_type: required_parts.append('title')\n",
    "    if 'abstract' in text_type: required_parts.append('pa01')\n",
    "    if 'claim1' in text_type: required_parts.append('c-en-0001')\n",
    "\n",
    "    for doc in tqdm(corpus, desc=f\"Processing {text_type}\", leave=False):\n",
    "        doc_id = doc.get('Application_Number', '') + doc.get('Application_Category', '')\n",
    "        if not doc_id: # Skip if ID is missing\n",
    "            cnt+=1\n",
    "            continue\n",
    "        content = doc.get('Content', {})\n",
    "        if not content: # Skip if content is missing\n",
    "             cnt += 1\n",
    "             continue\n",
    "\n",
    "        doc_text_parts = []\n",
    "        missing_part = False\n",
    "\n",
    "        part_map = {\n",
    "            'title': ['title'],\n",
    "            'abstract': ['pa01'],\n",
    "            'claim1': ['c-en-0001'],\n",
    "            'claims': [k for k in content if k.startswith('c-en-')],\n",
    "            'description': [k for k in content if k.startswith('p')],\n",
    "            'fulltext': list(content.keys())\n",
    "        }\n",
    "\n",
    "        keys_to_extract = set()\n",
    "        if text_type == 'title_abstract': keys_to_extract.update(part_map['title'] + part_map['abstract'])\n",
    "        elif text_type == 'title_abstract_claim1': keys_to_extract.update(part_map['title'] + part_map['abstract'] + part_map['claim1'])\n",
    "        elif text_type == 'title_abstract_claims': keys_to_extract.update(part_map['title'] + part_map['abstract'] + part_map['claims'])\n",
    "        elif text_type in part_map: keys_to_extract.update(part_map[text_type])\n",
    "        else: print(f\"Warning: Unknown text_type '{text_type}' in create_corpus.\")\n",
    "\n",
    "        extracted_texts = [content.get(key) for key in keys_to_extract if content.get(key)]\n",
    "        doc_text_parts = list(dict.fromkeys(filter(None, extracted_texts)))\n",
    "\n",
    "        if text_type in ['title', 'abstract', 'claim1', 'claims', 'description']:\n",
    "             if not doc_text_parts:\n",
    "                 missing_part = True\n",
    "\n",
    "        # Final check and processing\n",
    "        if not doc_text_parts or missing_part:\n",
    "            cnt += 1\n",
    "        else:\n",
    "            final_text = ' '.join(doc_text_parts)\n",
    "\n",
    "            # Apply preprocessing based on config and method type\n",
    "            if preprocess and config.get('method') != 'dense': # Only preprocess if requested AND method is not 'dense'\n",
    "                use_stemming_flag = config.get('use_stemming', False)\n",
    "                use_custom_stopwords_flag = config.get('use_custom_stopwords', True)\n",
    "                final_text = preprocess_text(final_text, use_stemming=use_stemming_flag, use_custom_stopwords=use_custom_stopwords_flag)\n",
    "\n",
    "            if not final_text or not final_text.strip():\n",
    "                 cnt += 1\n",
    "            else:\n",
    "                texts.append(final_text)\n",
    "                app_ids.append(doc_id)\n",
    "\n",
    "    if cnt > 0:\n",
    "         print(f\"Number of documents skipped (missing ID/Content or required text part for '{text_type}' or empty after preprocess): {cnt}\")\n",
    "         final_count = len(app_ids)\n",
    "         print(f\"Original corpus size: {len(corpus)}. Final corpus size: {final_count}\")\n",
    "         if final_count == 0:\n",
    "              print(f\"Warning: Resulting corpus for '{text_type}' is empty!\")\n",
    "\n",
    "    corpus_data = [{'id': app_id, 'text': text} for app_id, text in zip(app_ids, texts)]\n",
    "    return corpus_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate TF-IDF feature matrices for two sets of text data:\n",
    "- Concatenates `citing_texts` and `nonciting_texts` to fit the provided or default `TfidfVectorizer`.\n",
    "- Separately transforms the citing and non-citing texts into sparse TF-IDF matrices.\n",
    "- Prints progress updates and the resulting vocabulary size.\n",
    "- Returns three objects:\n",
    "  - `tfidf_matrix_citing`: TF-IDF matrix for citing documents.\n",
    "  - `tfidf_matrix_nonciting`: TF-IDF matrix for non-citing documents.\n",
    "  - `vectorizer`: The fitted TF-IDF vectorizer, which can be reused for other transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_matrix(citing_texts, nonciting_texts, vectorizer=TfidfVectorizer()):\n",
    "    all_text = citing_texts + nonciting_texts\n",
    "    print(\"Fitting TF-IDF Vectorizer...\")\n",
    "    vectorizer.fit(tqdm(all_text, desc=\"Fit TF-IDF\", leave=False))\n",
    "    print(\"Transforming Citing Texts...\")\n",
    "    tfidf_matrix_citing = vectorizer.transform(tqdm(citing_texts, desc=\"Transform Citing\", leave=False))\n",
    "    print(\"Transforming Non-Citing Texts...\")\n",
    "    tfidf_matrix_nonciting = vectorizer.transform(tqdm(nonciting_texts, desc=\"Transform Non-Citing\", leave=False))\n",
    "    print(\"Size of vocabulary:\", len(vectorizer.vocabulary_))\n",
    "    return tfidf_matrix_citing, tfidf_matrix_nonciting, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the BM25 ranking function for computing similarity between queries and documents:\n",
    "- **Initialization**: Accepts a matrix of vectorized documents (typically non-citing), along with BM25 parameters `k1` and `b`.\n",
    "- **`.fit()`**: \n",
    "  - Converts input to a CSR matrix if needed.\n",
    "  - Computes average document length and inverse document frequency (IDF) for each term.\n",
    "  - Ensures numerical stability with smoothing and clamping.\n",
    "- **`.predict()`**:\n",
    "  - Accepts a matrix of query vectors (typically citing documents).\n",
    "  - Calculates BM25 similarity scores between each query and all fitted documents.\n",
    "  - Uses the BM25 scoring formula in a vectorized fashion for efficiency.\n",
    "\n",
    "We generate BM25 similarity scores between citing and non-citing texts:\n",
    "- Fits a `CountVectorizer` on the combined corpus.\n",
    "- Transforms citing and non-citing texts into count matrices.\n",
    "- Initializes and fits the `BM25Score` model using the non-citing text matrix.\n",
    "- Computes and returns the BM25 similarity scores from citing to non-citing documents.\n",
    "- Also returns the fitted `vectorizer` and `bm25` model for reuse or inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Score:\n",
    "    def __init__(self, vectorized_docs, k1=1.5, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.vectorized_docs = vectorized_docs\n",
    "\n",
    "    def fit(self, vectorized_queries=None, query_ids=None, args=None):\n",
    "        if not isinstance(self.vectorized_docs, scipy.sparse.csr_matrix):\n",
    "            try:\n",
    "                self.vectorized_docs = scipy.sparse.csr_matrix(self.vectorized_docs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting BM25 input to CSR: {e}\")\n",
    "                raise\n",
    "\n",
    "        self.n_d = self.vectorized_docs.sum(axis=1).A\n",
    "        self.avgdl = np.mean(self.n_d)\n",
    "        if self.avgdl == 0:\n",
    "            print(\"Warning: Average document length is zero. Setting to 1.\")\n",
    "            self.avgdl = 1.0\n",
    "\n",
    "        self.n_docs = self.vectorized_docs.shape[0]\n",
    "        self.nq = np.array(self.vectorized_docs.getnnz(axis=0)).reshape(1,-1)\n",
    "        epsilon = 1e-9\n",
    "        self.idf = np.log(((self.n_docs - self.nq + 0.5) / (self.nq + 0.5 + epsilon)) + 1.0)\n",
    "        self.idf = np.maximum(self.idf, 0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, vectorized_queries):\n",
    "        if not isinstance(vectorized_queries, scipy.sparse.csr_matrix):\n",
    "            try:\n",
    "                vectorized_queries = scipy.sparse.csr_matrix(vectorized_queries)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting BM25 query input to CSR: {e}\")\n",
    "                raise\n",
    "\n",
    "        if vectorized_queries.shape[1] != self.vectorized_docs.shape[1]:\n",
    "             raise ValueError(f\"Query vector shape {vectorized_queries.shape} incompatible with document vector shape {self.vectorized_docs.shape}\")\n",
    "\n",
    "        idf = self.idf\n",
    "        term_freq_docs = self.vectorized_docs\n",
    "        term_freq_queries = vectorized_queries\n",
    "\n",
    "        doc_len_norm_factor = self.k1 * (1 - self.b + self.b * (self.n_d / self.avgdl))\n",
    "        k1_plus_1 = self.k1 + 1\n",
    "        denominator = term_freq_docs.copy().astype(np.float32)\n",
    "\n",
    "        denominator_dense = term_freq_docs.toarray() + doc_len_norm_factor\n",
    "        denominator_dense[denominator_dense == 0] = 1e-9\n",
    "\n",
    "        score_part_docs = term_freq_docs.multiply(k1_plus_1)\n",
    "        score_part_docs_dense = score_part_docs.toarray() / denominator_dense\n",
    "\n",
    "        weighted_scores = score_part_docs_dense * idf\n",
    "\n",
    "        query_term_presence = (term_freq_queries > 0).astype(np.float32)\n",
    "        final_scores = query_term_presence @ weighted_scores.T\n",
    "\n",
    "        return final_scores\n",
    "\n",
    "\n",
    "def create_bm25_matrix(citing_texts, nonciting_texts, vectorizer=CountVectorizer(), bm25_params={'k1': 1.5, 'b': 0.75}):\n",
    "    all_text = citing_texts + nonciting_texts\n",
    "    print(\"Fitting CountVectorizer...\")\n",
    "    vectorizer.fit(tqdm(all_text, desc=\"Fit CV\", leave=False))\n",
    "    print(\"Transforming Citing Texts...\")\n",
    "    count_matrix_citing = vectorizer.transform(tqdm(citing_texts, desc=\"Transform Citing\", leave=False))\n",
    "    print(\"Transforming Non-Citing Texts...\")\n",
    "    count_matrix_nonciting = vectorizer.transform(tqdm(nonciting_texts, desc=\"Transform Non-Citing\", leave=False))\n",
    "    print(\"Size of vocabulary:\", len(vectorizer.vocabulary_))\n",
    "    print(\"Fitting BM25 model...\")\n",
    "    bm25 = BM25Score(count_matrix_nonciting, k1=bm25_params.get('k1', 1.5), b=bm25_params.get('b', 0.75))\n",
    "    bm25.fit()\n",
    "    print(\"Computing BM25 scores...\")\n",
    "    bm25_scores = bm25.predict(count_matrix_citing)\n",
    "    return bm25_scores, vectorizer, bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate dense vector embeddings for input texts using a Sentence Transformers model:\n",
    "- Loads a specified model (default: `multi-qa-mpnet-base-dot-v1`) on GPU if available.\n",
    "- Encodes text in batches to produce dense embeddings as NumPy arrays.\n",
    "- Handles errors related to model loading or inference.\n",
    "- Returns `None` if Sentence Transformers is not available or if encoding fails.\n",
    "\n",
    "We compute cosine similarity between two sets of dense embeddings:\n",
    "- Accepts NumPy arrays or PyTorch tensors for citing and non-citing embeddings.\n",
    "- Converts PyTorch tensors to NumPy arrays if needed.\n",
    "- Returns a similarity matrix where each row represents a citing document and each column a non-citing one.\n",
    "- Returns `None` if embeddings are missing or similarity calculation fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dense_embeddings(texts, model_name='multi-qa-mpnet-base-dot-v1', batch_size=64):\n",
    "    if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Sentence Transformers not available. Skipping dense embeddings.\")\n",
    "        return None\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device} for embeddings\")\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name, device=device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Sentence Transformer model '{model_name}': {e}\")\n",
    "        return None\n",
    "    print(f\"Generating embeddings using {model_name}...\")\n",
    "    try:\n",
    "        embeddings = model.encode(texts, convert_to_tensor=True, show_progress_bar=True, batch_size=batch_size)\n",
    "        return embeddings.detach().cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Sentence Transformer encoding: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_dense_similarity(citing_embeddings, nonciting_embeddings):\n",
    "    if citing_embeddings is None or nonciting_embeddings is None:\n",
    "        print(\"Cannot calculate dense similarity due to missing embeddings.\")\n",
    "        return None\n",
    "    print(\"Calculating Dense Cosine Similarities...\")\n",
    "    if isinstance(citing_embeddings, torch.Tensor):\n",
    "        citing_embeddings = citing_embeddings.cpu().numpy()\n",
    "    if isinstance(nonciting_embeddings, torch.Tensor):\n",
    "        nonciting_embeddings = nonciting_embeddings.cpu().numpy()\n",
    "    try:\n",
    "        similarity_scores = cosine_similarity(citing_embeddings, nonciting_embeddings)\n",
    "        return similarity_scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating cosine similarity: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric Functions\n",
    "\n",
    "#### `get_true_and_predicted(citing_to_cited_dict, recommendations_dict)`\n",
    "Prepares ground truth and predicted labels for evaluation:\n",
    "- Matches citing document IDs from the recommendations with the mapping dictionary.\n",
    "- Returns:\n",
    "  - `true_labels`: list of cited document ID lists.\n",
    "  - `predicted_labels`: list of predicted document ID lists.\n",
    "  - `not_in_citation_mapping`: count of citing documents not found in the ground truth.\n",
    "\n",
    "#### `mean_recall_at_k(true_labels, predicted_labels, k=10)`\n",
    "Calculates the mean recall at cutoff rank `k`:\n",
    "- For each query, computes the fraction of relevant documents retrieved in the top-`k` predictions.\n",
    "- Returns the average recall across all queries.\n",
    "\n",
    "#### `mean_average_precision(true_labels, predicted_labels, k=10)`\n",
    "Computes Mean Average Precision (MAP) at rank `k`:\n",
    "- For each query, calculates precision at each relevant hit in the top-`k` predictions.\n",
    "- Averages these per-query precision values over all relevant documents.\n",
    "- Returns the mean of average precision scores across all queries.\n",
    "\n",
    "#### `mean_ranking(true_labels, predicted_labels)`\n",
    "Computes the average rank position of relevant documents:\n",
    "- For each query, determines the rank position of each relevant document in the predictions.\n",
    "- Returns the mean of these ranks across all queries.\n",
    "- Penalizes missed documents with the maximum possible rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping_dict(mapping_df):\n",
    "    mapping_dict = {}\n",
    "    if not isinstance(mapping_df, pd.DataFrame) or mapping_df.shape[1] < 3:\n",
    "        print(\"Warning: mapping_df invalid in get_mapping_dict.\")\n",
    "        return mapping_dict\n",
    "    for _, row in mapping_df.iterrows():\n",
    "        try:\n",
    "            key = row.iloc[0]\n",
    "            value = row.iloc[2]\n",
    "            if key in mapping_dict:\n",
    "                mapping_dict[key].append(value)\n",
    "            else:\n",
    "                mapping_dict[key] = [value]\n",
    "        except IndexError:\n",
    "            print(f\"Warning: Index error in mapping_df row: {row}\")\n",
    "            continue\n",
    "    return mapping_dict\n",
    "\n",
    "def get_true_and_predicted(citing_to_cited_dict, recommendations_dict):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    not_in_citation_mapping = 0\n",
    "    if not recommendations_dict: return [], [], 0\n",
    "    for citing_id in recommendations_dict.keys():\n",
    "        if citing_id in citing_to_cited_dict:\n",
    "            true_labels.append(citing_to_cited_dict[citing_id])\n",
    "            prediction = recommendations_dict[citing_id]\n",
    "            predicted_labels.append(prediction if isinstance(prediction, list) else [])\n",
    "        else:\n",
    "            not_in_citation_mapping += 1\n",
    "    return true_labels, predicted_labels, not_in_citation_mapping\n",
    "\n",
    "def mean_recall_at_k(true_labels, predicted_labels, k=10):\n",
    "    recalls_at_k = []\n",
    "    if not true_labels or not predicted_labels: return 0.0\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        if not isinstance(true, (list, set)) or not isinstance(pred, list): continue\n",
    "        true_set = set(true)\n",
    "        if not true_set: continue\n",
    "        actual_k = min(k, len(pred))\n",
    "        relevant_count = sum(1 for item in pred[:actual_k] if item in true_set)\n",
    "        recall = relevant_count / len(true_set)\n",
    "        recalls_at_k.append(recall)\n",
    "    mean_recall = sum(recalls_at_k) / len(recalls_at_k) if recalls_at_k else 0\n",
    "    return mean_recall\n",
    "\n",
    "def mean_average_precision(true_labels, predicted_labels, k=10):\n",
    "    average_precisions = []\n",
    "    if not true_labels or not predicted_labels: return 0.0\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        if not isinstance(true, (list, set)) or not isinstance(pred, list): continue\n",
    "        true_set = set(true)\n",
    "        if not true_set: continue\n",
    "        precision_at_k = []\n",
    "        relevant_count = 0\n",
    "        actual_k = min(k, len(pred))\n",
    "        for i, item in enumerate(pred[:actual_k]):\n",
    "            if item in true_set:\n",
    "                relevant_count += 1\n",
    "                precision_at_k.append(relevant_count / (i + 1))\n",
    "        average_precision = sum(precision_at_k) / len(true_set)\n",
    "        average_precisions.append(average_precision)\n",
    "    mean_average_precision_val = sum(average_precisions) / len(average_precisions) if average_precisions else 0\n",
    "    return mean_average_precision_val\n",
    "\n",
    "def mean_ranking(true_labels, predicted_labels):\n",
    "    mean_ranks = []\n",
    "    if not true_labels or not predicted_labels: return float('inf')\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        if not isinstance(true, (list, set)) or not isinstance(pred, list): continue\n",
    "        if not true: continue\n",
    "        ranks = []\n",
    "        pred_list = list(pred)\n",
    "        max_rank = len(pred_list) + 1\n",
    "        for item in true:\n",
    "            try:\n",
    "                rank = pred_list.index(item) + 1\n",
    "            except ValueError:\n",
    "                rank = max_rank\n",
    "            ranks.append(rank)\n",
    "        mean_rank = sum(ranks) / len(ranks) if ranks else max_rank\n",
    "        mean_ranks.append(mean_rank)\n",
    "    mean_of_mean_ranks = sum(mean_ranks) / len(mean_ranks) if mean_ranks else float('inf')\n",
    "    return mean_of_mean_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate top-`k` ranked recommendations for each citing document based on similarity scores:\n",
    "- Validates input dimensions and handles errors gracefully.\n",
    "- Converts sparse or non-array score types to NumPy arrays.\n",
    "- Sorts scores in descending order and retrieves top-`k` non-citing document IDs for each citing document.\n",
    "- Returns a dictionary: `{citing_id: [top_k_nonciting_ids]}`.\n",
    "\n",
    "We combine multiple ranking outputs using **Reciprocal Rank Fusion (RRF)**:\n",
    "- Requires at least two ranking dictionaries with overlapping query IDs.\n",
    "- Computes a combined score for each document using the formula: `1 / (k + rank)`.\n",
    "- Sorts documents per query by descending fused score.\n",
    "- Returns a new dictionary with combined rankings: `{query_id: [ranked_doc_ids]}`.\n",
    "\n",
    "RRF is useful for aggregating rankings from multiple models or similarity measures, enhancing diversity and robustness of final recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_ranks(citing_corpus_data, nonciting_corpus_data, similarity_scores, k=10):\n",
    "    top_k_results = {}\n",
    "    if similarity_scores is None or not citing_corpus_data or not nonciting_corpus_data:\n",
    "        print(\"Warning: Cannot generate ranks due to missing scores or corpus data.\")\n",
    "        return top_k_results\n",
    "\n",
    "    num_citing = similarity_scores.shape[0]\n",
    "    num_nonciting = len(nonciting_corpus_data)\n",
    "\n",
    "    if num_citing != len(citing_corpus_data):\n",
    "         print(f\"Warning: Citing scores ({num_citing}) != citing corpus ({len(citing_corpus_data)}). Adjusting...\")\n",
    "         num_citing = min(num_citing, len(citing_corpus_data))\n",
    "\n",
    "    if similarity_scores.shape[1] != num_nonciting:\n",
    "        print(f\"Warning: Similarity score columns ({similarity_scores.shape[1]}) != non-citing docs ({num_nonciting}). Cannot rank.\")\n",
    "        return {}\n",
    "\n",
    "    actual_k = min(k, num_nonciting)\n",
    "    print(f\"Generating top {actual_k} ranks...\")\n",
    "    for i in tqdm(range(num_citing), desc=\"Ranking\", leave=False):\n",
    "        try:\n",
    "            citing_id = citing_corpus_data[i]['id']\n",
    "            patent_scores = similarity_scores[i]\n",
    "            if isinstance(patent_scores, (np.matrix, scipy.sparse.spmatrix)):\n",
    "                patent_scores = patent_scores.toarray().flatten()\n",
    "            elif not isinstance(patent_scores, np.ndarray):\n",
    "                 patent_scores = np.array(patent_scores)\n",
    "\n",
    "            if patent_scores.ndim != 1 or len(patent_scores) != num_nonciting:\n",
    "                 print(f\"Warning: Skipping citing ID {citing_id} due to score shape/length mismatch.\")\n",
    "                 continue\n",
    "\n",
    "            top_indices = np.argsort(-patent_scores)[:actual_k] # Negate scores\n",
    "            top_nonciting_ids = [nonciting_corpus_data[j]['id'] for j in top_indices if j < num_nonciting]\n",
    "            top_k_results[citing_id] = top_nonciting_ids\n",
    "        except IndexError as e:\n",
    "            print(f\"Warning: Index error processing citing item {i} (ID: {citing_corpus_data[i].get('id', 'N/A')}). Skipping. Error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Unexpected error processing citing item {i} (ID: {citing_corpus_data[i].get('id', 'N/A')}): {e}. Skipping.\")\n",
    "    return top_k_results\n",
    "\n",
    "\n",
    "def combine_rankings_rrf(rank_dict_list, k_rrf=60):\n",
    "    print(f\"Combining {len(rank_dict_list)} rankings using RRF (k={k_rrf})...\")\n",
    "    if not rank_dict_list or len(rank_dict_list) < 2:\n",
    "        print(\"Warning: Need at least two ranking lists for RRF.\")\n",
    "        return rank_dict_list[0] if rank_dict_list else {}\n",
    "\n",
    "    query_ids = set(rank_dict_list[0].keys())\n",
    "    for r_dict in rank_dict_list[1:]:\n",
    "        query_ids.intersection_update(r_dict.keys())\n",
    "    if not query_ids:\n",
    "        print(\"Warning: No common query IDs found among ranking lists for RRF.\")\n",
    "        return {}\n",
    "\n",
    "    combined_scores = {query_id: {} for query_id in query_ids}\n",
    "    print(f\"Processing {len(query_ids)} common queries for RRF.\")\n",
    "\n",
    "    for ranks_dict in tqdm(rank_dict_list, desc=\"Processing Rank Lists\", leave=False):\n",
    "        for query_id in query_ids:\n",
    "            ranked_docs = ranks_dict.get(query_id, [])\n",
    "            for rank, doc_id in enumerate(ranked_docs):\n",
    "                rank_score = 1.0 / (k_rrf + rank + 1)\n",
    "                combined_scores[query_id][doc_id] = combined_scores[query_id].get(doc_id, 0) + rank_score\n",
    "\n",
    "    final_rankings = {}\n",
    "    for query_id, doc_scores in tqdm(combined_scores.items(), desc=\"Sorting RRF Results\", leave=False):\n",
    "        sorted_docs = sorted(doc_scores.items(), key=lambda item: (-item[1], item[0]))\n",
    "        final_rankings[query_id] = [doc_id for doc_id, score in sorted_docs]\n",
    "\n",
    "    return final_rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process Overview**:\n",
    "  1. **Corpus Creation**: Extracts and optionally preprocesses texts for citing and non-citing documents.\n",
    "  2. **Similarity Calculation**:\n",
    "     - `tfidf`: Uses `TfidfVectorizer` and cosine similarity.\n",
    "     - `bm25`: Uses `CountVectorizer` and BM25 scoring.\n",
    "     - `dense`: Uses Sentence Transformers and cosine similarity on embeddings.\n",
    "  3. **Ranking**: Generates top-k ranked results for each citing document.\n",
    "  4. **RRF Preparation**: Stores rankings if needed for Reciprocal Rank Fusion (RRF).\n",
    "  5. **Evaluation**: Computes key metrics:\n",
    "     - `Recall@10`, `20`, `50`, `100`\n",
    "     - `MAP@100` (Mean Average Precision)\n",
    "     - `Mean Rank`\n",
    "     - Count of measured queries and those missing from the mapping\n",
    "\n",
    "- **Outputs**:\n",
    "  - `metrics`: Dictionary of evaluation metrics.\n",
    "  - `model_details_run`: Includes trained vectorizers/models and non-citing representation data (matrix or embeddings).\n",
    "\n",
    "- **Error Handling**:\n",
    "  - Skips experiments if corpora are empty or libraries are missing.\n",
    "  - Gracefully handles exceptions and prints tracebacks for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config, json_citing_train, json_nonciting, mapping_dict, k_eval=100):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n--- Running Experiment: {config['name']} ---\")\n",
    "\n",
    "    if config['method'] == 'dense' and not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "        print(\"Skipping dense experiment as libraries are not available.\")\n",
    "        return None, None\n",
    "\n",
    "    print(\"Creating corpora...\")\n",
    "    citing_corpus = create_corpus(json_citing_train, config['text_type'], preprocess=config.get('preprocess', False), config=config)\n",
    "    nonciting_corpus = create_corpus(json_nonciting, config['text_type'], preprocess=config.get('preprocess', False), config=config)\n",
    "\n",
    "    if not citing_corpus or not nonciting_corpus:\n",
    "        print(\"Skipping experiment due to empty corpus.\")\n",
    "        return None, None\n",
    "\n",
    "    citing_texts = [doc['text'] for doc in citing_corpus]\n",
    "    nonciting_texts = [doc['text'] for doc in nonciting_corpus]\n",
    "\n",
    "    similarity_scores = None\n",
    "    fitted_vectorizer = None\n",
    "    fitted_bm25_model = None\n",
    "    nonciting_matrix_tfidf = None\n",
    "    nonciting_embeddings = None\n",
    "    model_details_run = {}\n",
    "\n",
    "    try:\n",
    "        if config['method'] == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(**config.get('vectorizer_params', {}))\n",
    "            tfidf_citing, tfidf_nonciting, fitted_vectorizer = create_tfidf_matrix(\n",
    "                citing_texts, nonciting_texts, vectorizer\n",
    "            )\n",
    "            print(\"Calculating Cosine Similarities...\")\n",
    "            similarity_scores = linear_kernel(tfidf_citing, tfidf_nonciting)\n",
    "            nonciting_matrix_tfidf = tfidf_nonciting\n",
    "\n",
    "        elif config['method'] == 'bm25':\n",
    "            vectorizer = CountVectorizer(**config.get('vectorizer_params', {}))\n",
    "            bm25_scores, fitted_vectorizer, fitted_bm25_model = create_bm25_matrix(\n",
    "                citing_texts, nonciting_texts, vectorizer, config.get('bm25_params', {})\n",
    "            )\n",
    "            similarity_scores = bm25_scores\n",
    "\n",
    "        elif config['method'] == 'dense':\n",
    "            print(\"Generating Dense Embeddings...\")\n",
    "            citing_embeddings = create_dense_embeddings(\n",
    "                citing_texts,\n",
    "                model_name=config.get('embedding_model'),\n",
    "                batch_size=config.get('embedding_batch_size')\n",
    "            )\n",
    "            nonciting_embeddings = create_dense_embeddings(\n",
    "                nonciting_texts,\n",
    "                model_name=config.get('embedding_model'),\n",
    "                batch_size=config.get('embedding_batch_size')\n",
    "            )\n",
    "            if citing_embeddings is None or nonciting_embeddings is None:\n",
    "                 raise ValueError(\"Dense embedding generation failed.\")\n",
    "            similarity_scores = calculate_dense_similarity(citing_embeddings, nonciting_embeddings)\n",
    "            model_details_run['nonciting_embeddings'] = nonciting_embeddings # Store needed embeddings\n",
    "\n",
    "        else:\n",
    "            print(f\"Unknown method: {config['method']}\")\n",
    "            return None, None\n",
    "\n",
    "        if similarity_scores is None:\n",
    "            raise ValueError(\"Failed to compute similarity scores.\")\n",
    "\n",
    "        print(f\"Shape of similarity/scores matrix: {similarity_scores.shape}\")\n",
    "\n",
    "        full_rank = top_k_ranks(citing_corpus, nonciting_corpus, similarity_scores, k=len(nonciting_corpus))\n",
    "\n",
    "        if config['name'] == best_bm25_config_name_for_rrf:\n",
    "            print(f\"Storing BM25 (Best MAP/Recall) ranks for RRF from {config['name']}...\")\n",
    "            global best_bm25_ranks_train\n",
    "            best_bm25_ranks_train = full_rank\n",
    "        elif config['name'] == best_dense_config_name_for_rrf:\n",
    "            print(f\"Storing Dense ranks for RRF from {config['name']}...\")\n",
    "            global best_dense_ranks_train\n",
    "            best_dense_ranks_train = full_rank\n",
    "        elif config['name'] == best_mean_rank_bm25_config_name:\n",
    "            print(f\"Storing BM25 (Best Mean Rank) ranks for RRF from {config['name']}...\")\n",
    "            global best_mean_rank_bm25_ranks_train\n",
    "            best_mean_rank_bm25_ranks_train = full_rank\n",
    "\n",
    "        top_k_rank_eval = {qid: ranks[:k_eval] for qid, ranks in full_rank.items()}\n",
    "        print(\"Calculating metrics...\")\n",
    "        true_labels, predicted_labels, not_in_mapping = get_true_and_predicted(mapping_dict, top_k_rank_eval)\n",
    "\n",
    "        if not predicted_labels:\n",
    "            print(\"No predictions generated for metric calculation.\")\n",
    "            metrics = {'recall@10': 0,'recall@20': 0,'recall@50': 0,'recall@100': 0, 'map@100': 0, 'mean_rank': float('inf'), 'num_measured': 0, 'not_in_mapping': not_in_mapping}\n",
    "        else:\n",
    "            metrics = {\n",
    "                'recall@10': mean_recall_at_k(true_labels, predicted_labels, k=10),\n",
    "                'recall@20': mean_recall_at_k(true_labels, predicted_labels, k=20),\n",
    "                'recall@50': mean_recall_at_k(true_labels, predicted_labels, k=50),\n",
    "                'recall@100': mean_recall_at_k(true_labels, predicted_labels, k=100),\n",
    "                'map@100': mean_average_precision(true_labels, predicted_labels, k=100),\n",
    "                'mean_rank': mean_ranking(true_labels, predicted_labels),\n",
    "                'num_measured': len(predicted_labels), 'not_in_mapping': not_in_mapping\n",
    "            }\n",
    "\n",
    "        print(f\"Recall@10: {metrics['recall@10']:.4f}\")\n",
    "        print(f\"Recall@100: {metrics['recall@100']:.4f}\")\n",
    "        print(f\"MAP@100: {metrics['map@100']:.4f}\")\n",
    "        print(f\"Mean Rank: {metrics['mean_rank']:.4f}\")\n",
    "\n",
    "        model_details_run.update({\n",
    "            'vectorizer': fitted_vectorizer, # None for dense\n",
    "            'bm25_model': fitted_bm25_model, # None for tfidf/dense\n",
    "            'nonciting_corpus': nonciting_corpus,\n",
    "            'nonciting_matrix': nonciting_matrix_tfidf, # None for bm25/dense\n",
    "            'nonciting_embeddings': model_details_run.get('nonciting_embeddings', None) # Added if dense\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during experiment '{config['name']}': {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Experiment '{config['name']}' completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    return metrics, model_details_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Files Loaded**:\n",
    "  - `citing_TRAIN.json`: Citing documents (training set)\n",
    "  - `citing_TEST.json`: Citing documents (test set)\n",
    "  - `CLEANED_CONTENT_DATASET_cited...json`: Pool of non-citing documents\n",
    "  - `Citation_Train.json`: Mapping of citing-to-cited pairs\n",
    "- **Validation**: Ensures all files are loaded; exits if any are missing.\n",
    "- **Metadata Printed**:\n",
    "  - Document counts for each loaded dataset\n",
    "  - Size of generated citing-to-cited mapping dictionary (`mapping_dict`)\n",
    "\n",
    "#### **Experiment Setup**\n",
    "- **Configurations Defined**:\n",
    "  - Specifies multiple experiment configurations using various retrieval methods (`bm25`, `dense`)\n",
    "  - Each config includes relevant parameters (e.g., preprocessing flags, model/vectorizer details)\n",
    "- **Best Configs for RRF**:\n",
    "  - Named configs designated for Reciprocal Rank Fusion (RRF), based on prior performance:\n",
    "    - `best_bm25_config_name_for_rrf`\n",
    "    - `best_dense_config_name_for_rrf`\n",
    "    - `best_mean_rank_bm25_config_name`\n",
    "- **Dense Model Handling**:\n",
    "  - Dense configurations are removed if Sentence Transformers library is not available.\n",
    "- **Results Initialization**:\n",
    "  - Tracks best performance across configurations using recall@100, MAP@100, and stores top model configuration and details.\n",
    "- **Global Variables for RRF**:\n",
    "  - Prepares variables (`best_bm25_ranks_train`, etc.) to store rankings used in later rank fusion steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading datasets...\")\n",
    "DATA_DIR = \"./datasets\"\n",
    "content_path = os.path.join(DATA_DIR, \"Content_JSONs\")\n",
    "citation_path = os.path.join(DATA_DIR, \"Citation_JSONs\")\n",
    "path_citing_train = os.path.join(content_path, \"Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TRAIN.json\")\n",
    "path_citing_test = os.path.join(content_path, \"Citing_2020_Cleaned_Content_12k/Citing_Train_Test/citing_TEST.json\")\n",
    "path_nonciting = os.path.join(content_path, \"Cited_2020_Uncited_2010-2019_Cleaned_Content_22k/CLEANED_CONTENT_DATASET_cited_patents_by_2020_uncited_2010-2019.json\")\n",
    "path_citations = os.path.join(citation_path, \"Citation_Train.json\")\n",
    "\n",
    "json_citing_train = load_json_data(path_citing_train)\n",
    "json_citing_test = load_json_data(path_citing_test)\n",
    "json_nonciting = load_json_data(path_nonciting)\n",
    "json_citing_to_cited = load_json_data(path_citations)\n",
    "\n",
    "if not all([json_citing_train, json_citing_test, json_nonciting, json_citing_to_cited]):\n",
    "    print(\"\\nCritical Error: One or more dataset files failed to load. Please check paths. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\nDatasets loaded successfully.\")\n",
    "print(f\"Citing Train: {len(json_citing_train)}\")\n",
    "print(f\"Citing Test: {len(json_citing_test)}\")\n",
    "print(f\"Non-Citing Pool: {len(json_nonciting)}\")\n",
    "print(f\"Training Citations Raw Pairs: {len(json_citing_to_cited)}\")\n",
    "\n",
    "mapping_dataset_df = pd.DataFrame(json_citing_to_cited)\n",
    "mapping_dict = get_mapping_dict(mapping_dataset_df)\n",
    "print(f\"Training Citations Dict (Unique Citing Patents): {len(mapping_dict)}\")\n",
    "\n",
    "print(\"\\nSetting up experiments...\")\n",
    "\n",
    "# Define base names for RRF components FIRST\n",
    "best_bm25_config_name_for_rrf = 'T+A+Claims BM25 (Pre, ngram=1, k1=2.0, b=0.9)'\n",
    "best_dense_config_name_for_rrf = 'Dense (multi-qa-mpnet, T+A+Claims)'\n",
    "best_mean_rank_bm25_config_name = 'T+A+Claims BM25 (Pre, ngram=1, k1=2.5, b=0.8)'\n",
    "\n",
    "configs = [\n",
    "    {'name': 'Title BM25', 'method': 'bm25', 'text_type': 'title', 'preprocess': False, 'vectorizer_params': {'stop_words': 'english', 'max_features': 10000}, 'bm25_params': {'k1': 1.5, 'b': 0.75}},\n",
    "    {'name': 'Claim1 BM25', 'method': 'bm25', 'text_type': 'claim1', 'preprocess': False, 'vectorizer_params': {'stop_words': 'english', 'max_features': 10000}, 'bm25_params': {'k1': 1.5, 'b': 0.75}},\n",
    "    {'name': best_bm25_config_name_for_rrf,\n",
    "     'method': 'bm25', 'text_type': 'title_abstract_claims',\n",
    "     'preprocess': True, 'use_stemming': False, 'use_custom_stopwords': True,\n",
    "     'vectorizer_params': {'max_features': 20000, 'ngram_range': (1, 1), 'min_df': 1},\n",
    "     'bm25_params': {'k1': 2.0, 'b': 0.9}},\n",
    "    {'name': best_mean_rank_bm25_config_name,\n",
    "     'method': 'bm25', 'text_type': 'title_abstract_claims',\n",
    "     'preprocess': True, 'use_stemming': False, 'use_custom_stopwords': True,\n",
    "     'vectorizer_params': {'max_features': 20000, 'ngram_range': (1, 1), 'min_df': 1},\n",
    "     'bm25_params': {'k1': 2.5, 'b': 0.8}},\n",
    "    {'name': best_dense_config_name_for_rrf,\n",
    "     'method': 'dense', 'text_type': 'title_abstract_claims', 'preprocess': False,\n",
    "     'embedding_model': 'multi-qa-mpnet-base-dot-v1', 'embedding_batch_size': 128 },\n",
    "    {'name': 'Dense (PatentSBERTa, T+A+Claims)',\n",
    "     'method': 'dense', 'text_type': 'title_abstract_claims', 'preprocess': False,\n",
    "     'embedding_model': 'AI-Growth-Lab/PatentSBERTa', 'embedding_batch_size': 64 },\n",
    "]\n",
    "\n",
    "if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "    print(\"\\nSentence Transformers not available, removing Dense configurations.\")\n",
    "    configs = [c for c in configs if c['method'] != 'dense']\n",
    "\n",
    "results = {}\n",
    "best_recall_100 = -1.0\n",
    "best_map_100 = -1.0\n",
    "best_config_name_recall = None\n",
    "best_config_name_map = None\n",
    "best_model_details = {}\n",
    "best_model_config = None\n",
    "\n",
    "best_bm25_ranks_train = None\n",
    "best_dense_ranks_train = None\n",
    "best_mean_rank_bm25_ranks_train = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Running Configured Experiments**\n",
    "- Iterates over each configuration in `configs` and runs the corresponding experiment using `run_experiment`.\n",
    "- Uses `k_eval = 100` for metric evaluation (e.g., Recall@100, MAP@100).\n",
    "\n",
    "#### **Results Tracking**\n",
    "- Stores metrics for each experiment under `results[config_name]`.\n",
    "- Continuously updates and tracks:\n",
    "  - **Best MAP@100**:\n",
    "    - Stores name, config, and model details for the best-performing model by MAP.\n",
    "  - **Best Recall@100**:\n",
    "    - Similarly tracks the configuration with the highest recall at rank 100.\n",
    "- If the same config achieves both best MAP and best Recall, it's saved as the best overall model.\n",
    "\n",
    "#### **Error Handling**\n",
    "- Prints a message if an experiment fails or returns no metrics, ensuring traceability during batch runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Running Experiments on Training Data ===\")\n",
    "k_eval_metrics = 100\n",
    "\n",
    "if not all([json_citing_train, json_nonciting, mapping_dict]):\n",
    "     print(\"Cannot run experiments, datasets not loaded properly.\")\n",
    "else:\n",
    "    for config in configs:\n",
    "        metrics, model_details_run = run_experiment(config, json_citing_train, json_nonciting, mapping_dict, k_eval=k_eval_metrics)\n",
    "        if metrics:\n",
    "            results[config['name']] = metrics\n",
    "            current_recall_100 = metrics['recall@100']\n",
    "            current_map_100 = metrics['map@100']\n",
    "\n",
    "            if current_map_100 > best_map_100:\n",
    "                 best_map_100 = current_map_100\n",
    "                 best_config_name_map = config['name']\n",
    "                 best_model_details = model_details_run\n",
    "                 best_model_config = config\n",
    "                 print(f\"*** New best MAP@100 model found: {best_config_name_map} ({best_map_100:.4f}) ***\")\n",
    "\n",
    "            if current_recall_100 > best_recall_100:\n",
    "                 best_recall_100 = current_recall_100\n",
    "                 best_config_name_recall = config['name']\n",
    "                 if config['name'] == best_config_name_map:\n",
    "                     best_model_details = model_details_run\n",
    "                     best_model_config = config\n",
    "                 print(f\"*** New best Recall@100 model found: {best_config_name_recall} ({best_recall_100:.4f}) ***\")\n",
    "        else:\n",
    "            print(f\"--- Experiment {config['name']} failed or produced no results. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RRF on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Overview**\n",
    "This section evaluates hybrid retrieval methods using **Reciprocal Rank Fusion (RRF)** to combine results from multiple base models (e.g., BM25 and Dense).\n",
    "\n",
    "#### **Prerequisites**\n",
    "- RRF is only executed if ranking results are available from:\n",
    "  - Best MAP@100 BM25 config (`best_bm25_ranks_train`)\n",
    "  - Best Mean Rank BM25 config (`best_mean_rank_bm25_ranks_train`)\n",
    "  - Best Dense config (`best_dense_ranks_train`)\n",
    "\n",
    "Warnings are shown if required rankings are missing.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Variant 1: RRF (Best MAP BM25 + Best Dense)**\n",
    "- Combines rankings from:\n",
    "  - BM25 model with best MAP@100\n",
    "  - Dense model with best MAP@100\n",
    "- Evaluates RRF with varying `k` values: `10`, `60`, and `120`.\n",
    "- For each fusion, calculates metrics:\n",
    "  - `Recall@10/20/50/100`\n",
    "  - `MAP@100`\n",
    "  - `Mean Rank`\n",
    "- Tracks and updates the best RRF configuration based on MAP@100.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Variant 2: RRF (Best Mean Rank BM25 + Best Dense)**\n",
    "- Uses a fixed `k=60` to combine:\n",
    "  - BM25 model with best Mean Rank\n",
    "  - Dense model with best MAP@100\n",
    "- Evaluates and tracks the resulting metrics similar to Variant 1.\n",
    "- Updates the best RRF config if it outperforms others based on MAP@100.\n",
    "\n",
    "---\n",
    "\n",
    "#### **RRF Results Storage**\n",
    "- All evaluated RRF configurations and their metrics are stored in `rrf_results`.\n",
    "- The best-performing fusion (by MAP@100) is saved in `best_rrf_config_details`.\n",
    "\n",
    "RRF offers a powerful ensemble approach to blend complementary retrieval strengths from lexical (BM25) and semantic (dense) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Evaluating Hybrid RRF Variants on Training Data ===\")\n",
    "rrf_results = {}\n",
    "best_rrf_map = -1.0\n",
    "best_rrf_config_details = {}\n",
    "\n",
    "rrf_possible_best_map = best_bm25_ranks_train is not None and best_dense_ranks_train is not None\n",
    "rrf_possible_best_mean_rank = best_mean_rank_bm25_ranks_train is not None and best_dense_ranks_train is not None\n",
    "\n",
    "if not rrf_possible_best_map: print(f\"Warning: Ranks missing for BM25 MAP ('{best_bm25_config_name_for_rrf}') or Dense ('{best_dense_config_name_for_rrf}'). Cannot run primary RRF.\")\n",
    "if not rrf_possible_best_mean_rank: print(f\"Warning: Ranks missing for BM25 Mean Rank ('{best_mean_rank_bm25_config_name}') or Dense ('{best_dense_config_name_for_rrf}'). Cannot run alternative RRF.\")\n",
    "\n",
    "# Variant 1: Best MAP/Recall BM25 + Best Dense, tune RRF k\n",
    "if rrf_possible_best_map:\n",
    "    base_rank_list = [best_bm25_ranks_train, best_dense_ranks_train]\n",
    "    bm25_map_val = results.get(best_bm25_config_name_for_rrf, {}).get('map@100', 0)\n",
    "    dense_map_val = results.get(best_dense_config_name_for_rrf, {}).get('map@100', 0)\n",
    "    base_component_names = f\"BM25(MAP={bm25_map_val:.3f}) + Dense(MAP={dense_map_val:.3f})\"\n",
    "\n",
    "    for rrf_k_val in [10, 60, 120]:\n",
    "        rrf_name = f\"RRF (k={rrf_k_val}, {base_component_names})\"\n",
    "        print(f\"\\nEvaluating: {rrf_name}\")\n",
    "        try:\n",
    "            rrf_combined_ranks = combine_rankings_rrf(base_rank_list, k_rrf=rrf_k_val)\n",
    "            true_labels_rrf, predicted_labels_rrf, not_in_mapping_rrf = get_true_and_predicted(mapping_dict, rrf_combined_ranks)\n",
    "            if not predicted_labels_rrf: raise ValueError(\"No predictions from RRF combine.\")\n",
    "\n",
    "            metrics = {\n",
    "                'recall@10': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=10),\n",
    "                'recall@20': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=20),\n",
    "                'recall@50': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=50),\n",
    "                'recall@100': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=100),\n",
    "                'map@100': mean_average_precision(true_labels_rrf, predicted_labels_rrf, k=100),\n",
    "                'mean_rank': mean_ranking(true_labels_rrf, predicted_labels_rrf),\n",
    "                'num_measured': len(predicted_labels_rrf), 'not_in_mapping': not_in_mapping_rrf\n",
    "            }\n",
    "            rrf_results[rrf_name] = metrics\n",
    "            print(f\"  RRF Metrics: R@100={metrics['recall@100']:.4f}, MAP@100={metrics['map@100']:.4f}, MeanRank={metrics['mean_rank']:.2f}\")\n",
    "\n",
    "            if metrics['map@100'] > best_rrf_map:\n",
    "                best_rrf_map = metrics['map@100']\n",
    "                best_rrf_config_details = {\n",
    "                    'name': rrf_name, 'k': rrf_k_val,\n",
    "                    'bm25_config_name': best_bm25_config_name_for_rrf,\n",
    "                    'dense_config_name': best_dense_config_name_for_rrf,\n",
    "                    'metrics': metrics}\n",
    "                print(f\"  *** New best RRF configuration found: {rrf_name} (MAP@100: {best_rrf_map:.4f}) ***\")\n",
    "        except Exception as e: print(f\"Error evaluating {rrf_name}: {e}\")\n",
    "\n",
    "# Variant 2: Best Mean Rank BM25 + Best Dense, k=60\n",
    "if rrf_possible_best_mean_rank:\n",
    "    alt_rank_list = [best_mean_rank_bm25_ranks_train, best_dense_ranks_train]\n",
    "    bm25_mr_val = results.get(best_mean_rank_bm25_config_name,{}).get('mean_rank', float('inf'))\n",
    "    dense_map_val = results.get(best_dense_config_name_for_rrf,{}).get('map@100',0)\n",
    "    alt_component_names = f\"BM25(MR={bm25_mr_val:.2f}) + Dense(MAP={dense_map_val:.3f})\"\n",
    "    rrf_k_val = 60\n",
    "    rrf_name = f\"RRF (k={rrf_k_val}, {alt_component_names})\"\n",
    "    print(f\"\\nEvaluating: {rrf_name}\")\n",
    "    try:\n",
    "        rrf_combined_ranks = combine_rankings_rrf(alt_rank_list, k_rrf=rrf_k_val)\n",
    "        true_labels_rrf, predicted_labels_rrf, not_in_mapping_rrf = get_true_and_predicted(mapping_dict, rrf_combined_ranks)\n",
    "        if not predicted_labels_rrf: raise ValueError(\"No predictions from RRF combine.\")\n",
    "\n",
    "        metrics = {\n",
    "            'recall@10': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=10),\n",
    "            'recall@20': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=20),\n",
    "            'recall@50': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=50),\n",
    "            'recall@100': mean_recall_at_k(true_labels_rrf, predicted_labels_rrf, k=100),\n",
    "            'map@100': mean_average_precision(true_labels_rrf, predicted_labels_rrf, k=100),\n",
    "            'mean_rank': mean_ranking(true_labels_rrf, predicted_labels_rrf),\n",
    "            'num_measured': len(predicted_labels_rrf), 'not_in_mapping': not_in_mapping_rrf\n",
    "        }\n",
    "        rrf_results[rrf_name] = metrics\n",
    "        print(f\"  RRF Metrics: R@100={metrics['recall@100']:.4f}, MAP@100={metrics['map@100']:.4f}, MeanRank={metrics['mean_rank']:.2f}\")\n",
    "\n",
    "        if metrics['map@100'] > best_rrf_map:\n",
    "            best_rrf_map = metrics['map@100']\n",
    "            best_rrf_config_details = {\n",
    "                'name': rrf_name, 'k': rrf_k_val,\n",
    "                'bm25_config_name': best_mean_rank_bm25_config_name, # Use the mean rank one\n",
    "                'dense_config_name': best_dense_config_name_for_rrf,\n",
    "                'metrics': metrics}\n",
    "            print(f\"  *** New best RRF configuration found: {rrf_name} (MAP@100: {best_rrf_map:.4f}) ***\")\n",
    "    except Exception as e: print(f\"Error evaluating {rrf_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine final best prediction method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Objective**\n",
    "Identify the best-performing prediction approach using **MAP@100** as the primary evaluation metric.\n",
    "\n",
    "#### **Selection Logic**\n",
    "- Compares the best RRF configuration's `MAP@100` (`best_rrf_map`) with the best single model's `MAP@100` (`best_map_100`).\n",
    "- **If RRF outperforms all single models**:\n",
    "  - Selects RRF as the best method.\n",
    "  - Stores its config and metrics in `final_prediction_config`.\n",
    "- **Otherwise**:\n",
    "  - Selects the best single model configuration.\n",
    "  - Also stores associated model details (e.g., vectorizer or embeddings).\n",
    "\n",
    "#### **Fallback Handling**\n",
    "- If no valid method is determined, logs a warning for further inspection.\n",
    "\n",
    "#### **Results Update**\n",
    "- Merges `rrf_results` into the overall `results` dictionary for unified tracking.\n",
    "\n",
    "This step finalizes which method should be used for making predictions and optionally for downstream testing or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Determining Best Prediction Method ---\")\n",
    "best_method_for_prediction = None\n",
    "final_prediction_config = None\n",
    "\n",
    "if best_rrf_map > best_map_100 and best_rrf_config_details:\n",
    "    print(f\"Best method is RRF: '{best_rrf_config_details['name']}' (MAP@100: {best_rrf_map:.4f})\")\n",
    "    best_method_for_prediction = 'rrf'\n",
    "    final_prediction_config = best_rrf_config_details # Store RRF details\n",
    "elif best_config_name_map:\n",
    "    print(f\"Best method is Single Model: '{best_config_name_map}' (MAP@100: {best_map_100:.4f})\")\n",
    "    best_method_for_prediction = best_config_name_map\n",
    "    final_prediction_config = best_model_config # Config dict of the best single model\n",
    "    if final_prediction_config: final_prediction_config['details'] = best_model_details # Attach fitted objects\n",
    "else:\n",
    "    print(\"Warning: Could not determine best method. Check experiment results and logs.\")\n",
    "\n",
    "results.update(rrf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Experiment Results Summary ===\")\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results).T.sort_values(by='map@100', ascending=False) # Sort by MAP\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    print(results_df)\n",
    "    pd.reset_option('display.max_rows')\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    k_values_plot = [10, 20, 50, 100]\n",
    "    sorted_results_plot = sorted(results.items(), key=lambda item: item[1].get('recall@100', 0), reverse=True)\n",
    "    for name, metrics_res in sorted_results_plot:\n",
    "        recalls = [metrics_res.get(f'recall@{k}', 0) for k in k_values_plot]\n",
    "        if any(not isinstance(r, (int, float)) for r in recalls): continue\n",
    "        plt.plot(k_values_plot, recalls, label=f\"{name} (MAP@100: {metrics_res.get('map@100', 0):.3f})\", marker='o', linewidth=1.5, markersize=5)\n",
    "\n",
    "    plt.xlabel('Top K')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Recall@K Comparison of Methods (Train Set)')\n",
    "    plt.xticks(k_values_plot)\n",
    "    plt.legend(loc='best', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.tight_layout(rect=[0, 0, 0.75, 1])\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test predictions for CodaBench\n",
    "\n",
    "#### **Generating Predictions Using Best Method**\n",
    "- Determines how to generate predictions based on the selected best-performing method (`best_method_for_prediction`), which could be:\n",
    "  - **RRF-based (Hybrid)**: Combines BM25 and Dense model rankings.\n",
    "  - **Single Model**: Uses either BM25, TF-IDF, or Dense embedding-based similarity.\n",
    "\n",
    "---\n",
    "\n",
    "#### **RRF Workflow**\n",
    "- Validates availability of component configs and required libraries.\n",
    "- Steps:\n",
    "  1. **Corpus Preparation**:\n",
    "     - Creates separate citing and non-citing corpora for both BM25 and Dense models.\n",
    "  2. **BM25 Scoring**:\n",
    "     - Fits a `CountVectorizer` on training + non-citing data.\n",
    "     - Scores test citing documents using a fresh `BM25Score` model.\n",
    "  3. **Dense Embedding & Similarity**:\n",
    "     - Generates Sentence Transformer embeddings for test corpora.\n",
    "     - Computes cosine similarity matrix.\n",
    "  4. **Rank Fusion via RRF**:\n",
    "     - Combines top-ranked results using Reciprocal Rank Fusion (RRF) with the best `k`.\n",
    "     - Filters to top `k_submission` (e.g., 100) for final output.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Single Model Workflow**\n",
    "- Loads previously stored components from training:\n",
    "  - Vectorizer / BM25 model / Dense embeddings.\n",
    "- Based on method:\n",
    "  - **TF-IDF**: Transforms test citing docs and applies `linear_kernel` similarity.\n",
    "  - **BM25**: Transforms test citing docs and uses stored BM25 model to score.\n",
    "  - **Dense**: Encodes citing texts and compares to stored non-citing embeddings using cosine similarity.\n",
    "- Produces top-k predictions from the similarity scores.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Output Handling**\n",
    "- Saves final predictions to a JSON file (default: `prediction1.json`) with structure:\n",
    "  ```json\n",
    "  {\n",
    "      \"citing_id_1\": [\"ranked_id_1\", \"ranked_id_2\", ..., \"ranked_id_100\"],\n",
    "      ...\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Generating Test Predictions for CodaBench ===\")\n",
    "\n",
    "if not best_method_for_prediction:\n",
    "     print(\"Error: No best method determined. Cannot generate predictions.\")\n",
    "     exit()\n",
    "\n",
    "print(f\"Selected approach for final prediction: {best_method_for_prediction}\")\n",
    "\n",
    "k_submission = 100\n",
    "test_predictions = None\n",
    "output_filename = 'prediction1.json'\n",
    "\n",
    "if best_method_for_prediction == 'rrf':\n",
    "    print(\"\\nGenerating RRF predictions for test set...\")\n",
    "    if not final_prediction_config or 'bm25_config_name' not in final_prediction_config or 'dense_config_name' not in final_prediction_config:\n",
    "         print(\"Error: RRF selected, but configuration details are missing.\")\n",
    "    else:\n",
    "        config_bm25 = next((c for c in configs if c['name'] == final_prediction_config['bm25_config_name']), None)\n",
    "        config_dense = next((c for c in configs if c['name'] == final_prediction_config['dense_config_name']), None)\n",
    "        rrf_k_val = final_prediction_config['k']\n",
    "\n",
    "        if not config_bm25 or not config_dense:\n",
    "            print(\"Error: Could not find original configurations for RRF components. Cannot proceed.\")\n",
    "        elif not SENTENCE_TRANSFORMERS_AVAILABLE and config_dense['method'] == 'dense':\n",
    "             print(\"Error: RRF requires dense model, but sentence-transformers is not available.\")\n",
    "        else:\n",
    "            try:\n",
    "                # A. Prepare Corpora\n",
    "                print(\"Creating corpora for RRF test prediction...\")\n",
    "                citing_corpus_test_bm25 = create_corpus(json_citing_test, config_bm25['text_type'], preprocess=True, config=config_bm25)\n",
    "                citing_texts_test_bm25 = [doc['text'] for doc in citing_corpus_test_bm25]\n",
    "                nonciting_corpus_bm25 = create_corpus(json_nonciting, config_bm25['text_type'], preprocess=True, config=config_bm25)\n",
    "                nonciting_texts_bm25 = [doc['text'] for doc in nonciting_corpus_bm25]\n",
    "\n",
    "                citing_corpus_test_dense = create_corpus(json_citing_test, config_dense['text_type'], preprocess=False, config=config_dense)\n",
    "                citing_texts_test_dense = [doc['text'] for doc in citing_corpus_test_dense]\n",
    "                nonciting_corpus_dense = create_corpus(json_nonciting, config_dense['text_type'], preprocess=False, config=config_dense)\n",
    "                nonciting_texts_dense = [doc['text'] for doc in nonciting_corpus_dense]\n",
    "\n",
    "                if not all([citing_corpus_test_bm25, nonciting_corpus_bm25, citing_corpus_test_dense, nonciting_corpus_dense]):\n",
    "                    raise ValueError(\"One or more corpora creation failed for RRF.\")\n",
    "\n",
    "                # B. Get BM25 Ranks for Test Set\n",
    "                print(f\"\\nCalculating BM25 scores for test set (using {config_bm25['name']} settings)...\")\n",
    "                train_corpus_bm25 = create_corpus(json_citing_train, config_bm25['text_type'], preprocess=True, config=config_bm25)\n",
    "                all_train_texts_bm25 = [d['text'] for d in train_corpus_bm25] + nonciting_texts_bm25\n",
    "                bm25_vectorizer = CountVectorizer(**config_bm25['vectorizer_params'])\n",
    "                bm25_vectorizer.fit(tqdm(all_train_texts_bm25, desc=\"Fit BM25 Vectorizer\", leave=False))\n",
    "                test_citing_counts = bm25_vectorizer.transform(tqdm(citing_texts_test_bm25, desc=\"Transform Test Citing (BM25)\"))\n",
    "                test_nonciting_counts = bm25_vectorizer.transform(tqdm(nonciting_texts_bm25, desc=\"Transform Non-Citing (BM25)\"))\n",
    "                bm25_model_test = BM25Score(test_nonciting_counts, **config_bm25['bm25_params'])\n",
    "                bm25_model_test.fit()\n",
    "                test_bm25_scores = bm25_model_test.predict(test_citing_counts)\n",
    "                print(f\"Shape of test BM25 scores matrix: {test_bm25_scores.shape}\")\n",
    "                test_bm25_ranks = top_k_ranks(citing_corpus_test_bm25, nonciting_corpus_bm25, test_bm25_scores, k=max(k_submission * 2, 500)) # Increase candidate pool size\n",
    "\n",
    "                # C. Get Dense Ranks for Test Set\n",
    "                print(f\"\\nCalculating Dense embeddings/similarities for test set (using {config_dense['name']} settings)...\")\n",
    "                test_citing_embed = create_dense_embeddings(citing_texts_test_dense, model_name=config_dense['embedding_model'], batch_size=config_dense['embedding_batch_size'])\n",
    "                test_nonciting_embed = create_dense_embeddings(nonciting_texts_dense, model_name=config_dense['embedding_model'], batch_size=config_dense['embedding_batch_size'])\n",
    "                if test_citing_embed is None or test_nonciting_embed is None: raise ValueError(\"Dense embedding failed for test.\")\n",
    "                test_dense_sim = calculate_dense_similarity(test_citing_embed, test_nonciting_embed)\n",
    "                if test_dense_sim is None: raise ValueError(\"Dense similarity failed for test.\")\n",
    "                print(f\"Shape of test Dense similarity matrix: {test_dense_sim.shape}\")\n",
    "                test_dense_ranks = top_k_ranks(citing_corpus_test_dense, nonciting_corpus_dense, test_dense_sim, k=max(k_submission * 2, 500)) # Increase candidate pool size\n",
    "\n",
    "                # D. Combine Ranks using RRF with the best k\n",
    "                print(f\"\\nCombining test rankings using RRF (k={rrf_k_val})...\")\n",
    "                common_test_citing_ids = set(test_bm25_ranks.keys()).intersection(test_dense_ranks.keys())\n",
    "                if len(common_test_citing_ids) < len(citing_corpus_test_bm25): # Check if we lost test queries\n",
    "                     print(f\"Warning: Mismatch in test citing IDs between BM25 ({len(test_bm25_ranks)}) and Dense ({len(test_dense_ranks)}). Using {len(common_test_citing_ids)} common IDs.\")\n",
    "                rank_list_for_rrf = [\n",
    "                    {qid: ranks for qid, ranks in test_bm25_ranks.items() if qid in common_test_citing_ids},\n",
    "                    {qid: ranks for qid, ranks in test_dense_ranks.items() if qid in common_test_citing_ids}\n",
    "                ]\n",
    "                test_predictions_rrf_combined = combine_rankings_rrf(rank_list_for_rrf, k_rrf=rrf_k_val)\n",
    "\n",
    "                # E. Trim to final k for submission\n",
    "                test_predictions = {qid: ranks[:k_submission] for qid, ranks in test_predictions_rrf_combined.items()}\n",
    "                print(f\"Generated RRF predictions for {len(test_predictions)} test patents.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during RRF test prediction generation: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                test_predictions = None # Ensure None on error\n",
    "\n",
    "elif best_method_for_prediction and final_prediction_config: # Fallback to best single model\n",
    "    print(f\"\\nGenerating predictions using best single model: {best_method_for_prediction}\")\n",
    "    best_config = final_prediction_config\n",
    "    single_model_details = best_config.get('details', {})\n",
    "\n",
    "    if not single_model_details:\n",
    "         print(f\"Error: Details (fitted models) for the best single model '{best_method_for_prediction}' are missing.\")\n",
    "    elif best_config['method'] == 'dense' and not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "         print(\"Error: Best single model is dense, but sentence-transformers not available.\")\n",
    "    else:\n",
    "        try:\n",
    "            print(\"Creating test citing corpus...\")\n",
    "            citing_corpus_test = create_corpus(json_citing_test, best_config['text_type'], preprocess=best_config.get('preprocess', False), config=best_config)\n",
    "            citing_texts_test = [doc['text'] for doc in citing_corpus_test]\n",
    "\n",
    "            # Retrieve components from the *training run* details stored in best_config\n",
    "            fitted_vectorizer = single_model_details.get('vectorizer')\n",
    "            fitted_bm25_model = single_model_details.get('bm25_model')\n",
    "            nonciting_corpus_for_ranking = single_model_details.get('nonciting_corpus')\n",
    "            nonciting_matrix_tfidf = single_model_details.get('nonciting_matrix')\n",
    "            nonciting_embeddings = single_model_details.get('nonciting_embeddings')\n",
    "\n",
    "            if not citing_corpus_test or not nonciting_corpus_for_ranking:\n",
    "                print(\"Test citing corpus or non-citing corpus for ranking is missing/empty.\")\n",
    "            else:\n",
    "                test_similarity_scores = None\n",
    "                print(f\"Applying method: {best_config['method']}\")\n",
    "                if best_config['method'] == 'tfidf':\n",
    "                     if fitted_vectorizer and nonciting_matrix_tfidf is not None:\n",
    "                         citing_matrix_test = fitted_vectorizer.transform(tqdm(citing_texts_test, desc=\"Transform Test Citing (TFIDF)\"))\n",
    "                         test_similarity_scores = linear_kernel(citing_matrix_test, nonciting_matrix_tfidf)\n",
    "                     else: print(\"Error: Missing components for TF-IDF prediction.\")\n",
    "                elif best_config['method'] == 'bm25':\n",
    "                     if fitted_vectorizer and fitted_bm25_model:\n",
    "                         citing_matrix_test = fitted_vectorizer.transform(tqdm(citing_texts_test, desc=\"Transform Test Citing (BM25)\"))\n",
    "                         test_similarity_scores = fitted_bm25_model.predict(citing_matrix_test)\n",
    "                     else: print(\"Error: Missing components for BM25 prediction.\")\n",
    "                elif best_config['method'] == 'dense':\n",
    "                     if nonciting_embeddings is not None:\n",
    "                         citing_embeddings_test = create_dense_embeddings(\n",
    "                             citing_texts_test,\n",
    "                             model_name=best_config.get('embedding_model'),\n",
    "                             batch_size=best_config.get('embedding_batch_size')\n",
    "                         )\n",
    "                         if citing_embeddings_test is not None:\n",
    "                              test_similarity_scores = calculate_dense_similarity(citing_embeddings_test, nonciting_embeddings)\n",
    "                         else: print(\"Error generating test dense embeddings.\")\n",
    "                     else: print(\"Error: Missing non-citing embeddings for dense prediction.\")\n",
    "\n",
    "                if test_similarity_scores is not None:\n",
    "                    print(f\"Shape of test similarity/scores matrix: {test_similarity_scores.shape}\")\n",
    "                    test_predictions = top_k_ranks(citing_corpus_test, nonciting_corpus_for_ranking, test_similarity_scores, k=k_submission)\n",
    "                    print(f\"Generated single model predictions for {len(test_predictions)} test patents.\")\n",
    "                else:\n",
    "                    print(\"Failed to compute test similarity scores for single best model.\")\n",
    "                    test_predictions = None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during single model test prediction generation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            test_predictions = None\n",
    "else:\n",
    "    print(\"No best model configuration identified or details missing. Cannot generate predictions.\")\n",
    "\n",
    "\n",
    "# 5. Save Final Predictions to JSON\n",
    "if test_predictions is not None and isinstance(test_predictions, dict) and test_predictions:\n",
    "    print(f\"\\nSaving final predictions ({len(test_predictions)} queries) to {output_filename} using method: {best_method_for_prediction}...\")\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(test_predictions, f, indent=4)\n",
    "        print(\"Predictions saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving predictions: {e}\")\n",
    "elif test_predictions is None:\n",
    "     print(\"No predictions were generated due to errors.\")\n",
    "else:\n",
    "     print(\"Predictions dictionary is empty, not saving.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, models, util\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import types\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Base Configuration (`config`)**\n",
    "Defines default settings used across all experiments, which can be overridden per experiment:\n",
    "\n",
    "- **File Paths**:\n",
    "  - `query_list_file`: List of queries for prediction.\n",
    "  - `pre_ranking_file`: Initial candidate document rankings (e.g., from BM25).\n",
    "  - `queries_content_file`, `documents_content_file`: Content and features for queries and candidate documents.\n",
    "\n",
    "- **Model Defaults**:\n",
    "  - `reranker_type`: `'bi-encoder'` (default, can also be `'cross-encoder'`)\n",
    "  - `bi_encoder_model`: `AI-Growth-Lab/PatentSBERTa`\n",
    "  - `cross_encoder_model`: `cross-encoder/ms-marco-MiniLM-L-6-v2`\n",
    "  - `text_type`: `'tac1'` (can be changed to e.g., `'claims'` or `'TA'`)\n",
    "  - `max_length`: Token limit for encoding input text.\n",
    "\n",
    "- **Execution Settings**:\n",
    "  - `batch_size`: Batch size for model inference.\n",
    "  - `device`: Automatically set to `'cuda'` if available, else falls back to `'cpu'`.\n",
    "\n",
    "- **Output Settings**:\n",
    "  - `save_individual_predictions`: If `True`, saves output JSON for each experiment.\n",
    "  - `output_file_prefix`: Prefix for saved prediction files.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Experiment Definitions (`experiments`)**\n",
    "A list of experiment configurations to run:\n",
    "\n",
    "- **Bi-Encoder Experiments**:\n",
    "  - Vary in model (`PatentSBERTa`, `MPNet`, `MultiQA`) and text type (`tac1`, `claims`).\n",
    "- **Cross-Encoder Experiments**:\n",
    "  - Use `cross-encoder/ms-marco-MiniLM-L-6-v2`.\n",
    "  - Test different text types: `tac1`, `claims`, and `TA`.\n",
    "\n",
    "Each experiment includes:\n",
    "- `exp_id`: A unique identifier.\n",
    "- `reranker_type`: Model type (`bi-encoder` or `cross-encoder`).\n",
    "- Model-specific settings: Either `bi_encoder_model` or `cross_encoder_model`.\n",
    "- `text_type`: Indicates which section(s) of the patent text to use.\n",
    "\n",
    "---\n",
    "\n",
    "This setup provides a flexible, modular framework for running multiple reranking experiments efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # --- Data Files ---\n",
    "    'base_dir': '.',\n",
    "    'query_list_file': 'test_queries.json',\n",
    "    'pre_ranking_file': 'shuffled_pre_ranking.json',\n",
    "    'queries_content_file': 'queries_content_with_features.json',\n",
    "    'documents_content_file': 'documents_content_with_features.json',\n",
    "    'reranker_type': 'bi-encoder',\n",
    "    'bi_encoder_model': 'AI-Growth-Lab/PatentSBERTa',\n",
    "    'cross_encoder_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "    'text_type': 'tac1',\n",
    "    'max_length': 512,\n",
    "    'batch_size': 32,\n",
    "    'device': None,\n",
    "    'save_individual_predictions': True,\n",
    "    'output_file_prefix': 'prediction_exp',\n",
    "}\n",
    "\n",
    "experiments = [\n",
    "    {\n",
    "        'exp_id': 'BiEnc_PatentSBERTa_tac1',\n",
    "        'reranker_type': 'bi-encoder',\n",
    "        'bi_encoder_model': 'AI-Growth-Lab/PatentSBERTa',\n",
    "        'text_type': 'tac1',\n",
    "    },\n",
    "    {\n",
    "        'exp_id': 'BiEnc_MPNet_tac1',\n",
    "        'reranker_type': 'bi-encoder',\n",
    "        'bi_encoder_model': 'all-mpnet-base-v2',\n",
    "        'text_type': 'tac1',\n",
    "    },\n",
    "    {\n",
    "        'exp_id': 'BiEnc_PatentSBERTa_claims',\n",
    "        'reranker_type': 'bi-encoder',\n",
    "        'bi_encoder_model': 'AI-Growth-Lab/PatentSBERTa',\n",
    "        'text_type': 'claims',\n",
    "    },\n",
    "    {\n",
    "        'exp_id': 'BiEnc_MPNet_claims',\n",
    "        'reranker_type': 'bi-encoder',\n",
    "        'bi_encoder_model': 'all-mpnet-base-v2',\n",
    "        'text_type': 'claims',\n",
    "    },\n",
    "    {\n",
    "        'exp_id': 'BiEnc_MultiQA_tac1',\n",
    "        'reranker_type': 'bi-encoder',\n",
    "        'bi_encoder_model': 'multi-qa-mpnet-base-dot-v1',\n",
    "        'text_type': 'tac1',\n",
    "    },\n",
    "    {\n",
    "        'exp_id': 'CrossEnc_L6_tac1',\n",
    "        'reranker_type': 'cross-encoder',\n",
    "        'cross_encoder_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "        'text_type': 'tac1',\n",
    "    },\n",
    "    {\n",
    "        'exp_id': 'CrossEnc_L6_claims',\n",
    "        'reranker_type': 'cross-encoder',\n",
    "        'cross_encoder_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "        'text_type': 'claims',\n",
    "     },\n",
    "     {\n",
    "        'exp_id': 'CrossEnc_L6_TA',\n",
    "        'reranker_type': 'cross-encoder',\n",
    "        'cross_encoder_model': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "        'text_type': 'TA',\n",
    "     },\n",
    "]\n",
    "\n",
    "if config['device'] is None:\n",
    "    config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "elif config['device'] == 'cuda' and not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA requested but not available. Using CPU.\")\n",
    "    config['device'] = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "We define helper functions so we can easily work with the json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f: data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError: print(f\"Error: File not found at {file_path}\"); return None\n",
    "    except json.JSONDecodeError: print(f\"Error: Could not decode JSON from {file_path}\"); return None\n",
    "    except Exception as e: print(f\"An unexpected error occurred loading {file_path}: {e}\"); return None\n",
    "\n",
    "def save_json_file(data, file_path):\n",
    "    print(f\"Saving predictions to: {file_path}\")\n",
    "    try:\n",
    "        output_dir = os.path.dirname(file_path)\n",
    "        if output_dir: os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(file_path, 'w', encoding='utf-8') as f: json.dump(data, f, indent=2)\n",
    "    except Exception as e: print(f\"An error occurred saving to {file_path}: {e}\")\n",
    "\n",
    "def load_content_data(file_path):\n",
    "    data = load_json_file(file_path)\n",
    "    if data is None: return {}\n",
    "    print(f\"Processing content file: {os.path.basename(file_path)}\")\n",
    "    content_dict = {}\n",
    "    key_options = ['FAN', 'Application_Number']\n",
    "    for item in tqdm(data, desc=\"Loading content\", leave=False):\n",
    "        fan_key = None\n",
    "        temp_key_val = None\n",
    "        for key_name in key_options:\n",
    "            if key_name in item:\n",
    "                temp_key_val = item[key_name]\n",
    "                if key_name == 'Application_Number' and 'Application_Category' in item:\n",
    "                   fan_key = str(temp_key_val) + str(item.get('Application_Category', ''))\n",
    "                else:\n",
    "                   fan_key = str(temp_key_val)\n",
    "                break\n",
    "        if fan_key and 'Content' in item:\n",
    "             content_dict[fan_key] = item['Content']\n",
    "    return content_dict\n",
    "\n",
    "def extract_text(content_dict, text_type=\"TA\"):\n",
    "    if not isinstance(content_dict, dict): return \"\"\n",
    "    text_parts = []\n",
    "    if text_type in [\"TA\", \"tac1\", \"full\", \"title_abstract\"]:\n",
    "        text_parts.append(content_dict.get(\"title\", \"\"))\n",
    "        text_parts.append(content_dict.get(\"pa01\", \"\"))\n",
    "    if text_type in [\"claims\", \"tac1\", \"full\"]:\n",
    "        claims, first_claim = [], None\n",
    "        claim_keys = [key for key in content_dict if key.startswith('c-')]\n",
    "        def get_sort_key(key_string):\n",
    "            parts = key_string.split('-', 1); return int(parts[1]) if len(parts) == 2 and parts[1].isdigit() else float('inf')\n",
    "        sorted_keys = sorted(claim_keys, key=get_sort_key)\n",
    "        for key in sorted_keys:\n",
    "            claim_text = content_dict.get(key, \"\")\n",
    "            if claim_text:\n",
    "                claims.append(claim_text)\n",
    "                if first_claim is None and text_type == \"tac1\": first_claim = claim_text\n",
    "        if text_type == \"claims\" or text_type == \"full\": text_parts.extend(claims)\n",
    "        elif text_type == \"tac1\" and first_claim: text_parts.append(first_claim)\n",
    "    if text_type in [\"description\", \"full\"]:\n",
    "        desc_parts = []\n",
    "        desc_keys = [key for key in content_dict if key.startswith('p')]\n",
    "        def get_p_sort_key(key_string):\n",
    "             parts = key_string.split('-', 1); return int(parts[1]) if len(parts) == 2 and parts[1].isdigit() else float('inf')\n",
    "        sorted_keys = sorted(desc_keys, key=get_p_sort_key)\n",
    "        for key in sorted_keys: desc_parts.append(content_dict.get(key,\"\"))\n",
    "        text_parts.extend(desc_parts)\n",
    "    if text_type == \"features\": text_parts.append(content_dict.get(\"features\", \"\"))\n",
    "    result = \" \".join(filter(None, text_parts)).strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform re-ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute reranking of candidate documents for a list of queries using either a **bi-encoder** or **cross-encoder** model from the Sentence Transformers framework.\n",
    "\n",
    "#### **Workflow Overview**\n",
    "1. **Model Loading**:\n",
    "   - Loads a SentenceTransformer (bi-encoder) or CrossEncoder model onto the specified device.\n",
    "   - Applies a max sequence length and batch size as per config.\n",
    "\n",
    "2. **Per-Query Processing**:\n",
    "   - For each query:\n",
    "     - Retrieves the query text and candidate document texts using the specified `text_type` (e.g., `'tac1'`, `'claims'`, etc.).\n",
    "     - Skips queries with missing data.\n",
    "     - Filters out documents with missing or empty content.\n",
    "\n",
    "3. **Scoring**:\n",
    "   - **Bi-Encoder**: Encodes query and documents into embeddings and computes cosine similarity.\n",
    "   - **Cross-Encoder**: Scores query-document pairs directly using the model.\n",
    "   - Handles exceptions and assigns fallback scores to maintain consistent ranking length.\n",
    "\n",
    "4. **Ranking**:\n",
    "   - Ranks candidates by score (descending).\n",
    "   - Ensures all originally ranked documents are included.\n",
    "   - Truncates results to original candidate list length.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Robustness Features**\n",
    "- Handles missing or malformed content gracefully.\n",
    "- Applies fallback scores to maintain list structure if model scoring fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_reranking(exp_config, query_ids, pre_ranking_data, queries_content, documents_content):\n",
    "    device = torch.device(exp_config['device'])\n",
    "    reranker_type = exp_config['reranker_type']\n",
    "    text_type = exp_config['text_type']\n",
    "    model = None\n",
    "    model_name = \"\"\n",
    "\n",
    "    # --- Load Model ---\n",
    "    try:\n",
    "        if reranker_type == 'bi-encoder':\n",
    "            model_name = exp_config.get('bi_encoder_model')\n",
    "            if not model_name: raise ValueError(\"bi_encoder_model must be specified\")\n",
    "            print(f\"Loading Bi-Encoder model: {model_name}...\")\n",
    "            model = SentenceTransformer(model_name, device=device)\n",
    "            model.max_seq_length = exp_config['max_length']\n",
    "\n",
    "        elif reranker_type == 'cross-encoder':\n",
    "            model_name = exp_config.get('cross_encoder_model')\n",
    "            if not model_name: raise ValueError(\"cross_encoder_model must be specified\")\n",
    "            print(f\"Loading Cross-Encoder model: {model_name}...\")\n",
    "            model = CrossEncoder(model_name, device=device, max_length=exp_config['max_length'])\n",
    "        else:\n",
    "             raise ValueError(f\"Invalid reranker_type: {reranker_type}\")\n",
    "        print(f\"Model '{model_name}' loaded.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError loading model '{model_name}' for experiment '{exp_config.get('exp_id', 'N/A')}': {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Re-ranking Process ---\n",
    "    results = {}\n",
    "    pbar = tqdm(query_ids, desc=f\"Re-ranking ({exp_config.get('exp_id', 'N/A')})\", leave=False)\n",
    "    for query_id in pbar:\n",
    "        candidate_doc_ids = pre_ranking_data.get(query_id, []) # query_id is string, doc_ids are strings\n",
    "        query_content_dict = queries_content.get(query_id) # query_id is string\n",
    "\n",
    "        if not candidate_doc_ids: results[query_id] = []; continue\n",
    "        if not query_content_dict: results[query_id] = candidate_doc_ids; continue\n",
    "\n",
    "        query_text = extract_text(query_content_dict, text_type)\n",
    "        if not query_text: results[query_id] = candidate_doc_ids; continue\n",
    "\n",
    "        valid_docs_texts = {}\n",
    "        for doc_id in candidate_doc_ids:\n",
    "            doc_content_dict = documents_content.get(doc_id)\n",
    "            if doc_content_dict:\n",
    "                doc_text = extract_text(doc_content_dict, text_type)\n",
    "                if doc_text: valid_docs_texts[doc_id] = doc_text\n",
    "\n",
    "        valid_doc_ids = list(valid_docs_texts.keys())\n",
    "        if not valid_doc_ids: results[query_id] = candidate_doc_ids; continue\n",
    "\n",
    "        doc_scores_calculated = {}\n",
    "        try:\n",
    "            doc_texts_for_scoring = [valid_docs_texts[doc_id] for doc_id in valid_doc_ids]\n",
    "\n",
    "            if reranker_type == 'bi-encoder':\n",
    "                query_embedding = model.encode(query_text, convert_to_tensor=True, show_progress_bar=False).to(device)\n",
    "                doc_embeddings = model.encode(doc_texts_for_scoring, convert_to_tensor=True, show_progress_bar=False, batch_size=exp_config['batch_size']).to(device)\n",
    "                if query_embedding is None or doc_embeddings is None or len(doc_embeddings) == 0: raise RuntimeError(\"Embedding generation failed\")\n",
    "                if query_embedding.shape[0] == 0 or doc_embeddings.shape[0] == 0: raise RuntimeError(\"Embedding tensor is empty\")\n",
    "                cosine_scores = util.cos_sim(query_embedding, doc_embeddings)[0].cpu().numpy()\n",
    "                doc_scores_calculated = dict(zip(valid_doc_ids, cosine_scores))\n",
    "\n",
    "            elif reranker_type == 'cross-encoder':\n",
    "                sentence_pairs = [[query_text, doc_text] for doc_text in doc_texts_for_scoring]\n",
    "                cross_scores = model.predict(sentence_pairs, show_progress_bar=False, batch_size=exp_config['batch_size'], convert_to_numpy=True)\n",
    "                doc_scores_calculated = dict(zip(valid_doc_ids, cross_scores))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during scoring query {query_id} in exp {exp_config.get('exp_id', 'N/A')}: {type(e).__name__} - {e}\")\n",
    "            results[query_id] = candidate_doc_ids; continue\n",
    "\n",
    "        # --- Rank Documents ---\n",
    "        min_score = min(doc_scores_calculated.values()) if doc_scores_calculated else 0\n",
    "        fallback_score = min_score - 1 if min_score > -float('inf') else -float('inf')\n",
    "        scored_doc_list = []\n",
    "        processed_docs = set()\n",
    "        for doc_id, score in doc_scores_calculated.items():\n",
    "            scored_doc_list.append((doc_id, float(score)))\n",
    "            processed_docs.add(doc_id)\n",
    "        for doc_id in candidate_doc_ids:\n",
    "            if doc_id not in processed_docs: scored_doc_list.append((doc_id, fallback_score))\n",
    "        scored_doc_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        final_ranked_list = [doc_id for doc_id, score in scored_doc_list]\n",
    "        results[query_id] = final_ranked_list[:len(candidate_doc_ids)]\n",
    "\n",
    "    del model\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Files Loaded**\n",
    "- `query_list_file`: List of query IDs.\n",
    "- `pre_ranking_file`: Initial candidate document rankings per query.\n",
    "- `queries_content_file`: Dictionary of query text content.\n",
    "- `documents_content_file`: Dictionary of document text content.\n",
    "- Ensures all IDs and keys are cast to strings for consistency.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Data Validation**\n",
    "- Ensures all required datasets (`query_ids`, `pre_ranking_data`, `queries_content`, `documents_content`) are loaded successfully.\n",
    "- Exits early with a warning if any are missing.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Running Experiments**\n",
    "- Iterates over all experiment configurations in `experiments`.\n",
    "- For each experiment:\n",
    "  1. **Preparation**:\n",
    "     - Merges base config with experiment-specific overrides (`exp_id`, model name, text type).\n",
    "  2. **Reranking**:\n",
    "     - Calls `perform_reranking()` to generate final document rankings per query.\n",
    "  3. **Timing**:\n",
    "     - Measures and prints duration of each experiment.\n",
    "  4. **Output Saving**:\n",
    "     - Saves predictions to a JSON file if `save_individual_predictions` is `True`.\n",
    "     - File name format: `prediction_exp_<exp_id>.json`.\n",
    "\n",
    "---\n",
    "\n",
    "This section automates the full reranking and prediction pipeline for multiple experiment configurations, outputting prediction files ready for downstream evaluation or submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Base device requested: {config.get('device', 'None specified')}\")\n",
    "print(f\"Using effective device: {config['device']}\") # Show auto-detected device\n",
    "\n",
    "# --- Construct Full Paths ---\n",
    "def get_full_path(path): return path if os.path.isabs(path) else os.path.join(config['base_dir'], path)\n",
    "query_list_file = get_full_path(config['query_list_file'])\n",
    "pre_ranking_file = get_full_path(config['pre_ranking_file'])\n",
    "queries_content_file = get_full_path(config['queries_content_file'])\n",
    "documents_content_file = get_full_path(config['documents_content_file'])\n",
    "\n",
    "print(\"\\nLoading shared data...\")\n",
    "query_ids_raw = load_json_file(query_list_file)\n",
    "pre_ranking_data_raw = load_json_file(pre_ranking_file)\n",
    "queries_content_raw = load_content_data(queries_content_file)\n",
    "documents_content_raw = load_content_data(documents_content_file)\n",
    "\n",
    "if query_ids_raw is None: print(\"\\nError: Failed to load query_list_file. Exiting.\");\n",
    "query_ids = [str(qid) for qid in query_ids_raw]\n",
    "print(f\"Loaded and processed {len(query_ids)} query IDs (as strings).\")\n",
    "\n",
    "if pre_ranking_data_raw is None: print(\"\\nError: Failed to load pre_ranking_file. Exiting.\");\n",
    "pre_ranking_data = {str(k): list(map(str, v)) for k, v in pre_ranking_data_raw.items()}\n",
    "print(f\"Processed {len(pre_ranking_data)} pre-ranking entries (keys/docs as strings).\")\n",
    "\n",
    "if queries_content_raw is None or documents_content_raw is None:\n",
    "        print(\"\\nError: Failed to load content files. Exiting.\");\n",
    "queries_content = {str(k): v for k, v in queries_content_raw.items()}\n",
    "documents_content = {str(k): v for k, v in documents_content_raw.items()}\n",
    "print(\"Ensured content dictionary keys are strings.\")\n",
    "# ------------------------------------\n",
    "\n",
    "if not all([pre_ranking_data, queries_content, documents_content]):\n",
    "    print(\"\\nError: Failed to load one or more data files (pre-ranking, content). Exiting.\")\n",
    "\n",
    "print(f\"\\nStarting {len(experiments)} experiments to generate prediction files...\")\n",
    "\n",
    "for i, exp_params in enumerate(experiments):\n",
    "    exp_id = exp_params.get('exp_id', f'exp_{i+1}')\n",
    "    print(f\"\\n--- Running Experiment: {exp_id} ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    run_config = config.copy()\n",
    "    run_config.update(exp_params)\n",
    "\n",
    "    # Perform re-ranking\n",
    "    predictions = perform_reranking(\n",
    "        run_config,\n",
    "        query_ids,\n",
    "        pre_ranking_data,\n",
    "        queries_content,\n",
    "        documents_content\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Experiment {exp_id} finished in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    if predictions is None:\n",
    "        print(f\"Experiment {exp_id} failed during re-ranking. No prediction file generated.\")\n",
    "    else:\n",
    "        # Save predictions if enabled\n",
    "        if run_config.get('save_individual_predictions', False):\n",
    "                pred_filename = f\"{run_config.get('output_file_prefix', 'pred')}_{exp_id}.json\"\n",
    "                pred_filepath = get_full_path(pred_filename)\n",
    "                save_json_file(predictions, pred_filepath) # Save the generated predictions\n",
    "        else:\n",
    "                print(f\"Skipping saving prediction file for {exp_id} as 'save_individual_predictions' is False.\")\n",
    "\n",
    "print(\"\\nAll experiments complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
